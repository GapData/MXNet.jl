{
    "docs": [
        {
            "location": "/", 
            "text": "MXNet Documentation\n\n\nMXNet.jl\n is the \nJulia\n package of \ndmlc/mxnet\n. MXNet.jl brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:\n\n\n\n\nEfficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.\n\n\nFlexible symbolic manipulation to composite and construct state-of-the-art deep learning models.\n\n\n\n\nFor more details, see documentation below. Please also checkout the \nexamples\n directory.\n\n\n\n\nTutorials\n\n\n\n\nDigit Recognition on MNIST\n\n\nSimple 3-layer MLP\n\n\nConvolutional Neural Networks\n\n\nPredicting with a trained model\n\n\n\n\n\n\nGenerating Random Sentence with LSTM RNN\n\n\nLSTM Cells\n\n\nUnfolding LSTM\n\n\nData Provider for Text Sequences\n\n\nTraining the LSTM\n\n\nSampling Random Sentences\n\n\nVisualizing the LSTM\n\n\n\n\n\n\n\n\n\n\nUser's Guide\n\n\n\n\nInstallation Guide\n\n\nAutomatic Installation\n\n\nManual Compilation\n\n\n\n\n\n\nOverview\n\n\nMXNet.jl Namespace\n\n\nLow Level Interface\n\n\nIntermediate Level Interface\n\n\nHigh Level Interface\n\n\n\n\n\n\nFAQ\n\n\nRunning MXNet on AWS GPU instances\n\n\n\n\n\n\n\n\n\n\nAPI Documentation\n\n\n\n\nContext\n\n\nNDArray API\n\n\nArithmetic Operations\n\n\nTrigonometric Functions\n\n\nHyperbolic Functions\n\n\nActivation Functions\n\n\nReference\n\n\n\n\n\n\nSymbolic API\n\n\nModel\n\n\nEvaluation Metrics\n\n\nData Providers\n\n\nAbstractDataProvider interface\n\n\nAbstractDataBatch interface\n\n\nImplemented providers and other methods\n\n\n\n\n\n\nNeural Network Factory\n\n\nExecutor\n\n\nNetwork Visualization", 
            "title": "Home"
        }, 
        {
            "location": "/#mxnet-documentation", 
            "text": "MXNet.jl  is the  Julia  package of  dmlc/mxnet . MXNet.jl brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:   Efficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.  Flexible symbolic manipulation to composite and construct state-of-the-art deep learning models.   For more details, see documentation below. Please also checkout the  examples  directory.", 
            "title": "MXNet Documentation"
        }, 
        {
            "location": "/#tutorials", 
            "text": "Digit Recognition on MNIST  Simple 3-layer MLP  Convolutional Neural Networks  Predicting with a trained model    Generating Random Sentence with LSTM RNN  LSTM Cells  Unfolding LSTM  Data Provider for Text Sequences  Training the LSTM  Sampling Random Sentences  Visualizing the LSTM", 
            "title": "Tutorials"
        }, 
        {
            "location": "/#users-guide", 
            "text": "Installation Guide  Automatic Installation  Manual Compilation    Overview  MXNet.jl Namespace  Low Level Interface  Intermediate Level Interface  High Level Interface    FAQ  Running MXNet on AWS GPU instances", 
            "title": "User's Guide"
        }, 
        {
            "location": "/#api-documentation", 
            "text": "Context  NDArray API  Arithmetic Operations  Trigonometric Functions  Hyperbolic Functions  Activation Functions  Reference    Symbolic API  Model  Evaluation Metrics  Data Providers  AbstractDataProvider interface  AbstractDataBatch interface  Implemented providers and other methods    Neural Network Factory  Executor  Network Visualization", 
            "title": "API Documentation"
        }, 
        {
            "location": "/tutorial/mnist/", 
            "text": "Digit Recognition on MNIST\n\n\nIn this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the \nMNIST handwritten digit dataset\n. The code for this tutorial could be found in \nexamples/mnist\n.  There are also two Jupyter notebooks that expand a little more on the \nMLP\n and the \nLeNet\n, using the more general \nArrayDataProvider\n. \n\n\n\n\nSimple 3-layer MLP\n\n\nThis is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with\n\n\nusing MXNet\n\n\n\n\nto load the \nMXNet\n module. Then we are ready to define the network architecture via the \nsymbolic API\n. We start with a placeholder \ndata\n symbol,\n\n\ndata = mx.Variable(:data)\n\n\n\n\nand then cascading fully-connected layers and activation functions:\n\n\nfc1  = mx.FullyConnected(data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(act2, name=:fc3, num_hidden=10)\n\n\n\n\nNote each composition we take the previous symbol as the first argument, forming a feedforward chain. The architecture looks like\n\n\nInput --\n 128 units (ReLU) --\n 64 units (ReLU) --\n 10 units\n\n\n\n\nwhere the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final \nSoftmaxOutput\n operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:\n\n\nmlp  = mx.SoftmaxOutput(fc3, name=:softmax)\n\n\n\n\nAs we can see, the MLP is just a chain of layers. For this case, we can also use the \nmx.chain\n macro. The same architecture above can be defined as\n\n\nmlp = @mx.chain mx.Variable(:data)             =\n\n  mx.FullyConnected(name=:fc1, num_hidden=128) =\n\n  mx.Activation(name=:relu1, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc2, num_hidden=64)  =\n\n  mx.Activation(name=:relu2, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc3, num_hidden=10)  =\n\n  mx.SoftmaxOutput(name=:softmax)\n\n\n\n\nAfter defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into \nPkg.dir(\"MXNet\")/data/mnist\n if necessary. We wrap the code to construct the data provider into \nmnist-data.jl\n so that it could be shared by both the MLP example and the LeNet ConvNets example.\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size)\n\n\n\n\nIf you need to write your own data providers for customized data format, please refer to \nmx.AbstractDataProvider\n.\n\n\nGiven the architecture and data, we can instantiate an \nmodel\n to do the actual training. \nmx.FeedForward\n is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the \ncontext\n on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.\n\n\nmodel = mx.FeedForward(mlp, context=mx.cpu())\n\n\n\n\nYou can use a \nmx.gpu()\n or if a list of devices (e.g. \n[mx.gpu(0), mx.gpu(1)]\n) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.\n\n\nThe last thing we need to specify is the optimization algorithm (a.k.a. \noptimizer\n) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:\n\n\noptimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)\n\n\n\n\nNow we can do the training. Here the \nn_epoch\n parameter specifies that we want to train for 20 epochs. We also supply a \neval_data\n to monitor validation accuracy on the validation set.\n\n\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nHere is a sample output\n\n\nINFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\nIn the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:\n\n\n# input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data, kernel=(5,5), num_filter=20)  =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(conv1, kernel=(5,5), num_filter=50) =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n\n\n\nWe basically defined two convolution modules. Each convolution module is actually a chain of \nConvolution\n, \ntanh\n activation and then max \nPooling\n operations.\n\n\nEach sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by \nNDArray\n, a batch of 100 samples is a tensor of shape \n(28,28,1,100)\n. The convolution and pooling operates in the spatial axis, so \nkernel=(5,5)\n indicate a square region of 5-width and 5-height. The rest of the architecture follows as:\n\n\n# first fully-connected\nfc1   = @mx.chain mx.Flatten(conv2) =\n\n                  mx.FullyConnected(num_hidden=500) =\n\n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(fc2, name=:softmax)\n\n\n\n\nNote a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a \nFlatten\n operator to flat the tensor, before connecting it to the \nFullyConnected\n operator.\n\n\nThe rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)\n\n\n\n\nNote we specified \nflat=false\n to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.\n\n\n# fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nAnd here is a sample of running outputs:\n\n\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915\n\n\n\n\n\n\nPredicting with a trained model\n\n\nPredicting with a trained model is very simple. By calling \nmx.predict\n with the model and a data provider, we get the model output as a Julia Array:\n\n\nprobs = mx.predict(model, eval_provider)\n\n\n\n\nThe following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:\n\n\n# collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format(\nAccuracy on eval set: {1:.2f}%\n, 100correct/length(labels)))\n\n\n\n\nAlternatively, when the dataset is huge, one can provide a callback to \nmx.predict\n, then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from \nmx.predict\n. See also predict.", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#digit-recognition-on-mnist", 
            "text": "In this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the  MNIST handwritten digit dataset . The code for this tutorial could be found in  examples/mnist .  There are also two Jupyter notebooks that expand a little more on the  MLP  and the  LeNet , using the more general  ArrayDataProvider .", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#simple-3-layer-mlp", 
            "text": "This is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with  using MXNet  to load the  MXNet  module. Then we are ready to define the network architecture via the  symbolic API . We start with a placeholder  data  symbol,  data = mx.Variable(:data)  and then cascading fully-connected layers and activation functions:  fc1  = mx.FullyConnected(data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(act2, name=:fc3, num_hidden=10)  Note each composition we take the previous symbol as the first argument, forming a feedforward chain. The architecture looks like  Input --  128 units (ReLU) --  64 units (ReLU) --  10 units  where the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final  SoftmaxOutput  operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:  mlp  = mx.SoftmaxOutput(fc3, name=:softmax)  As we can see, the MLP is just a chain of layers. For this case, we can also use the  mx.chain  macro. The same architecture above can be defined as  mlp = @mx.chain mx.Variable(:data)             = \n  mx.FullyConnected(name=:fc1, num_hidden=128) = \n  mx.Activation(name=:relu1, act_type=:relu)   = \n  mx.FullyConnected(name=:fc2, num_hidden=64)  = \n  mx.Activation(name=:relu2, act_type=:relu)   = \n  mx.FullyConnected(name=:fc3, num_hidden=10)  = \n  mx.SoftmaxOutput(name=:softmax)  After defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into  Pkg.dir(\"MXNet\")/data/mnist  if necessary. We wrap the code to construct the data provider into  mnist-data.jl  so that it could be shared by both the MLP example and the LeNet ConvNets example.  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size)  If you need to write your own data providers for customized data format, please refer to  mx.AbstractDataProvider .  Given the architecture and data, we can instantiate an  model  to do the actual training.  mx.FeedForward  is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the  context  on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.  model = mx.FeedForward(mlp, context=mx.cpu())  You can use a  mx.gpu()  or if a list of devices (e.g.  [mx.gpu(0), mx.gpu(1)] ) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.  The last thing we need to specify is the optimization algorithm (a.k.a.  optimizer ) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:  optimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)  Now we can do the training. Here the  n_epoch  parameter specifies that we want to train for 20 epochs. We also supply a  eval_data  to monitor validation accuracy on the validation set.  mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  Here is a sample output  INFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775", 
            "title": "Simple 3-layer MLP"
        }, 
        {
            "location": "/tutorial/mnist/#convolutional-neural-networks", 
            "text": "In the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:  # input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data, kernel=(5,5), num_filter=20)  = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(conv1, kernel=(5,5), num_filter=50) = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))  We basically defined two convolution modules. Each convolution module is actually a chain of  Convolution ,  tanh  activation and then max  Pooling  operations.  Each sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by  NDArray , a batch of 100 samples is a tensor of shape  (28,28,1,100) . The convolution and pooling operates in the spatial axis, so  kernel=(5,5)  indicate a square region of 5-width and 5-height. The rest of the architecture follows as:  # first fully-connected\nfc1   = @mx.chain mx.Flatten(conv2) = \n                  mx.FullyConnected(num_hidden=500) = \n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(fc2, name=:softmax)  Note a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a  Flatten  operator to flat the tensor, before connecting it to the  FullyConnected  operator.  The rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)  Note we specified  flat=false  to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.  # fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  And here is a sample of running outputs:  INFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915", 
            "title": "Convolutional Neural Networks"
        }, 
        {
            "location": "/tutorial/mnist/#predicting-with-a-trained-model", 
            "text": "Predicting with a trained model is very simple. By calling  mx.predict  with the model and a data provider, we get the model output as a Julia Array:  probs = mx.predict(model, eval_provider)  The following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:  # collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format( Accuracy on eval set: {1:.2f}% , 100correct/length(labels)))  Alternatively, when the dataset is huge, one can provide a callback to  mx.predict , then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from  mx.predict . See also predict.", 
            "title": "Predicting with a trained model"
        }, 
        {
            "location": "/tutorial/char-lstm/", 
            "text": "Generating Random Sentence with LSTM RNN\n\n\nThis tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called \nchar-rnn\n is described in \nAndrej Karpathy's blog\n, with a reference implementation in Torch available \nhere\n.\n\n\nBecause MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the \nchar-rnn example for MXNet's Python binding\n, which demonstrates how to use low-level \nSymbolic API\n to build customized neural network models directly.\n\n\nThe most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the \nexamples/char-lstm\n directory. You will need to install \nIterators.jl\n and \nStatsBase.jl\n to run this example.\n\n\n\n\nLSTM Cells\n\n\nChristopher Olah has a \ngreat blog post about LSTM\n with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input \nx\n, as well as previous states (including \nc\n and \nh\n), and produce the next states. We define a helper type to bundle the two state variables together:\n\n\nBecause LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.\n\n\nNote all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.\n\n\nThe following figure is stolen (permission requested) from \nChristopher Olah's blog\n, which illustrate exactly what the code snippet above is doing.\n\n\n\n\nIn particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.\n\n\n\n\nUnfolding LSTM\n\n\nUsing the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.\n\n\nThe \nembed_W\n is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The \npred_W\n and \npred_b\n are weights and bias for the final prediction at each time step.\n\n\nThen we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however, \nnot\n shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.\n\n\nUnrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as \n:ptb\n, the data and label at step \nt\n will be named \n:ptb_data_$t\n and \n:ptb_label_$t\n. Late on when we prepare the data, we will define the data provider to match those names.\n\n\nNote at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.\n\n\nIn the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.\n\n\n\n\nData Provider for Text Sequences\n\n\nNow we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.\n\n\nNote the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.\n\n\nThe text sequence data provider implements the \nData Providers\n api. We define the \nCharSeqProvider\n as below:\n\n\nThe provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from \n$name_data_$t\n and \n$name_label_$t\n, we also provides the initial \nc\n and \nh\n states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.\n\n\nNext we implement the \neachbatch\n method from the \nmx.AbstractDataProvider\n interface for the provider. We start by defining the data and label arrays, and the \nDataBatch\n object we will provide in each iteration.\n\n\nThe actual data providing iteration is implemented as a Julia \ncoroutine\n. In this way, we can write the data loading logic as a simple coherent \nfor\n loop, and do not need to implement the interface functions like Base.start, Base.next, etc.\n\n\nBasically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.\n\n\n\n\nTraining the LSTM\n\n\nNow we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:\n\n\nNote all the parameters are defined in \nexamples/char-lstm/config.jl\n. Now we load the text file and define the data provider. The data \ninput.txt\n we used in this example is \na tiny Shakespeare dataset\n. But you can try with other text files.\n\n\nThe last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.\n\n\nNote we are also using a customized \nNLL\n evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.\n\n\n...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'\n\n\n\n\n\n\nSampling Random Sentences\n\n\nAfter training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:\n\n\n\n\nStarting from some fixed character, take \na\n for example, and feed   it as input to the LSTM.\n\n\nThe LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.\n\n\nIn the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).\n\n\nContinue running until we sampled enough characters.\n\n\n\n\nNote we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.\n\n\n## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?\n\n\n\n\nSee \nAndrej Karpathy's blog post\n on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in \nexamples/char-lstm/sampler.jl\n.\n\n\n\n\nVisualizing the LSTM\n\n\nFinally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than \nChristopher Olah's illustrations\n, but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in \nexamples/char-lstm/visualize.jl\n.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#generating-random-sentence-with-lstm-rnn", 
            "text": "This tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called  char-rnn  is described in  Andrej Karpathy's blog , with a reference implementation in Torch available  here .  Because MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the  char-rnn example for MXNet's Python binding , which demonstrates how to use low-level  Symbolic API  to build customized neural network models directly.  The most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the  examples/char-lstm  directory. You will need to install  Iterators.jl  and  StatsBase.jl  to run this example.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#lstm-cells", 
            "text": "Christopher Olah has a  great blog post about LSTM  with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input  x , as well as previous states (including  c  and  h ), and produce the next states. We define a helper type to bundle the two state variables together:  Because LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.  Note all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.  The following figure is stolen (permission requested) from  Christopher Olah's blog , which illustrate exactly what the code snippet above is doing.   In particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.", 
            "title": "LSTM Cells"
        }, 
        {
            "location": "/tutorial/char-lstm/#unfolding-lstm", 
            "text": "Using the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.  The  embed_W  is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The  pred_W  and  pred_b  are weights and bias for the final prediction at each time step.  Then we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however,  not  shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.  Unrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as  :ptb , the data and label at step  t  will be named  :ptb_data_$t  and  :ptb_label_$t . Late on when we prepare the data, we will define the data provider to match those names.  Note at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.  In the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.", 
            "title": "Unfolding LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#data-provider-for-text-sequences", 
            "text": "Now we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.  Note the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.  The text sequence data provider implements the  Data Providers  api. We define the  CharSeqProvider  as below:  The provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from  $name_data_$t  and  $name_label_$t , we also provides the initial  c  and  h  states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.  Next we implement the  eachbatch  method from the  mx.AbstractDataProvider  interface for the provider. We start by defining the data and label arrays, and the  DataBatch  object we will provide in each iteration.  The actual data providing iteration is implemented as a Julia  coroutine . In this way, we can write the data loading logic as a simple coherent  for  loop, and do not need to implement the interface functions like Base.start, Base.next, etc.  Basically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.", 
            "title": "Data Provider for Text Sequences"
        }, 
        {
            "location": "/tutorial/char-lstm/#training-the-lstm", 
            "text": "Now we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:  Note all the parameters are defined in  examples/char-lstm/config.jl . Now we load the text file and define the data provider. The data  input.txt  we used in this example is  a tiny Shakespeare dataset . But you can try with other text files.  The last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.  Note we are also using a customized  NLL  evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.  ...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'", 
            "title": "Training the LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#sampling-random-sentences", 
            "text": "After training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:   Starting from some fixed character, take  a  for example, and feed   it as input to the LSTM.  The LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.  In the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).  Continue running until we sampled enough characters.   Note we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.  ## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?  See  Andrej Karpathy's blog post  on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in  examples/char-lstm/sampler.jl .", 
            "title": "Sampling Random Sentences"
        }, 
        {
            "location": "/tutorial/char-lstm/#visualizing-the-lstm", 
            "text": "Finally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than  Christopher Olah's illustrations , but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in  examples/char-lstm/visualize.jl .", 
            "title": "Visualizing the LSTM"
        }, 
        {
            "location": "/user-guide/install/", 
            "text": "Installation Guide\n\n\n\n\nAutomatic Installation\n\n\nTo install MXNet.jl, simply type\n\n\nPkg.add(\nMXNet\n)\n\n\n\n\nIn the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead\n\n\nPkg.checkout(\nMXNet\n)\n\n\n\n\nMXNet.jl is built on top of \nlibmxnet\n. Upon installation, Julia will try to automatically download and build libmxnet.\n\n\nThere are three environment variables that change this behaviour. If you already have a pre-installed version of mxnet you can use \nMXNET_HOME\n to point the build-process in the right direction. If the automatic cuda detection fails you can also set \nCUDA_HOME\n to override the process. To control which version of libmxnet will be compiled, you can use the \nMXNET_COMMIT\n variable to point to either a version tag (e.g. \nv0.10.0\n), a branch name (e.g. \nmaster\n) or a specific commit hash (e.g. \na0b1c2d3\n).\n\n\nThe libmxnet source is downloaded to \nPkg.dir(\"MXNet\", \"deps\", \"src\", \"mxnet\")\n. The automatic build is using default configurations, with OpenCV disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, you can compile and install libmxnet manually. Please see below for more details.\n\n\n\n\nManual Compilation\n\n\nIt is possible to compile libmxnet separately and point MXNet.jl to a existing library in case automatic compilation fails due to unresolved dependencies in an non-standard environment; Or when one want to work with a separate, maybe customized libmxnet.\n\n\nTo build libmxnet, please refer to \nthe installation guide of libmxnet\n. After successfully installing libmxnet, set the \nMXNET_HOME\n \nenvironment variable\n to the location of libmxnet. In other words, the compiled \nlibmxnet.so\n should be found in \n$MXNET_HOME/lib\n.\n\n\n\n\nnote\n\n\nThe constant \nMXNET_HOME\n is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by \nBase.compilecache(\"MXNet\")\n.\n\n\n\n\nWhen the \nMXNET_HOME\n environment variable is detected and the corresponding \nlibmxnet.so\n could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.\n\n\nBasically, MXNet.jl will search \nlibmxnet.so\n or \nlibmxnet.dll\n in the following paths (and in that order):\n\n\n\n\n$MXNET_HOME/lib\n: customized libmxnet builds\n\n\nPkg.dir(\"MXNet\", \"deps\", \"usr\", \"lib\")\n: automatic builds\n\n\nAny system wide library search path\n\n\n\n\nNote that MXNet.jl can not load \nlibmxnet.so\n even if it is on one of the paths above in case a library it depends upon is missing from the \nLD_LIBRARY_PATH\n. Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to \nLD_LIBRARY_PATH\n.", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#installation-guide", 
            "text": "", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#automatic-installation", 
            "text": "To install MXNet.jl, simply type  Pkg.add( MXNet )  In the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead  Pkg.checkout( MXNet )  MXNet.jl is built on top of  libmxnet . Upon installation, Julia will try to automatically download and build libmxnet.  There are three environment variables that change this behaviour. If you already have a pre-installed version of mxnet you can use  MXNET_HOME  to point the build-process in the right direction. If the automatic cuda detection fails you can also set  CUDA_HOME  to override the process. To control which version of libmxnet will be compiled, you can use the  MXNET_COMMIT  variable to point to either a version tag (e.g.  v0.10.0 ), a branch name (e.g.  master ) or a specific commit hash (e.g.  a0b1c2d3 ).  The libmxnet source is downloaded to  Pkg.dir(\"MXNet\", \"deps\", \"src\", \"mxnet\") . The automatic build is using default configurations, with OpenCV disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, you can compile and install libmxnet manually. Please see below for more details.", 
            "title": "Automatic Installation"
        }, 
        {
            "location": "/user-guide/install/#manual-compilation", 
            "text": "It is possible to compile libmxnet separately and point MXNet.jl to a existing library in case automatic compilation fails due to unresolved dependencies in an non-standard environment; Or when one want to work with a separate, maybe customized libmxnet.  To build libmxnet, please refer to  the installation guide of libmxnet . After successfully installing libmxnet, set the  MXNET_HOME   environment variable  to the location of libmxnet. In other words, the compiled  libmxnet.so  should be found in  $MXNET_HOME/lib .   note  The constant  MXNET_HOME  is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by  Base.compilecache(\"MXNet\") .   When the  MXNET_HOME  environment variable is detected and the corresponding  libmxnet.so  could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.  Basically, MXNet.jl will search  libmxnet.so  or  libmxnet.dll  in the following paths (and in that order):   $MXNET_HOME/lib : customized libmxnet builds  Pkg.dir(\"MXNet\", \"deps\", \"usr\", \"lib\") : automatic builds  Any system wide library search path   Note that MXNet.jl can not load  libmxnet.so  even if it is on one of the paths above in case a library it depends upon is missing from the  LD_LIBRARY_PATH . Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to  LD_LIBRARY_PATH .", 
            "title": "Manual Compilation"
        }, 
        {
            "location": "/user-guide/overview/", 
            "text": "Overview\n\n\n\n\nMXNet.jl Namespace\n\n\nMost the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a \nmx\n module. The convention of accessing the MXNet.jl interface is the to use the \nmx.\n prefix explicitly:\n\n\njulia\n using MXNet\n\njulia\n x = mx.zeros(2, 3)             # MXNet NDArray\n2\u00d73 mx.NDArray{Float32} @ CPU0:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia\n y = zeros(eltype(x), size(x))  # Julia Array\n2\u00d73 Array{Float32,2}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia\n copy!(y, x)                    # Overloaded function in Julia Base\n2\u00d73 Array{Float32,2}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia\n z = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\n2\u00d73 mx.NDArray{Float32} @ GPU0:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia\n mx.copy!(z, y)                 # Same as copy!(z, y)\n2\u00d73 mx.NDArray{Float32} @ GPU0:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\n\n\n\nNote functions like \nsize\n, \ncopy!\n that is extensively overloaded for various types works out of the box. But functions like \nzeros\n and \nones\n will be ambiguous, so we always use the \nmx.\n prefix. If you prefer, the \nmx.\n prefix can be used explicitly for all MXNet.jl functions, including \nsize\n and \ncopy!\n as shown in the last line.\n\n\n\n\nLow Level Interface\n\n\n\n\nNDArray\n\n\nNDArray\n is the basic building blocks of the actual computations in MXNet. It is like a Julia \nArray\n object, with some important differences listed here:\n\n\n\n\nThe actual data could live on different \nContext\n (e.g. GPUs). For some contexts, iterating into the elements one by one is very slow, thus indexing into NDArray is not recommanded in general. The easiest way to inspect the contents of an NDArray is to use the \ncopy\n function to copy the contents as a Julia \nArray\n.\n\n\nOperations on \nNDArray\n (including basic arithmetics and neural network related operators) are executed in parallel with automatic dependency tracking to ensure correctness.\n\n\nThere is no generics in \nNDArray\n, the \neltype\n is always \nmx.MX_float\n. Because for applications in machine learning, single precision floating point numbers are typical a best choice balancing between precision, speed and portability. Also since libmxnet is designed to support multiple languages as front-ends, it is much simpler to implement with a fixed data type.\n\n\n\n\nWhile most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the \nNDArray\n API is useful for implementing \nOptimizer\n or customized operators in Julia directly.\n\n\nThe followings are common ways to create \nNDArray\n objects:\n\n\n\n\nmx.empty(shape[, context])\n: create on uninitialized array of a given shape on a specific device. For example, \nmx.empty(2, 3)\n, \nmx.((2, 3), mx.gpu(2))\n.\n\n\nmx.zeros(shape[, context])\n and \nmx.ones(shape[, context])\n: similar to the Julia's built-in \nzeros\n and \nones\n.\n\n\nmx.copy(jl_arr, context)\n: copy the contents of a Julia \nArray\n to a specific device.\n\n\n\n\nMost of the convenient functions like \nsize\n, \nlength\n, \nndims\n, \neltype\n on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take \nslices\n:\n\n\njulia\n using MXNet\n\njulia\n a = mx.ones(2, 3)\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia\n b = mx.slice(a, 1:2)\n2\u00d72 mx.NDArray{Float32,2} @ CPU0:\n 1.0  1.0\n 1.0  1.0\n\njulia\n b[:] = 2\n2\n\njulia\n a\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 2.0  2.0  1.0\n 2.0  2.0  1.0\n\n\n\n\nA slice is a sub-region sharing the same memory with the original \nNDArray\n object. A slice is always a contiguous piece of memory, so only slicing on the \nlast\n dimension is supported. The example above also shows a way to set the contents of an \nNDArray\n.\n\n\njulia\n using MXNet\n\njulia\n mx.srand(42)\n\njulia\n a = mx.empty(2, 3)\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 2.04913f-36  1.79751f19  6.86636f-44\n 0.0          1.06216f-5  0.0\n\njulia\n a[:] = 0.5              # set all elements to a scalar\n0.5\n\njulia\n a[:] = rand(size(a))    # set contents with a Julia Array\n2\u00d73 Array{Float64,2}:\n 0.681619  0.377511  0.846168\n 0.968485  0.238212  0.50264\n\njulia\n copy!(a, rand(size(a))) # set value by copying a Julia Array\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 0.311015  0.144441  0.464355\n 0.027956  0.361556  0.77293\n\njulia\n b = mx.empty(size(a))\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 4.03418f-36  7.9325f34   1.19414f-7\n 0.0          4.17261f-8  0.0\n\njulia\n b[:] = a                # copying and assignment between NDArrays\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 0.311015  0.144441  0.464355\n 0.027956  0.361556  0.77293\n\n\n\n\nNote due to the intrinsic design of the Julia language, a normal assignment\n\n\na = b\n\n\n\n\ndoes \nnot\n mean copying the contents of \nb\n to \na\n. Instead, it just make the variable \na\n pointing to a new object, which is \nb\n. Similarly, inplace arithmetics does not work as expected:\n\n\njulia\n using MXNet\n\njulia\n a = mx.ones(2)\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0\n\njulia\n r = a           # keep a reference to a\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0\n\njulia\n b = mx.ones(2)\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0\n\njulia\n a += b          # translates to a = a + b\n2-element mx.NDArray{Float32,1} @ CPU0:\n 2.0\n 2.0\n\njulia\n a\n2-element mx.NDArray{Float32,1} @ CPU0:\n 2.0\n 2.0\n\njulia\n r\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0\n\n\n\n\nAs we can see, \na\n has expected value, but instead of inplace updating, a new \nNDArray\n is created and \na\n is set to point to this new object. If we look at \nr\n, which still reference to the old \na\n, its content has not changed. There is currently no way in Julia to overload the operators like \n+=\n to get customized behavior.\n\n\nInstead, you will need to write \na[:] = a + b\n, or if you want \nreal\n inplace \n+=\n operation, MXNet.jl provides a simple macro \n@mx.inplace\n:\n\n\njulia\n @mx.inplace a += b\n2-element mx.NDArray{Float32,1} @ CPU0:\n 3.0\n 3.0\n\njulia\n macroexpand(:(@mx.inplace a += b))\n2-element mx.NDArray{Float32,1} @ CPU0:\n 4.0\n 4.0\n\n\n\n\nAs we can see, it translate the \n+=\n operator to an explicit \nadd_to!\n function call, which invokes into libmxnet to add the contents of \nb\n into \na\n directly. For example, the following is the update rule in the \nSGD Optimizer\n (both \ngrad\n and \nweight\n are \nNDArray\n objects):\n\n\n@inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)\n\n\n\n\nNote there is no much magic in \nmx.inplace\n: it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by \ngrad_scale\n and adding the weight decay all create temporary \nNDArray\n objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp \nNDArray\n vs. pre-allocating:\n\n\nusing Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op()))) \n 1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))\n\n\n\n\nThe comparison on my laptop shows that \nnormal_op\n while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing \nN_REP\n), is only about twice slower than the pre-allocated one.\n\n\n\n\n\n\n\n\nRow\n\n\nFunction\n\n\nAverage\n\n\nRelative\n\n\nReplications\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\"inplace_op\"\n\n\n0.0074854\n\n\n1.0\n\n\n100\n\n\n\n\n\n\n2\n\n\n\"normal_op\"\n\n\n0.0174202\n\n\n2.32723\n\n\n100\n\n\n\n\n\n\n\n\nSo it will usually not be a big problem unless you are at the bottleneck of the computation.\n\n\n\n\nDistributed Key-value Store\n\n\nThe type \nKVStore\n and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.\n\n\nThe following example shows how to create a local \nKVStore\n, initialize a value and then pull it back.\n\n\nkv    = mx.KVStore(:local)\nshape = (2, 3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape) * 2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\na\n\n\n\n\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 2.0  2.0  2.0\n 2.0  2.0  2.0\n\n\n\n\n\n\nIntermediate Level Interface\n\n\n\n\nSymbols and Composition\n\n\nThe way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like \nTheano\n, except that we avoided long expression compilation time by providing \nlarger\n neural network related building blocks to guarantee computation performance. See also \nthis note\n for the design and trade-off of the MXNet symbolic composition system.\n\n\nThe basic type is \nmx.SymbolicNode\n. The following is a trivial example of composing two symbols with the \n+\n operation.\n\n\nA = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B\nprint(C)  # debug printing\n\n\n\n\nSymbol Outputs:\n    output[0]=_plus0(0)\nVariable:A\nVariable:B\n--------------------\nOp:elemwise_add, Name=_plus0\nInputs:\n    arg[0]=A(0) version=0\n    arg[1]=B(0) version=0\n\n\n\n\nWe get a new \nSymbolicNode\n by composing existing \nSymbolicNode\ns by some \noperations\n. A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a \nReLU\n activation function.\n\n\nnet = mx.Variable(:data)\nnet = mx.FullyConnected(net, name=:fc1, num_hidden=128)\nnet = mx.Activation(net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(net, name=:fc2, num_hidden=64)\nnet = mx.SoftmaxOutput(net, name=:out)\nprint(net)  # debug printing\n\n\n\n\nSymbol Outputs:\n    output[0]=out(0)\nVariable:data\nVariable:fc1_weight\nVariable:fc1_bias\n--------------------\nOp:FullyConnected, Name=fc1\nInputs:\n    arg[0]=data(0) version=0\n    arg[1]=fc1_weight(0) version=0\n    arg[2]=fc1_bias(0) version=0\nAttrs:\n    num_hidden=128\n--------------------\nOp:Activation, Name=relu1\nInputs:\n    arg[0]=fc1(0)\nAttrs:\n    act_type=relu\nVariable:fc2_weight\nVariable:fc2_bias\n--------------------\nOp:FullyConnected, Name=fc2\nInputs:\n    arg[0]=relu1(0)\n    arg[1]=fc2_weight(0) version=0\n    arg[2]=fc2_bias(0) version=0\nAttrs:\n    num_hidden=64\nVariable:out_label\n--------------------\nOp:SoftmaxOutput, Name=out\nInputs:\n    arg[0]=fc2(0)\n    arg[1]=out_label(0) version=0\n\n\n\n\nEach time we take the previous symbol, and compose with an operation. Unlike the simple \n+\n example above, the \noperations\n here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.\n\n\nEach of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g. \nnum_hidden\n, \nact_type\n) to further customize the composition results.\n\n\nWhen applying those operations, we can also specify a \nname\n for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.\n\n\nEach symbol takes some arguments. For example, in the \n+\n case above, to compute the value of \nC\n, we will need to know the values of the two inputs \nA\n and \nB\n. For neural networks, the arguments are primarily two categories: \ninputs\n and \nparameters\n. \ninputs\n are data and labels for the networks, while \nparameters\n are typically trainable \nweights\n, \nbias\n, \nfilters\n.\n\n\nWhen composing symbols, their arguments accumulates. We can list all the arguments by\n\n\nmx.list_arguments(net)\n\n\n\n\n6-element Array{Symbol,1}:\n :data\n :fc1_weight\n :fc1_bias\n :fc2_weight\n :fc2_bias\n :out_label\n\n\n\n\nNote the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:\n\n\njulia\n using MXNet\n\njulia\n net = mx.Variable(:data)\nMXNet.mx.SymbolicNode data\n\njulia\n w   = mx.Variable(:myweight)\nMXNet.mx.SymbolicNode myweight\n\njulia\n net = mx.FullyConnected(net, weight=w, name=:fc1, num_hidden=128)\nMXNet.mx.SymbolicNode fc1\n\njulia\n mx.list_arguments(net)\n3-element Array{Symbol,1}:\n :data\n :myweight\n :fc1_bias\n\n\n\n\nThe simple fact is that a \nVariable\n is just a placeholder \nmx.SymbolicNode\n. In composition, we can use arbitrary symbols for arguments. For example:\n\n\njulia\n using MXNet\n\njulia\n net  = mx.Variable(:data)\nMXNet.mx.SymbolicNode data\n\njulia\n net  = mx.FullyConnected(net, name=:fc1, num_hidden=128)\nMXNet.mx.SymbolicNode fc1\n\njulia\n net2 = mx.Variable(:data2)\nMXNet.mx.SymbolicNode data2\n\njulia\n net2 = mx.FullyConnected(net2, name=:net2, num_hidden=128)\nMXNet.mx.SymbolicNode net2\n\njulia\n mx.list_arguments(net2)\n3-element Array{Symbol,1}:\n :data2\n :net2_weight\n :net2_bias\n\njulia\n composed_net = net2(data2=net, name=:composed)\nMXNet.mx.SymbolicNode composed\n\njulia\n mx.list_arguments(composed_net)\n5-element Array{Symbol,1}:\n :data\n :fc1_weight\n :fc1_bias\n :net2_weight\n :net2_bias\n\n\n\n\nNote we use a composed symbol, \nnet\n as the argument \ndata2\n for \nnet2\n to get a new symbol, which we named \n:composed\n. It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.\n\n\n\n\nShape Inference\n\n\nGiven enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like \nnum_hidden\n, the shapes for the weights and bias in a neural network could be inferred.\n\n\njulia\n using MXNet\n\njulia\n net = mx.Variable(:data)\nMXNet.mx.SymbolicNode data\n\njulia\n net = mx.FullyConnected(net, name=:fc1, num_hidden=10)\nMXNet.mx.SymbolicNode fc1\n\njulia\n arg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))\n(Tuple[(10, 64), (10, 10), (10,)], Tuple[(10, 64)], Tuple[])\n\n\n\n\nThe returned shapes corresponds to arguments with the same order as returned by \nmx.list_arguments\n. The \nout_shapes\n are shapes for outputs, and \naux_shapes\n can be safely ignored for now.\n\n\njulia\n for (n, s) in zip(mx.list_arguments(net), arg_shapes)\n         println(\n$n\\t=\n $s\n)\n       end\ndata    =\n (10, 64)\nfc1_weight  =\n (10, 10)\nfc1_bias    =\n (10,)\n\n\n\n\njulia\n for (n, s) in zip(mx.list_outputs(net), out_shapes)\n         println(\n$n\\t=\n $s\n)\n       end\nfc1_output  =\n (10, 64)\n\n\n\n\n\n\nBinding and Executing\n\n\nIn order to execute the computation graph specified a composed symbol, we will \nbind\n the free variables to concrete values, specified as \nmx.NDArray\n. This will create an \nmx.Executor\n on a given \nmx.Context\n. A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.\n\n\njulia\n using MXNet\n\njulia\n A = mx.Variable(:A)\nMXNet.mx.SymbolicNode A\n\njulia\n B = mx.Variable(:B)\nMXNet.mx.SymbolicNode B\n\njulia\n C = A .* B\nMXNet.mx.SymbolicNode _mul0\n\njulia\n a = mx.ones(3) * 4\n3-element mx.NDArray{Float32,1} @ CPU0:\n 4.0\n 4.0\n 4.0\n\njulia\n b = mx.ones(3) * 2\n3-element mx.NDArray{Float32,1} @ CPU0:\n 2.0\n 2.0\n 2.0\n\njulia\n c_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =\n a, :B =\n b));\n\njulia\n mx.forward(c_exec)\n1-element Array{MXNet.mx.NDArray{Float32,1},1}:\n NDArray Float32[8.0, 8.0, 8.0]\n\njulia\n c_exec.outputs[1]\n3-element mx.NDArray{Float32,1} @ CPU0:\n 8.0\n 8.0\n 8.0\n\njulia\n copy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n3-element Array{Float32,1}:\n 8.0\n 8.0\n 8.0\n\n\n\n\nFor neural networks, it is easier to use \nsimple_bind\n. By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the \nModel\n interface.\n\n\nTODO\n Provide pointers to model tutorial and further details about binding and symbolic API.\n\n\n\n\nHigh Level Interface\n\n\nThe high level interface include model training and prediction API, etc.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#mxnetjl-namespace", 
            "text": "Most the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a  mx  module. The convention of accessing the MXNet.jl interface is the to use the  mx.  prefix explicitly:  julia  using MXNet\n\njulia  x = mx.zeros(2, 3)             # MXNet NDArray\n2\u00d73 mx.NDArray{Float32} @ CPU0:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia  y = zeros(eltype(x), size(x))  # Julia Array\n2\u00d73 Array{Float32,2}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia  copy!(y, x)                    # Overloaded function in Julia Base\n2\u00d73 Array{Float32,2}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia  z = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\n2\u00d73 mx.NDArray{Float32} @ GPU0:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia  mx.copy!(z, y)                 # Same as copy!(z, y)\n2\u00d73 mx.NDArray{Float32} @ GPU0:\n 0.0  0.0  0.0\n 0.0  0.0  0.0  Note functions like  size ,  copy!  that is extensively overloaded for various types works out of the box. But functions like  zeros  and  ones  will be ambiguous, so we always use the  mx.  prefix. If you prefer, the  mx.  prefix can be used explicitly for all MXNet.jl functions, including  size  and  copy!  as shown in the last line.", 
            "title": "MXNet.jl Namespace"
        }, 
        {
            "location": "/user-guide/overview/#low-level-interface", 
            "text": "", 
            "title": "Low Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#ndarray", 
            "text": "NDArray  is the basic building blocks of the actual computations in MXNet. It is like a Julia  Array  object, with some important differences listed here:   The actual data could live on different  Context  (e.g. GPUs). For some contexts, iterating into the elements one by one is very slow, thus indexing into NDArray is not recommanded in general. The easiest way to inspect the contents of an NDArray is to use the  copy  function to copy the contents as a Julia  Array .  Operations on  NDArray  (including basic arithmetics and neural network related operators) are executed in parallel with automatic dependency tracking to ensure correctness.  There is no generics in  NDArray , the  eltype  is always  mx.MX_float . Because for applications in machine learning, single precision floating point numbers are typical a best choice balancing between precision, speed and portability. Also since libmxnet is designed to support multiple languages as front-ends, it is much simpler to implement with a fixed data type.   While most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the  NDArray  API is useful for implementing  Optimizer  or customized operators in Julia directly.  The followings are common ways to create  NDArray  objects:   mx.empty(shape[, context]) : create on uninitialized array of a given shape on a specific device. For example,  mx.empty(2, 3) ,  mx.((2, 3), mx.gpu(2)) .  mx.zeros(shape[, context])  and  mx.ones(shape[, context]) : similar to the Julia's built-in  zeros  and  ones .  mx.copy(jl_arr, context) : copy the contents of a Julia  Array  to a specific device.   Most of the convenient functions like  size ,  length ,  ndims ,  eltype  on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take  slices :  julia  using MXNet\n\njulia  a = mx.ones(2, 3)\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia  b = mx.slice(a, 1:2)\n2\u00d72 mx.NDArray{Float32,2} @ CPU0:\n 1.0  1.0\n 1.0  1.0\n\njulia  b[:] = 2\n2\n\njulia  a\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 2.0  2.0  1.0\n 2.0  2.0  1.0  A slice is a sub-region sharing the same memory with the original  NDArray  object. A slice is always a contiguous piece of memory, so only slicing on the  last  dimension is supported. The example above also shows a way to set the contents of an  NDArray .  julia  using MXNet\n\njulia  mx.srand(42)\n\njulia  a = mx.empty(2, 3)\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 2.04913f-36  1.79751f19  6.86636f-44\n 0.0          1.06216f-5  0.0\n\njulia  a[:] = 0.5              # set all elements to a scalar\n0.5\n\njulia  a[:] = rand(size(a))    # set contents with a Julia Array\n2\u00d73 Array{Float64,2}:\n 0.681619  0.377511  0.846168\n 0.968485  0.238212  0.50264\n\njulia  copy!(a, rand(size(a))) # set value by copying a Julia Array\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 0.311015  0.144441  0.464355\n 0.027956  0.361556  0.77293\n\njulia  b = mx.empty(size(a))\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 4.03418f-36  7.9325f34   1.19414f-7\n 0.0          4.17261f-8  0.0\n\njulia  b[:] = a                # copying and assignment between NDArrays\n2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 0.311015  0.144441  0.464355\n 0.027956  0.361556  0.77293  Note due to the intrinsic design of the Julia language, a normal assignment  a = b  does  not  mean copying the contents of  b  to  a . Instead, it just make the variable  a  pointing to a new object, which is  b . Similarly, inplace arithmetics does not work as expected:  julia  using MXNet\n\njulia  a = mx.ones(2)\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0\n\njulia  r = a           # keep a reference to a\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0\n\njulia  b = mx.ones(2)\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0\n\njulia  a += b          # translates to a = a + b\n2-element mx.NDArray{Float32,1} @ CPU0:\n 2.0\n 2.0\n\njulia  a\n2-element mx.NDArray{Float32,1} @ CPU0:\n 2.0\n 2.0\n\njulia  r\n2-element mx.NDArray{Float32,1} @ CPU0:\n 1.0\n 1.0  As we can see,  a  has expected value, but instead of inplace updating, a new  NDArray  is created and  a  is set to point to this new object. If we look at  r , which still reference to the old  a , its content has not changed. There is currently no way in Julia to overload the operators like  +=  to get customized behavior.  Instead, you will need to write  a[:] = a + b , or if you want  real  inplace  +=  operation, MXNet.jl provides a simple macro  @mx.inplace :  julia  @mx.inplace a += b\n2-element mx.NDArray{Float32,1} @ CPU0:\n 3.0\n 3.0\n\njulia  macroexpand(:(@mx.inplace a += b))\n2-element mx.NDArray{Float32,1} @ CPU0:\n 4.0\n 4.0  As we can see, it translate the  +=  operator to an explicit  add_to!  function call, which invokes into libmxnet to add the contents of  b  into  a  directly. For example, the following is the update rule in the  SGD Optimizer  (both  grad  and  weight  are  NDArray  objects):  @inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)  Note there is no much magic in  mx.inplace : it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by  grad_scale  and adding the weight decay all create temporary  NDArray  objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp  NDArray  vs. pre-allocating:  using Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op())))   1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))  The comparison on my laptop shows that  normal_op  while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing  N_REP ), is only about twice slower than the pre-allocated one.     Row  Function  Average  Relative  Replications      1  \"inplace_op\"  0.0074854  1.0  100    2  \"normal_op\"  0.0174202  2.32723  100     So it will usually not be a big problem unless you are at the bottleneck of the computation.", 
            "title": "NDArray"
        }, 
        {
            "location": "/user-guide/overview/#distributed-key-value-store", 
            "text": "The type  KVStore  and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.  The following example shows how to create a local  KVStore , initialize a value and then pull it back.  kv    = mx.KVStore(:local)\nshape = (2, 3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape) * 2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\na  2\u00d73 mx.NDArray{Float32,2} @ CPU0:\n 2.0  2.0  2.0\n 2.0  2.0  2.0", 
            "title": "Distributed Key-value Store"
        }, 
        {
            "location": "/user-guide/overview/#intermediate-level-interface", 
            "text": "", 
            "title": "Intermediate Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#symbols-and-composition", 
            "text": "The way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like  Theano , except that we avoided long expression compilation time by providing  larger  neural network related building blocks to guarantee computation performance. See also  this note  for the design and trade-off of the MXNet symbolic composition system.  The basic type is  mx.SymbolicNode . The following is a trivial example of composing two symbols with the  +  operation.  A = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B\nprint(C)  # debug printing  Symbol Outputs:\n    output[0]=_plus0(0)\nVariable:A\nVariable:B\n--------------------\nOp:elemwise_add, Name=_plus0\nInputs:\n    arg[0]=A(0) version=0\n    arg[1]=B(0) version=0  We get a new  SymbolicNode  by composing existing  SymbolicNode s by some  operations . A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a  ReLU  activation function.  net = mx.Variable(:data)\nnet = mx.FullyConnected(net, name=:fc1, num_hidden=128)\nnet = mx.Activation(net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(net, name=:fc2, num_hidden=64)\nnet = mx.SoftmaxOutput(net, name=:out)\nprint(net)  # debug printing  Symbol Outputs:\n    output[0]=out(0)\nVariable:data\nVariable:fc1_weight\nVariable:fc1_bias\n--------------------\nOp:FullyConnected, Name=fc1\nInputs:\n    arg[0]=data(0) version=0\n    arg[1]=fc1_weight(0) version=0\n    arg[2]=fc1_bias(0) version=0\nAttrs:\n    num_hidden=128\n--------------------\nOp:Activation, Name=relu1\nInputs:\n    arg[0]=fc1(0)\nAttrs:\n    act_type=relu\nVariable:fc2_weight\nVariable:fc2_bias\n--------------------\nOp:FullyConnected, Name=fc2\nInputs:\n    arg[0]=relu1(0)\n    arg[1]=fc2_weight(0) version=0\n    arg[2]=fc2_bias(0) version=0\nAttrs:\n    num_hidden=64\nVariable:out_label\n--------------------\nOp:SoftmaxOutput, Name=out\nInputs:\n    arg[0]=fc2(0)\n    arg[1]=out_label(0) version=0  Each time we take the previous symbol, and compose with an operation. Unlike the simple  +  example above, the  operations  here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.  Each of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g.  num_hidden ,  act_type ) to further customize the composition results.  When applying those operations, we can also specify a  name  for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.  Each symbol takes some arguments. For example, in the  +  case above, to compute the value of  C , we will need to know the values of the two inputs  A  and  B . For neural networks, the arguments are primarily two categories:  inputs  and  parameters .  inputs  are data and labels for the networks, while  parameters  are typically trainable  weights ,  bias ,  filters .  When composing symbols, their arguments accumulates. We can list all the arguments by  mx.list_arguments(net)  6-element Array{Symbol,1}:\n :data\n :fc1_weight\n :fc1_bias\n :fc2_weight\n :fc2_bias\n :out_label  Note the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:  julia  using MXNet\n\njulia  net = mx.Variable(:data)\nMXNet.mx.SymbolicNode data\n\njulia  w   = mx.Variable(:myweight)\nMXNet.mx.SymbolicNode myweight\n\njulia  net = mx.FullyConnected(net, weight=w, name=:fc1, num_hidden=128)\nMXNet.mx.SymbolicNode fc1\n\njulia  mx.list_arguments(net)\n3-element Array{Symbol,1}:\n :data\n :myweight\n :fc1_bias  The simple fact is that a  Variable  is just a placeholder  mx.SymbolicNode . In composition, we can use arbitrary symbols for arguments. For example:  julia  using MXNet\n\njulia  net  = mx.Variable(:data)\nMXNet.mx.SymbolicNode data\n\njulia  net  = mx.FullyConnected(net, name=:fc1, num_hidden=128)\nMXNet.mx.SymbolicNode fc1\n\njulia  net2 = mx.Variable(:data2)\nMXNet.mx.SymbolicNode data2\n\njulia  net2 = mx.FullyConnected(net2, name=:net2, num_hidden=128)\nMXNet.mx.SymbolicNode net2\n\njulia  mx.list_arguments(net2)\n3-element Array{Symbol,1}:\n :data2\n :net2_weight\n :net2_bias\n\njulia  composed_net = net2(data2=net, name=:composed)\nMXNet.mx.SymbolicNode composed\n\njulia  mx.list_arguments(composed_net)\n5-element Array{Symbol,1}:\n :data\n :fc1_weight\n :fc1_bias\n :net2_weight\n :net2_bias  Note we use a composed symbol,  net  as the argument  data2  for  net2  to get a new symbol, which we named  :composed . It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.", 
            "title": "Symbols and Composition"
        }, 
        {
            "location": "/user-guide/overview/#shape-inference", 
            "text": "Given enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like  num_hidden , the shapes for the weights and bias in a neural network could be inferred.  julia  using MXNet\n\njulia  net = mx.Variable(:data)\nMXNet.mx.SymbolicNode data\n\njulia  net = mx.FullyConnected(net, name=:fc1, num_hidden=10)\nMXNet.mx.SymbolicNode fc1\n\njulia  arg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))\n(Tuple[(10, 64), (10, 10), (10,)], Tuple[(10, 64)], Tuple[])  The returned shapes corresponds to arguments with the same order as returned by  mx.list_arguments . The  out_shapes  are shapes for outputs, and  aux_shapes  can be safely ignored for now.  julia  for (n, s) in zip(mx.list_arguments(net), arg_shapes)\n         println( $n\\t=  $s )\n       end\ndata    =  (10, 64)\nfc1_weight  =  (10, 10)\nfc1_bias    =  (10,)  julia  for (n, s) in zip(mx.list_outputs(net), out_shapes)\n         println( $n\\t=  $s )\n       end\nfc1_output  =  (10, 64)", 
            "title": "Shape Inference"
        }, 
        {
            "location": "/user-guide/overview/#binding-and-executing", 
            "text": "In order to execute the computation graph specified a composed symbol, we will  bind  the free variables to concrete values, specified as  mx.NDArray . This will create an  mx.Executor  on a given  mx.Context . A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.  julia  using MXNet\n\njulia  A = mx.Variable(:A)\nMXNet.mx.SymbolicNode A\n\njulia  B = mx.Variable(:B)\nMXNet.mx.SymbolicNode B\n\njulia  C = A .* B\nMXNet.mx.SymbolicNode _mul0\n\njulia  a = mx.ones(3) * 4\n3-element mx.NDArray{Float32,1} @ CPU0:\n 4.0\n 4.0\n 4.0\n\njulia  b = mx.ones(3) * 2\n3-element mx.NDArray{Float32,1} @ CPU0:\n 2.0\n 2.0\n 2.0\n\njulia  c_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =  a, :B =  b));\n\njulia  mx.forward(c_exec)\n1-element Array{MXNet.mx.NDArray{Float32,1},1}:\n NDArray Float32[8.0, 8.0, 8.0]\n\njulia  c_exec.outputs[1]\n3-element mx.NDArray{Float32,1} @ CPU0:\n 8.0\n 8.0\n 8.0\n\njulia  copy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n3-element Array{Float32,1}:\n 8.0\n 8.0\n 8.0  For neural networks, it is easier to use  simple_bind . By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the  Model  interface.  TODO  Provide pointers to model tutorial and further details about binding and symbolic API.", 
            "title": "Binding and Executing"
        }, 
        {
            "location": "/user-guide/overview/#high-level-interface", 
            "text": "The high level interface include model training and prediction API, etc.", 
            "title": "High Level Interface"
        }, 
        {
            "location": "/user-guide/faq/", 
            "text": "FAQ\n\n\n\n\nRunning MXNet on AWS GPU instances\n\n\nSee the discussions and notes \nhere\n.", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#faq", 
            "text": "", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#running-mxnet-on-aws-gpu-instances", 
            "text": "See the discussions and notes  here .", 
            "title": "Running MXNet on AWS GPU instances"
        }, 
        {
            "location": "/api/context/", 
            "text": "Context\n\n\n#\n\n\nMXNet.mx.Context\n \n \nType\n.\n\n\nContext(dev_type, dev_id)\n\n\n\n\nA context describes the device type and id on which computation should be carried on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cpu\n \n \nFunction\n.\n\n\ncpu(dev_id)\n\n\n\n\nGet a CPU context with a specific id. \ncpu()\n is usually the default context for many operations when no context is specified.\n\n\nArguments\n\n\n\n\ndev_id::Int = 0\n: the CPU id.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gpu\n \n \nFunction\n.\n\n\ngpu(dev_id)\n\n\n\n\nGet a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.\n\n\nArguments\n\n\n\n\ndev_id :: Int = 0\n the GPU device id.\n\n\n\n\nsource", 
            "title": "Context"
        }, 
        {
            "location": "/api/context/#context", 
            "text": "#  MXNet.mx.Context     Type .  Context(dev_type, dev_id)  A context describes the device type and id on which computation should be carried on.  source  #  MXNet.mx.cpu     Function .  cpu(dev_id)  Get a CPU context with a specific id.  cpu()  is usually the default context for many operations when no context is specified.  Arguments   dev_id::Int = 0 : the CPU id.   source  #  MXNet.mx.gpu     Function .  gpu(dev_id)  Get a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.  Arguments   dev_id :: Int = 0  the GPU device id.   source", 
            "title": "Context"
        }, 
        {
            "location": "/api/model/", 
            "text": "Model\n\n\nThe model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.\n\n\n#\n\n\nMXNet.mx.AbstractModel\n \n \nType\n.\n\n\nAbstractModel\n\n\n\n\nThe abstract super type of all models in MXNet.jl.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nType\n.\n\n\nFeedForward\n\n\n\n\nThe feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of \ntime index\n, but it is relatively easy to implement unrolled RNN / LSTM under this framework (\nTODO\n: add example). For models that handles sequential data explicitly, please use \nTODO\n...\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nMethod\n.\n\n\nFeedForward(arch :: SymbolicNode, ctx)\n\n\n\n\nArguments:\n\n\n\n\narch\n: the architecture of the network constructed using the symbolic API.\n\n\nctx\n: the devices on which this model should do computation. It could be a single \nContext\n        or a list of \nContext\n objects. In the latter case, data parallelization will be used        for training. If no context is provided, the default context \ncpu()\n will be used.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.predict\n \n \nMethod\n.\n\n\npredict(self, data; overwrite=false, callback=nothing)\n\n\n\n\nPredict using an existing model. The model should be already initialized, or trained or loaded from a checkpoint. There is an overloaded function that allows to pass the callback as the first argument, so it is possible to do\n\n\npredict(model, data) do batch_output\n  # consume or write batch_output to file\nend\n\n\n\n\nArguments:\n\n\n\n\nself::FeedForward\n:  the model.\n\n\ndata::AbstractDataProvider\n: the data to perform prediction on.\n\n\noverwrite::Bool\n: an \nExecutor\n is initialized the first time predict is called. The memory                    allocation of the \nExecutor\n depends on the mini-batch size of the test                    data provider. If you call predict twice with data provider of the same batch-size,                    then the executor can be potentially be re-used. So, if \noverwrite\n is false,                    we will try to re-use, and raise an error if batch-size changed. If \noverwrite\n                    is true (the default), a new \nExecutor\n will be created to replace the old one.\n\n\nverbosity::Integer\n: Determines the verbosity of the print messages. Higher numbers         leads to more verbose printing. Acceptable values are         - \n0\n: Do not print anything during prediction         - \n1\n: Print allocation information during prediction\n\n\n\n\n\n\nNote\n\n\nPrediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO for copying mini-batches of data. Since there is no concern about convergence in prediction, it is better to set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a concern.\n\n\nFor the same reason, currently prediction will only use the first device even if multiple devices are provided to construct the model.\n\n\n\n\n\n\nNote\n\n\nIf you perform further after prediction. The weights are not automatically synchronized if \noverwrite\n is set to false and the old predictor is re-used. In this case setting \noverwrite\n to true (the default) will re-initialize the predictor the next time you call predict and synchronize the weights again.\n\n\n\n\nSee also \ntrain\n, \nfit\n, \ninit_model\n, and \nload_checkpoint\n\n\nsource\n\n\n#\n\n\nMXNet.mx._split_inputs\n \n \nMethod\n.\n\n\nGet a split of \nbatch_size\n into \nn_split\n pieces for data parallelization. Returns a vector of length \nn_split\n, with each entry a \nUnitRange{Int}\n indicating the slice index for that piece.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fit\n \n \nMethod\n.\n\n\nfit(model :: FeedForward, optimizer, data; kwargs...)\n\n\n\n\nTrain the \nmodel\n on \ndata\n with the \noptimizer\n.\n\n\n\n\nmodel::FeedForward\n: the model to be trained.\n\n\noptimizer::AbstractOptimizer\n: the optimization algorithm to use.\n\n\ndata::AbstractDataProvider\n: the training data provider.\n\n\nn_epoch::Int\n: default 10, the number of full data-passes to run.\n\n\neval_data::AbstractDataProvider\n: keyword argument, default \nnothing\n. The data provider for         the validation set.\n\n\neval_metric::AbstractEvalMetric\n: keyword argument, default \nAccuracy()\n. The metric used         to evaluate the training performance. If \neval_data\n is provided, the same metric is also         calculated on the validation set.\n\n\nkvstore\n: keyword argument, default \n:local\n. The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore: \nKVStore\n or \nSymbol\n\n\ninitializer::AbstractInitializer\n: keyword argument, default \nUniformInitializer(0.01)\n.\n\n\nforce_init::Bool\n: keyword argument, default false. By default, the random initialization using the         provided \ninitializer\n will be skipped if the model weights already exists, maybe from a previous         call to \ntrain\n or an explicit call to \ninit_model\n or \nload_checkpoint\n. When         this option is set, it will always do random initialization at the begining of training.\n\n\ncallbacks::Vector{AbstractCallback}\n: keyword argument, default \n[]\n. Callbacks to be invoked at each epoch or mini-batch,         see \nAbstractCallback\n.\n\n\nverbosity::Int\n: Determines the verbosity of the print messages. Higher numbers         leads to more verbose printing. Acceptable values are         - \n0\n: Do not print anything during training         - \n1\n: Print starting and final messages         - \n2\n: Print one time messages and a message at the start of each epoch         - \n3\n: Print a summary of the training and validation accuracy for each epoch\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.init_model\n \n \nMethod\n.\n\n\ninit_model(self, initializer; overwrite=false, input_shapes...)\n\n\n\n\nInitialize the weights in the model.\n\n\nThis method will be called automatically when training a model. So there is usually no need to call this method unless one needs to inspect a model with only randomly initialized weights.\n\n\nArguments:\n\n\n\n\nself::FeedForward\n: the model to be initialized.\n\n\ninitializer::AbstractInitializer\n: an initializer describing how the weights should be initialized.\n\n\noverwrite::Bool\n: keyword argument, force initialization even when weights already exists.\n\n\ninput_shapes\n: the shape of all data and label inputs to this model, given as keyword arguments.                 For example, \ndata=(28,28,1,100), label=(100,)\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_checkpoint\n \n \nMethod\n.\n\n\nload_checkpoint(prefix, epoch, ::mx.FeedForward; context)\n\n\n\n\nLoad a mx.FeedForward model from the checkpoint \nprefix\n, \nepoch\n and optionally provide a context.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.train\n \n \nMethod\n.\n\n\ntrain(model :: FeedForward, ...)\n\n\n\n\nAlias to \nfit\n.\n\n\nsource", 
            "title": "Models"
        }, 
        {
            "location": "/api/model/#model", 
            "text": "The model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.  #  MXNet.mx.AbstractModel     Type .  AbstractModel  The abstract super type of all models in MXNet.jl.  source  #  MXNet.mx.FeedForward     Type .  FeedForward  The feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of  time index , but it is relatively easy to implement unrolled RNN / LSTM under this framework ( TODO : add example). For models that handles sequential data explicitly, please use  TODO ...  source  #  MXNet.mx.FeedForward     Method .  FeedForward(arch :: SymbolicNode, ctx)  Arguments:   arch : the architecture of the network constructed using the symbolic API.  ctx : the devices on which this model should do computation. It could be a single  Context         or a list of  Context  objects. In the latter case, data parallelization will be used        for training. If no context is provided, the default context  cpu()  will be used.   source  #  MXNet.mx.predict     Method .  predict(self, data; overwrite=false, callback=nothing)  Predict using an existing model. The model should be already initialized, or trained or loaded from a checkpoint. There is an overloaded function that allows to pass the callback as the first argument, so it is possible to do  predict(model, data) do batch_output\n  # consume or write batch_output to file\nend  Arguments:   self::FeedForward :  the model.  data::AbstractDataProvider : the data to perform prediction on.  overwrite::Bool : an  Executor  is initialized the first time predict is called. The memory                    allocation of the  Executor  depends on the mini-batch size of the test                    data provider. If you call predict twice with data provider of the same batch-size,                    then the executor can be potentially be re-used. So, if  overwrite  is false,                    we will try to re-use, and raise an error if batch-size changed. If  overwrite                     is true (the default), a new  Executor  will be created to replace the old one.  verbosity::Integer : Determines the verbosity of the print messages. Higher numbers         leads to more verbose printing. Acceptable values are         -  0 : Do not print anything during prediction         -  1 : Print allocation information during prediction    Note  Prediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO for copying mini-batches of data. Since there is no concern about convergence in prediction, it is better to set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a concern.  For the same reason, currently prediction will only use the first device even if multiple devices are provided to construct the model.    Note  If you perform further after prediction. The weights are not automatically synchronized if  overwrite  is set to false and the old predictor is re-used. In this case setting  overwrite  to true (the default) will re-initialize the predictor the next time you call predict and synchronize the weights again.   See also  train ,  fit ,  init_model , and  load_checkpoint  source  #  MXNet.mx._split_inputs     Method .  Get a split of  batch_size  into  n_split  pieces for data parallelization. Returns a vector of length  n_split , with each entry a  UnitRange{Int}  indicating the slice index for that piece.  source  #  MXNet.mx.fit     Method .  fit(model :: FeedForward, optimizer, data; kwargs...)  Train the  model  on  data  with the  optimizer .   model::FeedForward : the model to be trained.  optimizer::AbstractOptimizer : the optimization algorithm to use.  data::AbstractDataProvider : the training data provider.  n_epoch::Int : default 10, the number of full data-passes to run.  eval_data::AbstractDataProvider : keyword argument, default  nothing . The data provider for         the validation set.  eval_metric::AbstractEvalMetric : keyword argument, default  Accuracy() . The metric used         to evaluate the training performance. If  eval_data  is provided, the same metric is also         calculated on the validation set.  kvstore : keyword argument, default  :local . The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore:  KVStore  or  Symbol  initializer::AbstractInitializer : keyword argument, default  UniformInitializer(0.01) .  force_init::Bool : keyword argument, default false. By default, the random initialization using the         provided  initializer  will be skipped if the model weights already exists, maybe from a previous         call to  train  or an explicit call to  init_model  or  load_checkpoint . When         this option is set, it will always do random initialization at the begining of training.  callbacks::Vector{AbstractCallback} : keyword argument, default  [] . Callbacks to be invoked at each epoch or mini-batch,         see  AbstractCallback .  verbosity::Int : Determines the verbosity of the print messages. Higher numbers         leads to more verbose printing. Acceptable values are         -  0 : Do not print anything during training         -  1 : Print starting and final messages         -  2 : Print one time messages and a message at the start of each epoch         -  3 : Print a summary of the training and validation accuracy for each epoch   source  #  MXNet.mx.init_model     Method .  init_model(self, initializer; overwrite=false, input_shapes...)  Initialize the weights in the model.  This method will be called automatically when training a model. So there is usually no need to call this method unless one needs to inspect a model with only randomly initialized weights.  Arguments:   self::FeedForward : the model to be initialized.  initializer::AbstractInitializer : an initializer describing how the weights should be initialized.  overwrite::Bool : keyword argument, force initialization even when weights already exists.  input_shapes : the shape of all data and label inputs to this model, given as keyword arguments.                 For example,  data=(28,28,1,100), label=(100,) .   source  #  MXNet.mx.load_checkpoint     Method .  load_checkpoint(prefix, epoch, ::mx.FeedForward; context)  Load a mx.FeedForward model from the checkpoint  prefix ,  epoch  and optionally provide a context.  source  #  MXNet.mx.train     Method .  train(model :: FeedForward, ...)  Alias to  fit .  source", 
            "title": "Model"
        }, 
        {
            "location": "/api/initializer/", 
            "text": "Initializer\n\n\n#\n\n\nMXNet.mx.AbstractInitializer\n \n \nType\n.\n\n\nAbstractInitializer\n\n\n\n\nThe abstract base class for all initializers.\n\n\nTo define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:\n\n\n_init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nOr, if full behavior customization is needed, override the following function\n\n\ninit(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nType\n.\n\n\nNormalInitializer\n\n\n\n\nInitialize weights according to a univariate Gaussian distribution.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nMethod\n.\n\n\nNormalInitializer(; mu=0, sigma=0.01)\n\n\n\n\nConstruct a \nNormalInitializer\n with mean \nmu\n and variance \nsigma\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nType\n.\n\n\nUniformInitializer\n\n\n\n\nInitialize weights according to a uniform distribution within the provided scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nMethod\n.\n\n\nUniformInitializer(scale=0.07)\n\n\n\n\nConstruct a \nUniformInitializer\n with the specified scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.XavierDistribution\n \n \nType\n.\n\n\nXavierInitializer\n\n\n\n\nThe initializer documented in the paper [Bengio and Glorot 2010]: \nUnderstanding the difficulty of training deep feedforward neuralnetworks\n.\n\n\nThere are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.\n\n\nSeveral different ways of calculating the variance are given in the literature or are used by various libraries.\n\n\n\n\n[Bengio and Glorot 2010]: \nmx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)\n\n\n[K. He, X. Zhang, S. Ren, and J. Sun 2015]: \nmx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)\n\n\ncaffe_avg: \nmx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)\n\n\n\n\nsource", 
            "title": "Initializers"
        }, 
        {
            "location": "/api/initializer/#initializer", 
            "text": "#  MXNet.mx.AbstractInitializer     Type .  AbstractInitializer  The abstract base class for all initializers.  To define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:  _init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  Or, if full behavior customization is needed, override the following function  init(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  source  #  MXNet.mx.NormalInitializer     Type .  NormalInitializer  Initialize weights according to a univariate Gaussian distribution.  source  #  MXNet.mx.NormalInitializer     Method .  NormalInitializer(; mu=0, sigma=0.01)  Construct a  NormalInitializer  with mean  mu  and variance  sigma .  source  #  MXNet.mx.UniformInitializer     Type .  UniformInitializer  Initialize weights according to a uniform distribution within the provided scale.  source  #  MXNet.mx.UniformInitializer     Method .  UniformInitializer(scale=0.07)  Construct a  UniformInitializer  with the specified scale.  source  #  MXNet.mx.XavierDistribution     Type .  XavierInitializer  The initializer documented in the paper [Bengio and Glorot 2010]:  Understanding the difficulty of training deep feedforward neuralnetworks .  There are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.  Several different ways of calculating the variance are given in the literature or are used by various libraries.   [Bengio and Glorot 2010]:  mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)  [K. He, X. Zhang, S. Ren, and J. Sun 2015]:  mx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)  caffe_avg:  mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)   source", 
            "title": "Initializer"
        }, 
        {
            "location": "/api/optimizer/", 
            "text": "Optimizers\n\n\n#\n\n\nMXNet.mx.AbstractOptimizer\n \n \nType\n.\n\n\nAbstractOptimizer\n\n\n\n\nBase type for all optimizers.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractLearningRateScheduler\n \n \nType\n.\n\n\nAbstractLearningRateScheduler\n\n\n\n\nBase type for all learning rate scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractMomentumScheduler\n \n \nType\n.\n\n\nAbstractMomentumScheduler\n\n\n\n\nBase type for all momentum scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractOptimizerOptions\n \n \nType\n.\n\n\nAbstractOptimizerOptions\n\n\n\n\nBase class for all optimizer options.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.OptimizationState\n \n \nType\n.\n\n\nOptimizationState\n\n\n\n\nAttributes:\n\n\n\n\nbatch_size\n: The size of the mini-batch used in stochastic training.\n\n\ncurr_epoch\n: The current epoch count. Epoch 0 means no training yet, during the first pass through the data, the epoch will be 1; during the second pass, the epoch count will be 1, and so on.\n\n\ncurr_batch\n: The current mini-batch count. The batch count is reset during every epoch. The batch count 0 means the beginning of each epoch, with no mini-batch seen yet. During the first mini-batch, the mini-batch count will be 1.\n\n\ncurr_iter\n: The current iteration count. One iteration corresponds to one mini-batch, but unlike the mini-batch count, the iteration count does \nnot\n reset in each epoch. So it track the \ntotal\n number of mini-batches seen so far.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_learning_rate\n \n \nFunction\n.\n\n\nget_learning_rate(scheduler, state)\n\n\n\n\nArguments\n\n\n\n\nscheduler::AbstractLearningRateScheduler\n: a learning rate scheduler.\n\n\nstate::OptimizationState\n: the current state about epoch, mini-batch and iteration count.\n\n\n\n\nReturns the current learning rate.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_momentum\n \n \nFunction\n.\n\n\nget_momentum(scheduler, state)\n\n\n\n\n\n\nscheduler::AbstractMomentumScheduler\n: the momentum scheduler.\n\n\nstate::OptimizationState\n: the state about current epoch, mini-batch and iteration count.\n\n\n\n\nReturns the current momentum.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_updater\n \n \nMethod\n.\n\n\nget_updater(optimizer)\n\n\n\n\nA utility function to create an updater function, that uses its closure to store all the states needed for each weights.\n\n\n\n\noptimizer::AbstractOptimizer\n: the underlying optimizer.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normalized_gradient\n \n \nMethod\n.\n\n\nnormalized_gradient(opts, state, W, \u2207)\n\n\n\n\nGet the properly normalized gradient (re-scaled and clipped if necessary).\n\n\n\n\nopts::AbstractOptimizerOptions\n: options for the optimizer, should contain the field \ngrad_clip\n and \nweight_decay\n.\n\n\nstate::OptimizationState\n: the current optimization state.\n\n\nW::NDArray\n: the trainable weights.\n\n\n\u2207::NDArray\n: the original gradient of the weights.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Exp\n \n \nType\n.\n\n\nLearningRate.Exp\n\n\n\n\n$\u001bta_t = \u001bta_0gamma^t$. Here $t$ is the epoch count, or the iteration count if \ndecay_on_iteration\n is set to true.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Fixed\n \n \nType\n.\n\n\nLearningRate.Fixed\n\n\n\n\nFixed learning rate scheduler always return the same learning rate.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Inv\n \n \nType\n.\n\n\nLearningRate.Inv\n\n\n\n\n$\u001bta_t = \u001bta_0 * (1 + gamma * t)^(-power)$. Here $t$ is the epoch count, or the iteration count if \ndecay_on_iteration\n is set to true.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.Fixed\n \n \nType\n.\n\n\nMomentum.Fixed\n\n\n\n\nFixed momentum scheduler always returns the same value.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.NadamScheduler\n \n \nType\n.\n\n\nMomentum.NadamScheduler\n\n\n\n\nNesterov-accelerated adaptive momentum scheduler.\n\n\nDescription in \"Incorporating Nesterov Momentum into Adam.\" \nhttp://cs229.stanford.edu/proj2015/054_report.pdf\n\n\n$mu_t = mu_0 * (1 - gamma * \u0007lpha^{t * delta})$. Here\n\n\n\n\n$t$ is the iteration count\n\n\n$delta$: default \n0.004\n is scheduler decay,\n\n\n$gamma$: default \n0.5\n\n\n$\u0007lpha$: default \n0.96\n\n\n$mu_0$: default \n0.99\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.Null\n \n \nType\n.\n\n\nMomentum.Null\n\n\n\n\nThe null momentum scheduler always returns 0 for momentum. It is also used to explicitly indicate momentum should not be used.\n\n\nsource\n\n\n\n\nBuilt-in optimizers\n\n\n\n\nStochastic Gradient Descent\n\n\n#\n\n\nMXNet.mx.SGD\n \n \nType\n.\n\n\nSGD\n\n\n\n\nStochastic gradient descent optimizer.\n\n\nSGD(; kwargs...)\n\n\n\n\nArguments:\n\n\n\n\nlr::Real\n: default \n0.01\n, learning rate.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a      dynamic learning rate scheduler. If set, will overwrite the \nlr\n      parameter.\n\n\nmomentum::Real\n: default \n0.0\n, the momentum.\n\n\nmomentum_scheduler::AbstractMomentumScheduler\n: default \nnothing\n,      a dynamic momentum scheduler. If set, will overwrite the \nmomentum\n      parameter.\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient      into the bounded range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.0001\n, weight decay is equivalent to      adding a global l2 regularizer to the parameters.\n\n\n\n\nsource\n\n\n\n\nADAM\n\n\n#\n\n\nMXNet.mx.ADAM\n \n \nType\n.\n\n\n ADAM\n\n\n\n\nThe solver described in Diederik Kingma, Jimmy Ba: \nAdam: A Method for Stochastic Optimization\n. arXiv:1412.6980 [cs.LG].\n\n\nADAM(; kwargs...)\n\n\n\n\n\n\nlr::Real\n: default \n0.001\n, learning rate.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a      dynamic learning rate scheduler. If set, will overwrite the \nlr\n      parameter.\n\n\nbeta1::Real\n: default \n0.9\n.\n\n\nbeta2::Real\n: default \n0.999\n.\n\n\nepsilon::Real\n: default \n1e-8\n.\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient      into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent      to adding a global l2 regularizer for all the parameters.\n\n\n\n\nsource\n\n\n\n\nAdaGrad\n\n\n#\n\n\nMXNet.mx.AdaGrad\n \n \nType\n.\n\n\nAdaGrad\n\n\n\n\nScale learning rates by dividing with the square root of accumulated squared gradients. See [1] for further description.\n\n\nAdaGrad(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.1\n, the learning rate controlling the size of update steps\n\n\nepsilon::Real\n: default \n1e-6\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nNotes\n\n\nUsing step size lr AdaGrad calculates the learning rate for feature i at time step t as: $\u03b7_{t,i} = \frac{lr}{sqrt{sum^t_{t^prime} g^2_{t^prime,i} + \u03f5}} g_{t,i}$ as such the learning rate is monotonically decreasing. Epsilon is not included in the typical formula, see [2].\n\n\nReferences\n\n\n\n\n[1]: Duchi, J., Hazan, E., \n Singer, Y. (2011): Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121-2159.\n\n\n[2]: Chris Dyer: Notes on AdaGrad. \nhttp://www.ark.cs.cmu.edu/cdyer/adagrad.pdf\n\n\n\n\nsource\n\n\n\n\nAdaDelta\n\n\n#\n\n\nMXNet.mx.AdaDelta\n \n \nType\n.\n\n\nAdaDelta\n\n\n\n\nScale learning rates by the ratio of accumulated gradients to accumulated updates, see [1] and notes for further description.\n\n\nAdaDelta(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n1.0\n, the learning rate controlling the size of update steps\n\n\nrho::Real\n: default \n0.9\n, squared gradient moving average decay factor\n\n\nepsilon::Real\n: default \n1e-6\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nNotes\n\n\nrho\n should be between 0 and 1. A value of \nrho\n close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.\n\n\nrho\n = 0.95 and \nepsilon\n = 1e-6 are suggested in the paper and reported to work for multiple datasets (MNIST, speech). In the paper, no learning rate is considered (so \nlr\n = 1.0). Probably best to keep it at this value.\n\n\nepsilon\n is important for the very first update (so the numerator does not become 0).\n\n\nUsing the step size \nlr\n and a decay factor \nrho\n the learning rate is calculated as: \nr_t \n=  ho r_{t-1} + (1- ho)*g^2\n\u001bta_t \n= \u001bta \frac{sqrt{s_{t-1} + \u001bpsilon}} {sqrt{r_t + \u001bpsilon}}\ns_t \n=  ho s_{t-1} + (1- ho)*(\u001bta_t*g)^2\n\n\nReferences\n\n\n\n\n[1]: Zeiler, M. D. (2012): ADADELTA: An Adaptive Learning Rate Method. arXiv Preprint arXiv:1212.5701.\n\n\n\n\nsource\n\n\n\n\nAdaMax\n\n\n#\n\n\nMXNet.mx.AdaMax\n \n \nType\n.\n\n\nAdaMax\n\n\n\n\nThis is a variant of of the Adam algorithm based on the infinity norm. See [1] for further description.\n\n\nAdaMax(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.002\n, the learning rate controlling the size of update steps\n\n\nbeta1::Real\n: default \n0.9\n, exponential decay rate for the first moment estimates\n\n\nbeta2::Real\n: default \n0.999\n, exponential decay rate for the weighted infinity norm estimates\n\n\nepsilon::Real\n: default \n1e-8\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nReferences\n\n\n\n\n[1]: Kingma, Diederik, and Jimmy Ba (2014): Adam: A Method for Stochastic Optimization. \nhttp://arxiv.org/abs/1412.6980v8\n.\n\n\n\n\nsource\n\n\n\n\nRMSProp\n\n\n#\n\n\nMXNet.mx.RMSProp\n \n \nType\n.\n\n\nRMSProp\n\n\n\n\nScale learning rates by dividing with the moving average of the root mean squared (RMS) gradients. See [1] for further description.\n\n\nRMSProp(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.1\n, the learning rate controlling the size of update steps\n\n\nrho::Real\n: default \n0.9\n, gradient moving average decay factor\n\n\nepsilon::Real\n: default \n1e-6\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\n\n\nNotes\n\n\nrho\n should be between 0 and 1. A value of \nrho\n close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.\n\n\nUsing the step size $lr$ and a decay factor $ho$ the learning rate $\u001bta_t$ is calculated as: \nr_t \n= \u03c1 r_{t-1} + (1 - \u03c1)*g^2 \n  \u03b7_t \n= \frac{lr}{sqrt{r_t + \u03f5}}\n\n\nReferences\n\n\n\n\n[1]: Tieleman, T. and Hinton, G. (2012): Neural Networks for Machine Learning, Lecture 6.5 - rmsprop. Coursera. \nhttp://www.youtube.com/watch?v=O3sxAc4hxZU\n (formula @5:20)\n\n\n\n\nsource\n\n\n\n\nNadam\n\n\n#\n\n\nMXNet.mx.Nadam\n \n \nType\n.\n\n\nNadam\n\n\n\n\nNesterov Adam optimizer: Adam RMSprop with Nesterov momentum, see [1] and notes for further description.\n\n\nNadam(; kwargs...)\n\n\n\n\nAttributes\n\n\n\n\nlr::Real\n: default \n0.001\n, learning rate.\n\n\nbeta1::Real\n: default \n0.99\n.\n\n\nbeta2::Real\n: default \n0.999\n.\n\n\nepsilon::Real\n: default \n1e-8\n, small value added for numerical stability\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent to adding a global l2 regularizer for all the parameters.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a dynamic learning rate scheduler. If set, will overwrite the \nlr\n parameter.\n\n\nmomentum_scheduler::AbstractMomentumScheduler\n default \nNadamScheduler\n of the form $mu_t = beta1 * (1 - 0.5 * 0.96^{t * 0.004})$\n\n\n\n\nNotes\n\n\nDefault parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values.\n\n\nReferences\n\n\n\n\n[1]: Incorporating Nesterov Momentum into Adam. \nhttp://cs229.stanford.edu/proj2015/054_report.pdf\n\n\n[2]: On the importance of initialization and momentum in deep learning \nhttp://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n\n\n\n\nsource", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#optimizers", 
            "text": "#  MXNet.mx.AbstractOptimizer     Type .  AbstractOptimizer  Base type for all optimizers.  source  #  MXNet.mx.AbstractLearningRateScheduler     Type .  AbstractLearningRateScheduler  Base type for all learning rate scheduler.  source  #  MXNet.mx.AbstractMomentumScheduler     Type .  AbstractMomentumScheduler  Base type for all momentum scheduler.  source  #  MXNet.mx.AbstractOptimizerOptions     Type .  AbstractOptimizerOptions  Base class for all optimizer options.  source  #  MXNet.mx.OptimizationState     Type .  OptimizationState  Attributes:   batch_size : The size of the mini-batch used in stochastic training.  curr_epoch : The current epoch count. Epoch 0 means no training yet, during the first pass through the data, the epoch will be 1; during the second pass, the epoch count will be 1, and so on.  curr_batch : The current mini-batch count. The batch count is reset during every epoch. The batch count 0 means the beginning of each epoch, with no mini-batch seen yet. During the first mini-batch, the mini-batch count will be 1.  curr_iter : The current iteration count. One iteration corresponds to one mini-batch, but unlike the mini-batch count, the iteration count does  not  reset in each epoch. So it track the  total  number of mini-batches seen so far.   source  #  MXNet.mx.get_learning_rate     Function .  get_learning_rate(scheduler, state)  Arguments   scheduler::AbstractLearningRateScheduler : a learning rate scheduler.  state::OptimizationState : the current state about epoch, mini-batch and iteration count.   Returns the current learning rate.  source  #  MXNet.mx.get_momentum     Function .  get_momentum(scheduler, state)   scheduler::AbstractMomentumScheduler : the momentum scheduler.  state::OptimizationState : the state about current epoch, mini-batch and iteration count.   Returns the current momentum.  source  #  MXNet.mx.get_updater     Method .  get_updater(optimizer)  A utility function to create an updater function, that uses its closure to store all the states needed for each weights.   optimizer::AbstractOptimizer : the underlying optimizer.   source  #  MXNet.mx.normalized_gradient     Method .  normalized_gradient(opts, state, W, \u2207)  Get the properly normalized gradient (re-scaled and clipped if necessary).   opts::AbstractOptimizerOptions : options for the optimizer, should contain the field  grad_clip  and  weight_decay .  state::OptimizationState : the current optimization state.  W::NDArray : the trainable weights.  \u2207::NDArray : the original gradient of the weights.   source  #  MXNet.mx.LearningRate.Exp     Type .  LearningRate.Exp  $\u001bta_t = \u001bta_0gamma^t$. Here $t$ is the epoch count, or the iteration count if  decay_on_iteration  is set to true.  source  #  MXNet.mx.LearningRate.Fixed     Type .  LearningRate.Fixed  Fixed learning rate scheduler always return the same learning rate.  source  #  MXNet.mx.LearningRate.Inv     Type .  LearningRate.Inv  $\u001bta_t = \u001bta_0 * (1 + gamma * t)^(-power)$. Here $t$ is the epoch count, or the iteration count if  decay_on_iteration  is set to true.  source  #  MXNet.mx.Momentum.Fixed     Type .  Momentum.Fixed  Fixed momentum scheduler always returns the same value.  source  #  MXNet.mx.Momentum.NadamScheduler     Type .  Momentum.NadamScheduler  Nesterov-accelerated adaptive momentum scheduler.  Description in \"Incorporating Nesterov Momentum into Adam.\"  http://cs229.stanford.edu/proj2015/054_report.pdf  $mu_t = mu_0 * (1 - gamma * \u0007lpha^{t * delta})$. Here   $t$ is the iteration count  $delta$: default  0.004  is scheduler decay,  $gamma$: default  0.5  $\u0007lpha$: default  0.96  $mu_0$: default  0.99   source  #  MXNet.mx.Momentum.Null     Type .  Momentum.Null  The null momentum scheduler always returns 0 for momentum. It is also used to explicitly indicate momentum should not be used.  source", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#built-in-optimizers", 
            "text": "", 
            "title": "Built-in optimizers"
        }, 
        {
            "location": "/api/optimizer/#stochastic-gradient-descent", 
            "text": "#  MXNet.mx.SGD     Type .  SGD  Stochastic gradient descent optimizer.  SGD(; kwargs...)  Arguments:   lr::Real : default  0.01 , learning rate.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a      dynamic learning rate scheduler. If set, will overwrite the  lr       parameter.  momentum::Real : default  0.0 , the momentum.  momentum_scheduler::AbstractMomentumScheduler : default  nothing ,      a dynamic momentum scheduler. If set, will overwrite the  momentum       parameter.  grad_clip::Real : default  0 , if positive, will clip the gradient      into the bounded range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.0001 , weight decay is equivalent to      adding a global l2 regularizer to the parameters.   source", 
            "title": "Stochastic Gradient Descent"
        }, 
        {
            "location": "/api/optimizer/#adam", 
            "text": "#  MXNet.mx.ADAM     Type .   ADAM  The solver described in Diederik Kingma, Jimmy Ba:  Adam: A Method for Stochastic Optimization . arXiv:1412.6980 [cs.LG].  ADAM(; kwargs...)   lr::Real : default  0.001 , learning rate.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a      dynamic learning rate scheduler. If set, will overwrite the  lr       parameter.  beta1::Real : default  0.9 .  beta2::Real : default  0.999 .  epsilon::Real : default  1e-8 .  grad_clip::Real : default  0 , if positive, will clip the gradient      into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent      to adding a global l2 regularizer for all the parameters.   source", 
            "title": "ADAM"
        }, 
        {
            "location": "/api/optimizer/#adagrad", 
            "text": "#  MXNet.mx.AdaGrad     Type .  AdaGrad  Scale learning rates by dividing with the square root of accumulated squared gradients. See [1] for further description.  AdaGrad(; kwargs...)  Attributes   lr::Real : default  0.1 , the learning rate controlling the size of update steps  epsilon::Real : default  1e-6 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   Notes  Using step size lr AdaGrad calculates the learning rate for feature i at time step t as: $\u03b7_{t,i} = \frac{lr}{sqrt{sum^t_{t^prime} g^2_{t^prime,i} + \u03f5}} g_{t,i}$ as such the learning rate is monotonically decreasing. Epsilon is not included in the typical formula, see [2].  References   [1]: Duchi, J., Hazan, E.,   Singer, Y. (2011): Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121-2159.  [2]: Chris Dyer: Notes on AdaGrad.  http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf   source", 
            "title": "AdaGrad"
        }, 
        {
            "location": "/api/optimizer/#adadelta", 
            "text": "#  MXNet.mx.AdaDelta     Type .  AdaDelta  Scale learning rates by the ratio of accumulated gradients to accumulated updates, see [1] and notes for further description.  AdaDelta(; kwargs...)  Attributes   lr::Real : default  1.0 , the learning rate controlling the size of update steps  rho::Real : default  0.9 , squared gradient moving average decay factor  epsilon::Real : default  1e-6 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   Notes  rho  should be between 0 and 1. A value of  rho  close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.  rho  = 0.95 and  epsilon  = 1e-6 are suggested in the paper and reported to work for multiple datasets (MNIST, speech). In the paper, no learning rate is considered (so  lr  = 1.0). Probably best to keep it at this value.  epsilon  is important for the very first update (so the numerator does not become 0).  Using the step size  lr  and a decay factor  rho  the learning rate is calculated as:  r_t  =  ho r_{t-1} + (1- ho)*g^2\n\u001bta_t  = \u001bta \frac{sqrt{s_{t-1} + \u001bpsilon}} {sqrt{r_t + \u001bpsilon}}\ns_t  =  ho s_{t-1} + (1- ho)*(\u001bta_t*g)^2  References   [1]: Zeiler, M. D. (2012): ADADELTA: An Adaptive Learning Rate Method. arXiv Preprint arXiv:1212.5701.   source", 
            "title": "AdaDelta"
        }, 
        {
            "location": "/api/optimizer/#adamax", 
            "text": "#  MXNet.mx.AdaMax     Type .  AdaMax  This is a variant of of the Adam algorithm based on the infinity norm. See [1] for further description.  AdaMax(; kwargs...)  Attributes   lr::Real : default  0.002 , the learning rate controlling the size of update steps  beta1::Real : default  0.9 , exponential decay rate for the first moment estimates  beta2::Real : default  0.999 , exponential decay rate for the weighted infinity norm estimates  epsilon::Real : default  1e-8 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   References   [1]: Kingma, Diederik, and Jimmy Ba (2014): Adam: A Method for Stochastic Optimization.  http://arxiv.org/abs/1412.6980v8 .   source", 
            "title": "AdaMax"
        }, 
        {
            "location": "/api/optimizer/#rmsprop", 
            "text": "#  MXNet.mx.RMSProp     Type .  RMSProp  Scale learning rates by dividing with the moving average of the root mean squared (RMS) gradients. See [1] for further description.  RMSProp(; kwargs...)  Attributes   lr::Real : default  0.1 , the learning rate controlling the size of update steps  rho::Real : default  0.9 , gradient moving average decay factor  epsilon::Real : default  1e-6 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.   Notes  rho  should be between 0 and 1. A value of  rho  close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.  Using the step size $lr$ and a decay factor $ho$ the learning rate $\u001bta_t$ is calculated as:  r_t  = \u03c1 r_{t-1} + (1 - \u03c1)*g^2 \n  \u03b7_t  = \frac{lr}{sqrt{r_t + \u03f5}}  References   [1]: Tieleman, T. and Hinton, G. (2012): Neural Networks for Machine Learning, Lecture 6.5 - rmsprop. Coursera.  http://www.youtube.com/watch?v=O3sxAc4hxZU  (formula @5:20)   source", 
            "title": "RMSProp"
        }, 
        {
            "location": "/api/optimizer/#nadam", 
            "text": "#  MXNet.mx.Nadam     Type .  Nadam  Nesterov Adam optimizer: Adam RMSprop with Nesterov momentum, see [1] and notes for further description.  Nadam(; kwargs...)  Attributes   lr::Real : default  0.001 , learning rate.  beta1::Real : default  0.99 .  beta2::Real : default  0.999 .  epsilon::Real : default  1e-8 , small value added for numerical stability  grad_clip::Real : default  0 , if positive, will clip the gradient into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent to adding a global l2 regularizer for all the parameters.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a dynamic learning rate scheduler. If set, will overwrite the  lr  parameter.  momentum_scheduler::AbstractMomentumScheduler  default  NadamScheduler  of the form $mu_t = beta1 * (1 - 0.5 * 0.96^{t * 0.004})$   Notes  Default parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values.  References   [1]: Incorporating Nesterov Momentum into Adam.  http://cs229.stanford.edu/proj2015/054_report.pdf  [2]: On the importance of initialization and momentum in deep learning  http://www.cs.toronto.edu/~fritz/absps/momentum.pdf   source", 
            "title": "Nadam"
        }, 
        {
            "location": "/api/callback/", 
            "text": "Callback in training\n\n\n#\n\n\nMXNet.mx.AbstractBatchCallback\n \n \nType\n.\n\n\nAbstractBatchCallback\n\n\n\n\nAbstract type of callbacks to be called every mini-batch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractCallback\n \n \nType\n.\n\n\nAbstractCallback\n\n\n\n\nAbstract type of callback functions used in training.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEpochCallback\n \n \nType\n.\n\n\nAbstractEpochCallback\n\n\n\n\nAbstract type of callbacks to be called every epoch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.do_checkpoint\n \n \nMethod\n.\n\n\ndo_checkpoint(prefix; frequency=1, save_epoch_0=false)\n\n\n\n\nCreate an \nAbstractEpochCallback\n that save checkpoints of the model to disk. The checkpoints can be loaded back later on.\n\n\nArguments\n\n\n\n\nprefix::AbstractString\n: the prefix of the filenames to save the model. The model architecture will be saved to prefix-symbol.json, while the weights will be saved to prefix-0012.params, for example, for the 12-th epoch.\n\n\nfrequency::Int\n: keyword argument, default is 1. The frequency (measured in epochs) to save checkpoints.\n\n\nsave_epoch_0::Bool\n: keyword argument, default false. Whether we should save a checkpoint for epoch 0 (model initialized but not seen any data yet).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_batch\n \n \nMethod\n.\n\n\nevery_n_batch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every \nn\n mini-batches.\n\n\nArguments\n\n\n\n\ncall_on_0::Bool\n: keyword argument, default false. Unless set, the callback         will \nnot\n be run on batch 0.\n\n\n\n\nFor example, the \nspeedometer\n callback is defined as\n\n\nevery_n_batch(frequency, call_on_0=true) do state :: OptimizationState\n  if state.curr_batch == 0\n    # reset timer\n  else\n    # compute and print speed\n  end\nend\n\n\n\n\nSee also \nevery_n_epoch\n and \nspeedometer\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_epoch\n \n \nMethod\n.\n\n\nevery_n_epoch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every \nn\n full data-passes.\n\n\n\n\ncall_on_0::Bool\n: keyword argument, default false. Unless set, the callback         will \nnot\n be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.\n\n\n\n\nSee also \nevery_n_batch\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.speedometer\n \n \nMethod\n.\n\n\nspeedometer(;frequency=50)\n\n\n\n\nCreate an \nAbstractBatchCallback\n that measure the training speed    (number of samples processed per second) every k mini-batches.\n\n\nArguments\n\n\n\n\nfrequency::Int\n: keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.\n\n\n\n\nsource", 
            "title": "Callbacks in training"
        }, 
        {
            "location": "/api/callback/#callback-in-training", 
            "text": "#  MXNet.mx.AbstractBatchCallback     Type .  AbstractBatchCallback  Abstract type of callbacks to be called every mini-batch.  source  #  MXNet.mx.AbstractCallback     Type .  AbstractCallback  Abstract type of callback functions used in training.  source  #  MXNet.mx.AbstractEpochCallback     Type .  AbstractEpochCallback  Abstract type of callbacks to be called every epoch.  source  #  MXNet.mx.do_checkpoint     Method .  do_checkpoint(prefix; frequency=1, save_epoch_0=false)  Create an  AbstractEpochCallback  that save checkpoints of the model to disk. The checkpoints can be loaded back later on.  Arguments   prefix::AbstractString : the prefix of the filenames to save the model. The model architecture will be saved to prefix-symbol.json, while the weights will be saved to prefix-0012.params, for example, for the 12-th epoch.  frequency::Int : keyword argument, default is 1. The frequency (measured in epochs) to save checkpoints.  save_epoch_0::Bool : keyword argument, default false. Whether we should save a checkpoint for epoch 0 (model initialized but not seen any data yet).   source  #  MXNet.mx.every_n_batch     Method .  every_n_batch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every  n  mini-batches.  Arguments   call_on_0::Bool : keyword argument, default false. Unless set, the callback         will  not  be run on batch 0.   For example, the  speedometer  callback is defined as  every_n_batch(frequency, call_on_0=true) do state :: OptimizationState\n  if state.curr_batch == 0\n    # reset timer\n  else\n    # compute and print speed\n  end\nend  See also  every_n_epoch  and  speedometer .  source  #  MXNet.mx.every_n_epoch     Method .  every_n_epoch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every  n  full data-passes.   call_on_0::Bool : keyword argument, default false. Unless set, the callback         will  not  be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.   See also  every_n_batch .  source  #  MXNet.mx.speedometer     Method .  speedometer(;frequency=50)  Create an  AbstractBatchCallback  that measure the training speed    (number of samples processed per second) every k mini-batches.  Arguments   frequency::Int : keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.   source", 
            "title": "Callback in training"
        }, 
        {
            "location": "/api/metric/", 
            "text": "Evaluation Metrics\n\n\nEvaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.\n\n\n#\n\n\nMXNet.mx.ACE\n \n \nType\n.\n\n\nACE\n\n\n\n\nCalculates the averaged cross-entropy (logloss) for classification.\n\n\nArguments:\n\n\n\n\neps::Float64\n: Prevents returning \nInf\n if \np = 0\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEvalMetric\n \n \nType\n.\n\n\nAbstractEvalMetric\n\n\n\n\nThe base class for all evaluation metrics. The sub-types should implement the following interfaces:\n\n\n\n\nupdate!\n\n\nreset!\n\n\nget\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Accuracy\n \n \nType\n.\n\n\nAccuracy\n\n\n\n\nMulticlass classification accuracy.\n\n\nCalculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MSE\n \n \nType\n.\n\n\nMSE\n\n\n\n\nMean Squared Error.\n\n\nCalculates the mean squared error regression loss. Requires that label and prediction have the same shape.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MultiACE\n \n \nType\n.\n\n\nMultiACE\n\n\n\n\nCalculates the averaged cross-entropy per class and overall (see \nACE\n). This can be used to quantify the influence of different classes on the overall loss.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MultiMetric\n \n \nType\n.\n\n\nMultiMetric(metrics::Vector{AbstractEvalMetric})\n\n\n\n\nCombine multiple metrics in one and get a result for all of them.\n\n\nUsage\n\n\nTo calculate both mean-squared error \nAccuracy\n and log-loss \nACE\n:\n\n\n  mx.fit(..., eval_metric = mx.MultiMetric([mx.Accuracy(), mx.ACE()]))\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NMSE\n \n \nType\n.\n\n\nNMSE\n\n\n\n\nNormalized Mean Squared Error\n\n\n\n\n\n\\sum_i (\\frac{label_i - pred_i}{label_i})^2\n\n\n\n\n\nNote that there are various ways to do the \nnormalization\n. It depends on your own context. Please judge the problem setting you have first. If the current implementation do not suitable for you, feel free to file it on GitHub.\n\n\nLet me show you a use case of this kind of normalization:\n\n\nBob is training a network for option pricing. The option pricing problem is a regression problem (pirce predicting). There are lots of option contracts on same target stock but different strike price. For example, there is a stock \nS\n; it's market price is 1000. And, there are two call option contracts with different strike price. Assume Bob obtains the outcome as following table:\n\n\n+--------+----------------+----------------+--------------+\n|        | Strike Price   | Market Price   | Pred Price   |\n+--------+----------------+----------------+--------------+\n| Op 1   | 1500           |  100           | 80           |\n+--------+----------------+----------------+--------------+\n| Op 2   | 500            |  10            | 8            |\n+--------+----------------+----------------+--------------+\n\n\n\n\nNow, obviously, Bob will calculate the normalized MSE as:\n\n\n\n\n\n    (\\frac{100 - 80}{100})^2\n    \\text{ vs }\n    (\\frac{10 - 8}{10}) ^2\n\n\n\n\n\nBoth of the pred prices got the same degree of error.\n\n\nFor more discussion about normalized MSE, please see \n#211\n also.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SeqMetric\n \n \nType\n.\n\n\nSeqMetric(metrics::Vector{AbstractEvalMetric})\n\n\n\n\nApply a different metric to each output. This is especially useful for \nmx.Group\n.\n\n\nUsage\n\n\nCalculate accuracy \nAccuracy\n for the first output and log-loss \nACE\n for the second output:\n\n\n  mx.fit(..., eval_metric = mx.SeqMetric([mx.Accuracy(), mx.ACE()]))\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NullMetric\n \n \nType\n.\n\n\nNullMetric()\n\n\n\n\nA metric that calculates nothing. Can be used to ignore an output during training.\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nMethod\n.\n\n\nget(metric)\n\n\n\n\nGet the accumulated metrics.\n\n\nReturns \nVector{Tuple{Base.Symbol, Real}}\n, a list of name-value pairs. For example, \n[(:accuracy, 0.9)]\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.hasNDArraySupport\n \n \nMethod\n.\n\n\nhasNDArraySupport(metric) -\n Val{true/false}\n\n\n\n\nTrait for \n_update_single_output\n should return \nVal{true}() if metric can handle\nNDArray\ndirectly and\nVal{false}()\nif requires\nArray`. Metric that work with NDArrays can be async, while native Julia arrays require that we copy the output of the network, which is a blocking operation.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.reset!\n \n \nMethod\n.\n\n\nreset!(metric)\n\n\n\n\nReset the accumulation counter.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.update!\n \n \nMethod\n.\n\n\nupdate!(metric, labels, preds)\n\n\n\n\nUpdate and accumulate metrics.\n\n\nArguments:\n\n\n\n\nmetric::AbstractEvalMetric\n: the metric object.\n\n\nlabels::Vector{NDArray}\n: the labels from the data provider.\n\n\npreds::Vector{NDArray}\n: the outputs (predictions) of the network.\n\n\n\n\nsource", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/metric/#evaluation-metrics", 
            "text": "Evaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.  #  MXNet.mx.ACE     Type .  ACE  Calculates the averaged cross-entropy (logloss) for classification.  Arguments:   eps::Float64 : Prevents returning  Inf  if  p = 0 .   source  #  MXNet.mx.AbstractEvalMetric     Type .  AbstractEvalMetric  The base class for all evaluation metrics. The sub-types should implement the following interfaces:   update!  reset!  get   source  #  MXNet.mx.Accuracy     Type .  Accuracy  Multiclass classification accuracy.  Calculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.  source  #  MXNet.mx.MSE     Type .  MSE  Mean Squared Error.  Calculates the mean squared error regression loss. Requires that label and prediction have the same shape.  source  #  MXNet.mx.MultiACE     Type .  MultiACE  Calculates the averaged cross-entropy per class and overall (see  ACE ). This can be used to quantify the influence of different classes on the overall loss.  source  #  MXNet.mx.MultiMetric     Type .  MultiMetric(metrics::Vector{AbstractEvalMetric})  Combine multiple metrics in one and get a result for all of them.  Usage  To calculate both mean-squared error  Accuracy  and log-loss  ACE :    mx.fit(..., eval_metric = mx.MultiMetric([mx.Accuracy(), mx.ACE()]))  source  #  MXNet.mx.NMSE     Type .  NMSE  Normalized Mean Squared Error   \n\\sum_i (\\frac{label_i - pred_i}{label_i})^2   Note that there are various ways to do the  normalization . It depends on your own context. Please judge the problem setting you have first. If the current implementation do not suitable for you, feel free to file it on GitHub.  Let me show you a use case of this kind of normalization:  Bob is training a network for option pricing. The option pricing problem is a regression problem (pirce predicting). There are lots of option contracts on same target stock but different strike price. For example, there is a stock  S ; it's market price is 1000. And, there are two call option contracts with different strike price. Assume Bob obtains the outcome as following table:  +--------+----------------+----------------+--------------+\n|        | Strike Price   | Market Price   | Pred Price   |\n+--------+----------------+----------------+--------------+\n| Op 1   | 1500           |  100           | 80           |\n+--------+----------------+----------------+--------------+\n| Op 2   | 500            |  10            | 8            |\n+--------+----------------+----------------+--------------+  Now, obviously, Bob will calculate the normalized MSE as:   \n    (\\frac{100 - 80}{100})^2\n    \\text{ vs }\n    (\\frac{10 - 8}{10}) ^2   Both of the pred prices got the same degree of error.  For more discussion about normalized MSE, please see  #211  also.  source  #  MXNet.mx.SeqMetric     Type .  SeqMetric(metrics::Vector{AbstractEvalMetric})  Apply a different metric to each output. This is especially useful for  mx.Group .  Usage  Calculate accuracy  Accuracy  for the first output and log-loss  ACE  for the second output:    mx.fit(..., eval_metric = mx.SeqMetric([mx.Accuracy(), mx.ACE()]))  source  #  MXNet.mx.NullMetric     Type .  NullMetric()  A metric that calculates nothing. Can be used to ignore an output during training.  source  #  Base.get     Method .  get(metric)  Get the accumulated metrics.  Returns  Vector{Tuple{Base.Symbol, Real}} , a list of name-value pairs. For example,  [(:accuracy, 0.9)] .  source  #  MXNet.mx.hasNDArraySupport     Method .  hasNDArraySupport(metric) -  Val{true/false}  Trait for  _update_single_output  should return  Val{true}() if metric can handle NDArray directly and Val{false}() if requires Array`. Metric that work with NDArrays can be async, while native Julia arrays require that we copy the output of the network, which is a blocking operation.  source  #  MXNet.mx.reset!     Method .  reset!(metric)  Reset the accumulation counter.  source  #  MXNet.mx.update!     Method .  update!(metric, labels, preds)  Update and accumulate metrics.  Arguments:   metric::AbstractEvalMetric : the metric object.  labels::Vector{NDArray} : the labels from the data provider.  preds::Vector{NDArray} : the outputs (predictions) of the network.   source", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/io/", 
            "text": "Data Providers\n\n\nData providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.\n\n\n\n\nAbstractDataProvider interface\n\n\n#\n\n\nMXNet.mx.AbstractDataProvider\n \n \nType\n.\n\n\nAbstractDataProvider\n\n\n\n\nThe root type for all data provider. A data provider should implement the following interfaces:\n\n\n\n\nget_batch_size\n\n\nprovide_data\n\n\nprovide_label\n\n\n\n\nAs well as the Julia iterator interface (see \nthe Julia manual\n). Normally this involves defining:\n\n\n\n\nBase.eltype(provider) -\n AbstractDataBatch\n\n\nBase.start(provider) -\n AbstractDataProviderState\n\n\nBase.done(provider, state) -\n Bool\n\n\nBase.next(provider, state) -\n (AbstractDataBatch, AbstractDataProvider)\n\n\n\n\nsource\n\n\nThe difference between \ndata\n and \nlabel\n is that during training stage, both \ndata\n and \nlabel\n will be feeded into the model, while during prediction stage, only \ndata\n is loaded. Otherwise, they could be anything, with any names, and of any shapes. The provided data and label names here should match the input names in a target \nSymbolicNode\n.\n\n\nA data provider should also implement the Julia iteration interface, in order to allow iterating through the data set. The provider will be called in the following way:\n\n\nfor batch in eachbatch(provider)\n    data = get_data(provider, batch)\nend\n\n\n\n\nwhich will be translated by Julia compiler into\n\n\nstate = Base.start(eachbatch(provider))\nwhile !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\nend\n\n\n\n\nBy default, \neachbatch\n simply returns the provider itself, so the iterator interface is implemented on the provider type itself. But the extra layer of abstraction allows us to implement a data provider easily via a Julia \nTask\n coroutine. See the data provider defined in \nthe char-lstm example\n for an example of using coroutine to define data providers.\n\n\nThe detailed interface functions for the iterator API is listed below:\n\n\nBase.eltype(provider) -\n AbstractDataBatch\n\n\n\n\nReturns the specific subtype representing a data batch. See \nAbstractDataBatch\n.\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nBase.start(provider) -\n AbstractDataProviderState\n\n\n\n\n\n\nThis function is always called before iterating into the dataset. It should initialize the iterator, reset the index, and do data shuffling if needed.\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nBase.done(provider, state) -\n Bool\n\n\n\n\n\n\nTrue if there is no more data to iterate in this dataset.\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nstate::AbstractDataProviderState\n: the state returned by \nBase.start\n and \nBase.next\n.\n\n\nBase.next(provider) -\n (AbstractDataBatch, AbstractDataProviderState)\n\n\n\n\n\n\nReturns the current data batch, and the state for the next iteration.\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nNote sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that\n\n\n\n\nBase.start\n will always be called, and called only once before the iteration starts.\n\n\nBase.done\n will always be called at the beginning of every iteration and always be called once.\n\n\nIf \nBase.done\n return true, the iteration will stop, until the next round, again, starting with a call to \nBase.start\n.\n\n\nBase.next\n will always be called only once in each iteration. It will always be called after one and only one call to \nBase.done\n; but if \nBase.done\n returns true, \nBase.next\n will not be called.\n\n\n\n\nWith those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in \nMXDataProvider\n for example.\n\n\n\n\nNote\n\n\nPlease do not use the one data provider simultaneously in two different places, either in parallel, or in a nested loop. For example, the behavior for the following code is undefined\n\n\n```julia\nfor batch in data\n    # updating the parameters\n\n\n# now let's test the performance on the training set\nfor b2 in data\n    # ...\nend\n\n\n\nend\n```\n\n\n\n\n#\n\n\nMXNet.mx.get_batch_size\n \n \nFunction\n.\n\n\nget_batch_size(provider) -\n Int\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns the mini-batch size of the provided data. All the provided data should have the same mini-batch size (i.e. the last dimension).\n\n\nsource\n\n\n#\n\n\nMXNet.mx.provide_data\n \n \nFunction\n.\n\n\nprovide_data(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns a vector of (name, shape) pairs describing the names of the data it provides, and the corresponding shapes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.provide_label\n \n \nFunction\n.\n\n\nprovide_label(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns a vector of (name, shape) pairs describing the names of the labels it provides, and the corresponding shapes.\n\n\nsource\n\n\n\n\nAbstractDataBatch interface\n\n\n#\n\n\nMXNet.mx.AbstractDataProviderState\n \n \nType\n.\n\n\nAbstractDataProviderState\n\n\n\n\nBase type for data provider states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.count_samples\n \n \nFunction\n.\n\n\ncount_samples(provider, batch) -\n Int\n\n\n\n\nArguments:\n\n\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns the number of samples in this batch. This number should be greater than 0, but less than or equal to the batch size. This is used to indicate at the end of the data set, there might not be enough samples for a whole mini-batch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_data\n \n \nFunction\n.\n\n\nget_data(provider, batch) -\n Vector{NDArray}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns a vector of data in this batch, should be in the same order as declared in \nprovide_data() \nAbstractDataProvider.provide_data\n.\n\n\nThe last dimension of each \nNDArray\n should always match the batch_size, even when \ncount_samples\n returns a value less than the batch size. In this case,      the data provider is free to pad the remaining contents with any value.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_label\n \n \nFunction\n.\n\n\nget_label(provider, batch) -\n Vector{NDArray}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns a vector of labels in this batch. Similar to \nget_data\n.\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nFunction\n.\n\n\nget(metric)\n\n\n\n\nGet the accumulated metrics.\n\n\nReturns \nVector{Tuple{Base.Symbol, Real}}\n, a list of name-value pairs. For example, \n[(:accuracy, 0.9)]\n.\n\n\nsource\n\n\nget(provider, batch, name) -\n NDArray\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\nname::Symbol\n: the name of the data to get, should be one of the names provided in either \nprovide_data() \nAbstractDataProvider.provide_data\n or \nprovide_label() \nAbstractDataProvider.provide_label\n.\n\n\n\n\nReturns the corresponding data array corresponding to that name.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_data!\n \n \nFunction\n.\n\n\nload_data!(provider, batch, targets)\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load data into.\n\n\n\n\nThe targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of \nSlicedNDArray\n. This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where \nslice\n specify the range of samples in the mini-batch that should be loaded into the corresponding \nndarray\n.\n\n\nThis utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_label!\n \n \nFunction\n.\n\n\nload_label!(provider, batch, targets)\n\n\n\n\n\n\nprovider::AbstractDataProvider provider\n: the data provider.\n\n\nbatch::AbstractDataBatch batch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load label into.\n\n\n\n\nThe same as \nload_data!\n, except that this is for loading labels.\n\n\nsource\n\n\n\n\nImplemented providers and other methods\n\n\n#\n\n\nMXNet.mx.AbstractDataBatch\n \n \nType\n.\n\n\nAbstractDataBatch\n\n\n\n\nBase type for a data mini-batch. It should implement the following interfaces:\n\n\n\n\ncount_samples\n\n\nget_data\n\n\nget_label\n\n\n\n\nThe following utility functions will be automatically defined:\n\n\n\n\nget\n\n\nload_data!\n\n\nload_label!\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ArrayDataProvider\n \n \nType\n.\n\n\nArrayDataProvider\n\n\n\n\nA convenient tool to iterate \nNDArray\n or Julia \nArray\n.\n\n\nArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)\n\n\n\n\nConstruct a data provider from \nNDArray\n or Julia Arrays.\n\n\nArguments:\n\n\n\n\n\n\ndata\n: the data, could be\n\n\n\n\na \nNDArray\n, or a Julia Array. This is equivalent to \n:data =\n data\n.\n\n\na name-data pair, like \n:mydata =\n array\n, where \n:mydata\n is the name of the data\n\n\nand \narray\n is an \nNDArray\n or a Julia Array.\n\n\na list of name-data pairs.\n\n\nlabel\n: the same as the \ndata\n parameter. When this argument is omitted, the constructed provider will provide no labels.\n\n\nbatch_size::Int\n: the batch size, default is 0, which means treating the whole array as a single mini-batch.\n\n\nshuffle::Bool\n: turn on if the data should be shuffled at every epoch.\n\n\ndata_padding::Real\n: when the mini-batch goes beyond the dataset boundary, there might be less samples to include than a mini-batch. This value specify a scalar to pad the contents of all the missing data points.\n\n\nlabel_padding::Real\n: the same as \ndata_padding\n, except for the labels.\n\n\n\n\n\n\n\n\nTODO: remove \ndata_padding\n and \nlabel_padding\n, and implement rollover that copies the last or first several training samples to feed the padding.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.DataBatch\n \n \nType\n.\n\n\nDataBatch\n\n\n\n\nA basic subclass of \nAbstractDataBatch\n, that implement the interface by accessing member fields.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MXDataProvider\n \n \nType\n.\n\n\nMXDataProvider\n\n\n\n\nA data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SlicedNDArray\n \n \nType\n.\n\n\nSlicedNDArray\n\n\n\n\nA alias type of \nTuple{UnitRange{Int},NDArray}\n.\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nMethod\n.\n\n\nget(provider, batch, name) -\n NDArray\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\nname::Symbol\n: the name of the data to get, should be one of the names provided in either \nprovide_data() \nAbstractDataProvider.provide_data\n or \nprovide_label() \nAbstractDataProvider.provide_label\n.\n\n\n\n\nReturns the corresponding data array corresponding to that name.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.CSVIter\n \n \nMethod\n.\n\n\nCSVIter(data_csv, data_shape, label_csv, label_shape, batch_size, round_batch, prefetch_buffer, dtype)\n\n\n\n\nCan also be called with the alias \nCSVProvider\n. Returns the CSV file iterator.\n\n\nIn this function, the \ndata_shape\n parameter is used to set the shape of each line of the input data. If a row in an input file is \n1,2,3,4,5,6``and\ndata_shape` is (3,2), that row will be reshaped, yielding the array [[1,2],[3,4],[5,6]] of shape (3,2).\n\n\nBy default, the \nCSVIter\n has \nround_batch\n parameter set to $True$. So, if \nbatch_size\n is 3 and there are 4 total rows in CSV file, 2 more examples are consumed at the first round. If \nreset\n function is called after first round, the call is ignored and remaining examples are returned in the second round.\n\n\nIf one wants all the instances in the second round after calling \nreset\n, make sure to set \nround_batch\n to False.\n\n\nIf $data_csv = 'data/'$ is set, then all the files in this directory will be read.\n\n\n$reset()$ is expected to be called only after a complete pass of data.\n\n\nExamples::\n\n\n// Contents of CSV file $data/data.csv$.   1,2,3   2,3,4   3,4,5   4,5,6\n\n\n// Creates a \nCSVIter\n with \nbatch_size\n=2 and default \nround_batch\n=True.   CSVIter = mx.io.CSVIter(data_csv = 'data/data.csv', data_shape = (3,),   batch_size = 2)\n\n\n// Two batches read from the above iterator are as follows:   [[ 1.  2.  3.]   [ 2.  3.  4.]]   [[ 3.  4.  5.]   [ 4.  5.  6.]]\n\n\n// Creates a \nCSVIter\n with default \nround_batch\n set to True.   CSVIter = mx.io.CSVIter(data_csv = 'data/data.csv', data_shape = (3,),   batch_size = 3)\n\n\n// Two batches read from the above iterator in the first pass are as follows:   [[1.  2.  3.]   [2.  3.  4.]   [3.  4.  5.]]\n\n\n[[4.  5.  6.]   [1.  2.  3.]   [2.  3.  4.]]\n\n\n// Now, \nreset\n method is called.   CSVIter.reset()\n\n\n// Batch read from the above iterator in the second pass is as follows:   [[ 3.  4.  5.]   [ 4.  5.  6.]   [ 1.  2.  3.]]\n\n\n// Creates a \nCSVIter\n with \nround_batch\n=False.   CSVIter = mx.io.CSVIter(data_csv = 'data/data.csv', data_shape = (3,),   batch_size = 3, round_batch=False)\n\n\n// Contents of two batches read from the above iterator in both passes, after calling   // \nreset\n method before second pass, is as follows:   [[1.  2.  3.]   [2.  3.  4.]   [3.  4.  5.]]\n\n\n[[4.  5.  6.]   [2.  3.  4.]   [3.  4.  5.]]\n\n\nDefined in src/io/iter_csv.cc:L223\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\ndata_csv::string, required\n: The input CSV file or a directory path.\n\n\ndata_shape::Shape(tuple), required\n: The shape of one example.\n\n\nlabel_csv::string, optional, default='NULL'\n: The input CSV file or a directory path. If NULL, all labels will be returned as 0.\n\n\nlabel_shape::Shape(tuple), optional, default=[1]\n: The shape of one label.\n\n\nbatch_size::int (non-negative), required\n: Batch size.\n\n\nround_batch::boolean, optional, default=1\n: Whether to use round robin to handle overflow batch or not.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageDetRecordIter\n \n \nMethod\n.\n\n\nImageDetRecordIter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, label_pad_width, label_pad_value, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop_prob, min_crop_scales, max_crop_scales, min_crop_aspect_ratios, max_crop_aspect_ratios, min_crop_overlaps, max_crop_overlaps, min_crop_sample_coverages, max_crop_sample_coverages, min_crop_object_coverages, max_crop_object_coverages, num_crop_sampler, crop_emit_mode, emit_overlap_thresh, max_crop_trials, rand_pad_prob, max_pad_scale, max_random_hue, random_hue_prob, max_random_saturation, random_saturation_prob, max_random_illumination, random_illumination_prob, max_random_contrast, random_contrast_prob, rand_mirror_prob, fill_value, inter_method, data_shape, resize_mode, seed, mean_img, mean_r, mean_g, mean_b, mean_a, std_r, std_g, std_b, std_a, scale, verbose)\n\n\n\n\nCan also be called with the alias \nImageDetRecordProvider\n. Create iterator for image detection dataset packed in recordio.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Dataset Param: Path to image list.\n\n\npath_imgrec::string, optional, default='./data/imgrec.rec'\n: Dataset Param: Path to image record file.\n\n\naug_seq::string, optional, default='det_aug_default'\n: Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters. Make sure you don't use normal augmenters for detection tasks.\n\n\nlabel_width::int, optional, default='-1'\n: Dataset Param: How many labels for an image, -1 for variable label size.\n\n\ndata_shape::Shape(tuple), required\n: Dataset Param: Shape of each instance generated by the DataIter.\n\n\npreprocess_threads::int, optional, default='4'\n: Backend Param: Number of thread to do preprocessing.\n\n\nverbose::boolean, optional, default=1\n: Auxiliary Param: Whether to output parser information.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nshuffle_chunk_size::long (non-negative), optional, default=0\n: the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling\n\n\nshuffle_chunk_seed::int, optional, default='0'\n: the seed for chunk shuffling\n\n\nlabel_pad_width::int, optional, default='0'\n: pad output label width if set larger than 0, -1 for auto estimate\n\n\nlabel_pad_value::float, optional, default=-1\n: label padding value if enabled\n\n\nshuffle::boolean, optional, default=0\n: Augmentation Param: Whether to shuffle data.\n\n\nseed::int, optional, default='0'\n: Augmentation Param: Random Seed.\n\n\nbatch_size::int (non-negative), required\n: Batch size.\n\n\nround_batch::boolean, optional, default=1\n: Whether to use round robin to handle overflow batch or not.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\nresize::int, optional, default='-1'\n: Augmentation Param: scale shorter edge to size before applying other augmentations, -1 to disable.\n\n\nrand_crop_prob::float, optional, default=0\n: Augmentation Param: Probability of random cropping, \n= 0 to disable\n\n\nmin_crop_scales::tuple of \nfloat\n, optional, default=[0]\n: Augmentation Param: Min crop scales.\n\n\nmax_crop_scales::tuple of \nfloat\n, optional, default=[1]\n: Augmentation Param: Max crop scales.\n\n\nmin_crop_aspect_ratios::tuple of \nfloat\n, optional, default=[1]\n: Augmentation Param: Min crop aspect ratios.\n\n\nmax_crop_aspect_ratios::tuple of \nfloat\n, optional, default=[1]\n: Augmentation Param: Max crop aspect ratios.\n\n\nmin_crop_overlaps::tuple of \nfloat\n, optional, default=[0]\n: Augmentation Param: Minimum crop IOU between crop_box and ground-truths.\n\n\nmax_crop_overlaps::tuple of \nfloat\n, optional, default=[1]\n: Augmentation Param: Maximum crop IOU between crop_box and ground-truth.\n\n\nmin_crop_sample_coverages::tuple of \nfloat\n, optional, default=[0]\n: Augmentation Param: Minimum ratio of intersect/crop_area between crop box and ground-truths.\n\n\nmax_crop_sample_coverages::tuple of \nfloat\n, optional, default=[1]\n: Augmentation Param: Maximum ratio of intersect/crop_area between crop box and ground-truths.\n\n\nmin_crop_object_coverages::tuple of \nfloat\n, optional, default=[0]\n: Augmentation Param: Minimum ratio of intersect/gt_area between crop box and ground-truths.\n\n\nmax_crop_object_coverages::tuple of \nfloat\n, optional, default=[1]\n: Augmentation Param: Maximum ratio of intersect/gt_area between crop box and ground-truths.\n\n\nnum_crop_sampler::int, optional, default='1'\n: Augmentation Param: Number of crop samplers.\n\n\ncrop_emit_mode::{'center', 'overlap'},optional, default='center'\n: Augmentation Param: Emition mode for invalid ground-truths after crop. center: emit if centroid of object is out of crop region; overlap: emit if overlap is less than emit_overlap_thresh.\n\n\nemit_overlap_thresh::float, optional, default=0.3\n: Augmentation Param: Emit overlap thresh for emit mode overlap only.\n\n\nmax_crop_trials::Shape(tuple), optional, default=[25]\n: Augmentation Param: Skip cropping if fail crop trail count exceeds this number.\n\n\nrand_pad_prob::float, optional, default=0\n: Augmentation Param: Probability for random padding.\n\n\nmax_pad_scale::float, optional, default=1\n: Augmentation Param: Maximum padding scale.\n\n\nmax_random_hue::int, optional, default='0'\n: Augmentation Param: Maximum random value of H channel in HSL color space.\n\n\nrandom_hue_prob::float, optional, default=0\n: Augmentation Param: Probability to apply random hue.\n\n\nmax_random_saturation::int, optional, default='0'\n: Augmentation Param: Maximum random value of S channel in HSL color space.\n\n\nrandom_saturation_prob::float, optional, default=0\n: Augmentation Param: Probability to apply random saturation.\n\n\nmax_random_illumination::int, optional, default='0'\n: Augmentation Param: Maximum random value of L channel in HSL color space.\n\n\nrandom_illumination_prob::float, optional, default=0\n: Augmentation Param: Probability to apply random illumination.\n\n\nmax_random_contrast::float, optional, default=0\n: Augmentation Param: Maximum random value of delta contrast.\n\n\nrandom_contrast_prob::float, optional, default=0\n: Augmentation Param: Probability to apply random contrast.\n\n\nrand_mirror_prob::float, optional, default=0\n: Augmentation Param: Probability to apply horizontal flip aka. mirror.\n\n\nfill_value::int, optional, default='127'\n: Augmentation Param: Filled color value while padding.\n\n\ninter_method::int, optional, default='1'\n: Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\nresize_mode::{'fit', 'force', 'shrink'},optional, default='force'\n: Augmentation Param: How image data fit in data_shape. force: force reshape to data_shape regardless of aspect ratio; shrink: ensure each side fit in data_shape, preserve aspect ratio; fit: fit image to data_shape, preserve ratio, will upscale if applicable.\n\n\nmean_img::string, optional, default=''\n: Augmentation Param: Mean Image to be subtracted.\n\n\nmean_r::float, optional, default=0\n: Augmentation Param: Mean value on R channel.\n\n\nmean_g::float, optional, default=0\n: Augmentation Param: Mean value on G channel.\n\n\nmean_b::float, optional, default=0\n: Augmentation Param: Mean value on B channel.\n\n\nmean_a::float, optional, default=0\n: Augmentation Param: Mean value on Alpha channel.\n\n\nstd_r::float, optional, default=0\n: Augmentation Param: Standard deviation on R channel.\n\n\nstd_g::float, optional, default=0\n: Augmentation Param: Standard deviation on G channel.\n\n\nstd_b::float, optional, default=0\n: Augmentation Param: Standard deviation on B channel.\n\n\nstd_a::float, optional, default=0\n: Augmentation Param: Standard deviation on Alpha channel.\n\n\nscale::float, optional, default=1\n: Augmentation Param: Scale in color space.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageRecordIter\n \n \nMethod\n.\n\n\nImageRecordIter(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, std_r, std_g, std_b, std_a, scale, max_random_contrast, max_random_illumination, verbose)\n\n\n\n\nCan also be called with the alias \nImageRecordProvider\n. Iterates on image RecordIO files\n\n\nReads batches of images from .rec RecordIO files. One can use $im2rec.py$ tool (in tools/) to pack raw image files into RecordIO files. This iterator is less flexible to customization but is fast and has lot of language bindings. To iterate over raw images directly use $ImageIter$ instead (in Python).\n\n\nExample::\n\n\ndata_iter = mx.io.ImageRecordIter(     path_imgrec=\"./sample.rec\", # The target record file.     data_shape=(3, 227, 227), # Output data shape; 227x227 region will be cropped from the original image.     batch_size=4, # Number of items per batch.     resize=256 # Resize the shorter edge to 256 before cropping.     # You can specify more augmentation options. Use help(mx.io.ImageRecordIter) to see all the options.     )\n\n\nYou can now use the data_iter to access batches of images.\n\n\nbatch = data_iter.next() # first batch.   images = batch.data[0] # This will contain 4 (=batch_size) images each of 3x227x227.\n\n\nprocess the images\n\n\n...   data_iter.reset() # To restart the iterator from the beginning.\n\n\nDefined in src/io/iter_image_recordio_2.cc:L748\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated): \n \n    \n.\n\n\npath_imgrec::string, optional, default=''\n: Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.\n\n\npath_imgidx::string, optional, default=''\n: Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.\n\n\naug_seq::string, optional, default='aug_default'\n: The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.\n\n\nlabel_width::int, optional, default='1'\n: The number of labels per image.\n\n\ndata_shape::Shape(tuple), required\n: The shape of one output image in (channels, height, width) format.\n\n\npreprocess_threads::int, optional, default='4'\n: The number of threads to do preprocessing.\n\n\nverbose::boolean, optional, default=1\n: If or not output verbose information.\n\n\nnum_parts::int, optional, default='1'\n: Virtually partition the data into these many parts.\n\n\npart_index::int, optional, default='0'\n: The \ni\n-th virtual partition to be read.\n\n\nshuffle_chunk_size::long (non-negative), optional, default=0\n: The data shuffle buffer size in MB. Only valid if shuffle is true.\n\n\nshuffle_chunk_seed::int, optional, default='0'\n: The random seed for shuffling\n\n\nshuffle::boolean, optional, default=0\n: Whether to shuffle data randomly or not.\n\n\nseed::int, optional, default='0'\n: The random seed.\n\n\nbatch_size::int (non-negative), required\n: Batch size.\n\n\nround_batch::boolean, optional, default=1\n: Whether to use round robin to handle overflow batch or not.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\nresize::int, optional, default='-1'\n: Down scale the shorter edge to a new size  before applying other augmentations.\n\n\nrand_crop::boolean, optional, default=0\n: If or not randomly crop the image\n\n\nmax_rotate_angle::int, optional, default='0'\n: Rotate by a random degree in $[-v, v]$\n\n\nmax_aspect_ratio::float, optional, default=0\n: Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$\n\n\nmax_shear_ratio::float, optional, default=0\n: Apply a shear transformation (namely $(x,y)-\n(x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$\n\n\nmax_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmin_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmax_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmin_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmax_img_size::float, optional, default=1e+10\n: Set the maximal width and height after all resize and rotate argumentation  are applied\n\n\nmin_img_size::float, optional, default=0\n: Set the minimal width and height after all resize and rotate argumentation  are applied\n\n\nrandom_h::int, optional, default='0'\n: Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.\n\n\nrandom_s::int, optional, default='0'\n: Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.\n\n\nrandom_l::int, optional, default='0'\n: Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.\n\n\nrotate::int, optional, default='-1'\n: Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.\n\n\nfill_value::int, optional, default='255'\n: Set the padding pixes value into $fill_value$.\n\n\ninter_method::int, optional, default='1'\n: The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\npad::int, optional, default='0'\n: Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes\n\n\nmirror::boolean, optional, default=0\n: Whether to mirror the image or not. If true, images are flipped along the horizontal axis.\n\n\nrand_mirror::boolean, optional, default=0\n: Whether to randomly mirror images or not. If true, 50% of the images will be randomly mirrored (flipped along the horizontal axis)\n\n\nmean_img::string, optional, default=''\n: Filename of the mean image.\n\n\nmean_r::float, optional, default=0\n: The mean value to be subtracted on the R channel\n\n\nmean_g::float, optional, default=0\n: The mean value to be subtracted on the G channel\n\n\nmean_b::float, optional, default=0\n: The mean value to be subtracted on the B channel\n\n\nmean_a::float, optional, default=0\n: The mean value to be subtracted on the alpha channel\n\n\nstd_r::float, optional, default=1\n: Augmentation Param: Standard deviation on R channel.\n\n\nstd_g::float, optional, default=1\n: Augmentation Param: Standard deviation on G channel.\n\n\nstd_b::float, optional, default=1\n: Augmentation Param: Standard deviation on B channel.\n\n\nstd_a::float, optional, default=1\n: Augmentation Param: Standard deviation on Alpha channel.\n\n\nscale::float, optional, default=1\n: Multiply the image with a scale value.\n\n\nmax_random_contrast::float, optional, default=0\n: Change the contrast with a value randomly chosen from $[-max_random_contrast, max_random_contrast]$\n\n\nmax_random_illumination::float, optional, default=0\n: Change the illumination with a value randomly chosen from $[-max_random_illumination, max_random_illumination]$\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageRecordIter_v1\n \n \nMethod\n.\n\n\nImageRecordIter_v1(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, std_r, std_g, std_b, std_a, scale, max_random_contrast, max_random_illumination, verbose)\n\n\n\n\nIterating on image RecordIO files\n\n\nRead images batches from RecordIO files with a rich of data augmentation options.\n\n\nOne can use $tools/im2rec.py$ to pack individual image files into RecordIO files.\n\n\nDefined in src/io/iter_image_recordio.cc:L347\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated): \n \n    \n.\n\n\npath_imgrec::string, optional, default=''\n: Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.\n\n\npath_imgidx::string, optional, default=''\n: Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.\n\n\naug_seq::string, optional, default='aug_default'\n: The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.\n\n\nlabel_width::int, optional, default='1'\n: The number of labels per image.\n\n\ndata_shape::Shape(tuple), required\n: The shape of one output image in (channels, height, width) format.\n\n\npreprocess_threads::int, optional, default='4'\n: The number of threads to do preprocessing.\n\n\nverbose::boolean, optional, default=1\n: If or not output verbose information.\n\n\nnum_parts::int, optional, default='1'\n: Virtually partition the data into these many parts.\n\n\npart_index::int, optional, default='0'\n: The \ni\n-th virtual partition to be read.\n\n\nshuffle_chunk_size::long (non-negative), optional, default=0\n: The data shuffle buffer size in MB. Only valid if shuffle is true.\n\n\nshuffle_chunk_seed::int, optional, default='0'\n: The random seed for shuffling\n\n\nshuffle::boolean, optional, default=0\n: Whether to shuffle data randomly or not.\n\n\nseed::int, optional, default='0'\n: The random seed.\n\n\nbatch_size::int (non-negative), required\n: Batch size.\n\n\nround_batch::boolean, optional, default=1\n: Whether to use round robin to handle overflow batch or not.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\nresize::int, optional, default='-1'\n: Down scale the shorter edge to a new size  before applying other augmentations.\n\n\nrand_crop::boolean, optional, default=0\n: If or not randomly crop the image\n\n\nmax_rotate_angle::int, optional, default='0'\n: Rotate by a random degree in $[-v, v]$\n\n\nmax_aspect_ratio::float, optional, default=0\n: Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$\n\n\nmax_shear_ratio::float, optional, default=0\n: Apply a shear transformation (namely $(x,y)-\n(x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$\n\n\nmax_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmin_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmax_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmin_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmax_img_size::float, optional, default=1e+10\n: Set the maximal width and height after all resize and rotate argumentation  are applied\n\n\nmin_img_size::float, optional, default=0\n: Set the minimal width and height after all resize and rotate argumentation  are applied\n\n\nrandom_h::int, optional, default='0'\n: Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.\n\n\nrandom_s::int, optional, default='0'\n: Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.\n\n\nrandom_l::int, optional, default='0'\n: Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.\n\n\nrotate::int, optional, default='-1'\n: Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.\n\n\nfill_value::int, optional, default='255'\n: Set the padding pixes value into $fill_value$.\n\n\ninter_method::int, optional, default='1'\n: The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\npad::int, optional, default='0'\n: Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes\n\n\nmirror::boolean, optional, default=0\n: Whether to mirror the image or not. If true, images are flipped along the horizontal axis.\n\n\nrand_mirror::boolean, optional, default=0\n: Whether to randomly mirror images or not. If true, 50% of the images will be randomly mirrored (flipped along the horizontal axis)\n\n\nmean_img::string, optional, default=''\n: Filename of the mean image.\n\n\nmean_r::float, optional, default=0\n: The mean value to be subtracted on the R channel\n\n\nmean_g::float, optional, default=0\n: The mean value to be subtracted on the G channel\n\n\nmean_b::float, optional, default=0\n: The mean value to be subtracted on the B channel\n\n\nmean_a::float, optional, default=0\n: The mean value to be subtracted on the alpha channel\n\n\nstd_r::float, optional, default=1\n: Augmentation Param: Standard deviation on R channel.\n\n\nstd_g::float, optional, default=1\n: Augmentation Param: Standard deviation on G channel.\n\n\nstd_b::float, optional, default=1\n: Augmentation Param: Standard deviation on B channel.\n\n\nstd_a::float, optional, default=1\n: Augmentation Param: Standard deviation on Alpha channel.\n\n\nscale::float, optional, default=1\n: Multiply the image with a scale value.\n\n\nmax_random_contrast::float, optional, default=0\n: Change the contrast with a value randomly chosen from $[-max_random_contrast, max_random_contrast]$\n\n\nmax_random_illumination::float, optional, default=0\n: Change the illumination with a value randomly chosen from $[-max_random_illumination, max_random_illumination]$\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageRecordUInt8Iter\n \n \nMethod\n.\n\n\nImageRecordUInt8Iter(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad)\n\n\n\n\nCan also be called with the alias \nImageRecordUInt8Provider\n. Iterating on image RecordIO files\n\n\nThis iterator is identical to $ImageRecordIter$ except for using $uint8$ as the data type instead of $float$.\n\n\nDefined in src/io/iter_image_recordio_2.cc:L765\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated): \n \n    \n.\n\n\npath_imgrec::string, optional, default=''\n: Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.\n\n\npath_imgidx::string, optional, default=''\n: Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.\n\n\naug_seq::string, optional, default='aug_default'\n: The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.\n\n\nlabel_width::int, optional, default='1'\n: The number of labels per image.\n\n\ndata_shape::Shape(tuple), required\n: The shape of one output image in (channels, height, width) format.\n\n\npreprocess_threads::int, optional, default='4'\n: The number of threads to do preprocessing.\n\n\nverbose::boolean, optional, default=1\n: If or not output verbose information.\n\n\nnum_parts::int, optional, default='1'\n: Virtually partition the data into these many parts.\n\n\npart_index::int, optional, default='0'\n: The \ni\n-th virtual partition to be read.\n\n\nshuffle_chunk_size::long (non-negative), optional, default=0\n: The data shuffle buffer size in MB. Only valid if shuffle is true.\n\n\nshuffle_chunk_seed::int, optional, default='0'\n: The random seed for shuffling\n\n\nshuffle::boolean, optional, default=0\n: Whether to shuffle data randomly or not.\n\n\nseed::int, optional, default='0'\n: The random seed.\n\n\nbatch_size::int (non-negative), required\n: Batch size.\n\n\nround_batch::boolean, optional, default=1\n: Whether to use round robin to handle overflow batch or not.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\nresize::int, optional, default='-1'\n: Down scale the shorter edge to a new size  before applying other augmentations.\n\n\nrand_crop::boolean, optional, default=0\n: If or not randomly crop the image\n\n\nmax_rotate_angle::int, optional, default='0'\n: Rotate by a random degree in $[-v, v]$\n\n\nmax_aspect_ratio::float, optional, default=0\n: Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$\n\n\nmax_shear_ratio::float, optional, default=0\n: Apply a shear transformation (namely $(x,y)-\n(x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$\n\n\nmax_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmin_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmax_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmin_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmax_img_size::float, optional, default=1e+10\n: Set the maximal width and height after all resize and rotate argumentation  are applied\n\n\nmin_img_size::float, optional, default=0\n: Set the minimal width and height after all resize and rotate argumentation  are applied\n\n\nrandom_h::int, optional, default='0'\n: Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.\n\n\nrandom_s::int, optional, default='0'\n: Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.\n\n\nrandom_l::int, optional, default='0'\n: Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.\n\n\nrotate::int, optional, default='-1'\n: Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.\n\n\nfill_value::int, optional, default='255'\n: Set the padding pixes value into $fill_value$.\n\n\ninter_method::int, optional, default='1'\n: The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\npad::int, optional, default='0'\n: Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageRecordUInt8Iter_v1\n \n \nMethod\n.\n\n\nImageRecordUInt8Iter_v1(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad)\n\n\n\n\nIterating on image RecordIO files\n\n\nThis iterator is identical to $ImageRecordIter$ except for using $uint8$ as the data type instead of $float$.\n\n\nDefined in src/io/iter_image_recordio.cc:L368\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated): \n \n    \n.\n\n\npath_imgrec::string, optional, default=''\n: Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.\n\n\npath_imgidx::string, optional, default=''\n: Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.\n\n\naug_seq::string, optional, default='aug_default'\n: The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.\n\n\nlabel_width::int, optional, default='1'\n: The number of labels per image.\n\n\ndata_shape::Shape(tuple), required\n: The shape of one output image in (channels, height, width) format.\n\n\npreprocess_threads::int, optional, default='4'\n: The number of threads to do preprocessing.\n\n\nverbose::boolean, optional, default=1\n: If or not output verbose information.\n\n\nnum_parts::int, optional, default='1'\n: Virtually partition the data into these many parts.\n\n\npart_index::int, optional, default='0'\n: The \ni\n-th virtual partition to be read.\n\n\nshuffle_chunk_size::long (non-negative), optional, default=0\n: The data shuffle buffer size in MB. Only valid if shuffle is true.\n\n\nshuffle_chunk_seed::int, optional, default='0'\n: The random seed for shuffling\n\n\nshuffle::boolean, optional, default=0\n: Whether to shuffle data randomly or not.\n\n\nseed::int, optional, default='0'\n: The random seed.\n\n\nbatch_size::int (non-negative), required\n: Batch size.\n\n\nround_batch::boolean, optional, default=1\n: Whether to use round robin to handle overflow batch or not.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\nresize::int, optional, default='-1'\n: Down scale the shorter edge to a new size  before applying other augmentations.\n\n\nrand_crop::boolean, optional, default=0\n: If or not randomly crop the image\n\n\nmax_rotate_angle::int, optional, default='0'\n: Rotate by a random degree in $[-v, v]$\n\n\nmax_aspect_ratio::float, optional, default=0\n: Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$\n\n\nmax_shear_ratio::float, optional, default=0\n: Apply a shear transformation (namely $(x,y)-\n(x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$\n\n\nmax_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmin_crop_size::int, optional, default='-1'\n: Crop both width and height into a random size in $[min_crop_size, max_crop_size]$\n\n\nmax_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmin_random_scale::float, optional, default=1\n: Resize into $[width\ns, height\ns]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$\n\n\nmax_img_size::float, optional, default=1e+10\n: Set the maximal width and height after all resize and rotate argumentation  are applied\n\n\nmin_img_size::float, optional, default=0\n: Set the minimal width and height after all resize and rotate argumentation  are applied\n\n\nrandom_h::int, optional, default='0'\n: Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.\n\n\nrandom_s::int, optional, default='0'\n: Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.\n\n\nrandom_l::int, optional, default='0'\n: Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.\n\n\nrotate::int, optional, default='-1'\n: Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.\n\n\nfill_value::int, optional, default='255'\n: Set the padding pixes value into $fill_value$.\n\n\ninter_method::int, optional, default='1'\n: The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\npad::int, optional, default='0'\n: Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LibSVMIter\n \n \nMethod\n.\n\n\nLibSVMIter(data_libsvm, data_shape, label_libsvm, label_shape, num_parts, part_index, batch_size, round_batch, prefetch_buffer, dtype)\n\n\n\n\nCan also be called with the alias \nLibSVMProvider\n. Returns the LibSVM iterator which returns data with \ncsr\n storage type. This iterator is experimental and should be used with care.\n\n\nThe input data is stored in a format similar to LibSVM file format, except that the \nindices are expected to be zero-based instead of one-based, and the column indices for each row are expected to be sorted in ascending order\n. Details of the LibSVM format are available \nhere. \nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n_\n\n\nThe \ndata_shape\n parameter is used to set the shape of each line of the data. The dimension of both \ndata_shape\n and \nlabel_shape\n are expected to be 1.\n\n\nThe \ndata_libsvm\n parameter is used to set the path input LibSVM file. When it is set to a directory, all the files in the directory will be read.\n\n\nWhen \nlabel_libsvm\n is set to $NULL$, both data and label are read from the file specified by \ndata_libsvm\n. In this case, the data is stored in \ncsr\n storage type, while the label is a 1D dense array.\n\n\nThe \nLibSVMIter\n only support \nround_batch\n parameter set to $True$. Therefore, if \nbatch_size\n is 3 and there are 4 total rows in libsvm file, 2 more examples are consumed at the first round.\n\n\nWhen \nnum_parts\n and \npart_index\n are provided, the data is split into \nnum_parts\n partitions, and the iterator only reads the \npart_index\n-th partition. However, the partitions are not guaranteed to be even.\n\n\n$reset()$ is expected to be called only after a complete pass of data.\n\n\nExample::\n\n\nContents of libsvm file $data.t$.\n\n\n1.0 0:0.5 2:1.2   -2.0   -3.0 0:0.6 1:2.4 2:1.2   4 2:-1.2\n\n\nCreates a \nLibSVMIter\n with \nbatch_size\n=3.\n\n\n\n\n\n\n\n\ndata_iter = mx.io.LibSVMIter(data_libsvm = 'data.t', data_shape = (3,), batch_size = 3)\n\n\n\n\n\n\n\n\nThe data of the first batch is stored in csr storage type\n\n\n\n\n\n\n\n\nbatch = data_iter.next() csr = batch.data[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncsr.asnumpy()\n\n\n\n\n\n\n\n\n[[ 0.5        0.          1.2 ]   [ 0.          0.          0.  ]   [ 0.6         2.4         1.2]]\n\n\nThe label of first batch\n\n\n\n\n\n\n\n\nlabel = batch.label[0] label\n\n\n\n\n\n\n\n\n[ 1. -2. -3.]   \n\n\n\n\n\n\n\n\nsecond_batch = data_iter.next()\n\n\n\n\n\n\n\n\nThe data of the second batch\n\n\n\n\n\n\n\n\nsecond_batch.data[0].asnumpy()\n\n\n\n\n\n\n\n\n[[ 0.          0.         -1.2 ]    [ 0.5         0.          1.2 ]    [ 0.          0.          0. ]]\n\n\nThe label of the second batch\n\n\n\n\n\n\n\n\nsecond_batch.label[0].asnumpy()\n\n\n\n\n\n\n\n\n[ 4.  1. -2.]\n\n\n\n\n\n\n\n\ndata_iter.reset()\n\n\n\n\n\n\n\n\nTo restart the iterator for the second pass of the data\n\n\nWhen \nlabel_libsvm\n is set to the path to another LibSVM file, data is read from \ndata_libsvm\n and label from \nlabel_libsvm\n. In this case, both data and label are stored in the csr format. If the label column in the \ndata_libsvm\n file is ignored.\n\n\nExample::\n\n\nContents of libsvm file $label.t$\n\n\n1.0   -2.0 0:0.125   -3.0 2:1.2   4 1:1.0 2:-1.2\n\n\nCreates a \nLibSVMIter\n with specified label file\n\n\n\n\n\n\n\n\ndata_iter = mx.io.LibSVMIter(data_libsvm = 'data.t', data_shape = (3,),\n\n\n\n\n\n\n\n\n               label_libsvm = 'label.t', label_shape = (3,), batch_size = 3)\n\n\n\n\nBoth data and label are in csr storage type\n\n\n\n\n\n\n\n\nbatch = data_iter.next() csr_data = batch.data[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncsr_data.asnumpy()\n\n\n\n\n\n\n\n\n[[ 0.5         0.          1.2  ]    [ 0.          0.          0.   ]    [ 0.6         2.4         1.2 ]]\n\n\n\n\n\n\n\n\ncsr_label = batch.label[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncsr_label.asnumpy()\n\n\n\n\n\n\n\n\n[[ 0.          0.          0.   ]    [ 0.125       0.          0.   ]    [ 0.          0.          1.2 ]]\n\n\nDefined in src/io/iter_libsvm.cc:L298\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\ndata_libsvm::string, required\n: The input zero-base indexed LibSVM data file or a directory path.\n\n\ndata_shape::Shape(tuple), required\n: The shape of one example.\n\n\nlabel_libsvm::string, optional, default='NULL'\n: The input LibSVM label file or a directory path. If NULL, all labels will be read from $data_libsvm$.\n\n\nlabel_shape::Shape(tuple), optional, default=[1]\n: The shape of one label.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nbatch_size::int (non-negative), required\n: Batch size.\n\n\nround_batch::boolean, optional, default=1\n: Whether to use round robin to handle overflow batch or not.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MNISTIter\n \n \nMethod\n.\n\n\nMNISTIter(image, label, batch_size, shuffle, flat, seed, silent, num_parts, part_index, prefetch_buffer, dtype)\n\n\n\n\nCan also be called with the alias \nMNISTProvider\n. Iterating on the MNIST dataset.\n\n\nOne can download the dataset from http://yann.lecun.com/exdb/mnist/\n\n\nDefined in src/io/iter_mnist.cc:L265\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\nimage::string, optional, default='./train-images-idx3-ubyte'\n: Dataset Param: Mnist image path.\n\n\nlabel::string, optional, default='./train-labels-idx1-ubyte'\n: Dataset Param: Mnist label path.\n\n\nbatch_size::int, optional, default='128'\n: Batch Param: Batch Size.\n\n\nshuffle::boolean, optional, default=1\n: Augmentation Param: Whether to shuffle data.\n\n\nflat::boolean, optional, default=0\n: Augmentation Param: Whether to flat the data into 1D.\n\n\nseed::int, optional, default='0'\n: Augmentation Param: Random Seed.\n\n\nsilent::boolean, optional, default=0\n: Auxiliary Param: Whether to print out data info.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Maximum number of batches to prefetch.\n\n\ndtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None'\n: Output data type. $None$ means no change.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.eachbatch\n \n \nMethod\n.\n\n\neachbatch(provider::AbstractDataProvider)\n\n\n\n\nAllows you to perform operations on data every epoch. This is especially useful when you need to perform real-time augmentation of the data.\n\n\nArguments:\n\n\n\n\nprovider\n: an instance of the custom DataProvider type. You must return this\n\n\n\n\ninstance after modifying its fields.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_data!\n \n \nMethod\n.\n\n\nload_data!(provider, batch, targets)\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load data into.\n\n\n\n\nThe targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of \nSlicedNDArray\n. This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where \nslice\n specify the range of samples in the mini-batch that should be loaded into the corresponding \nndarray\n.\n\n\nThis utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_label!\n \n \nMethod\n.\n\n\nload_label!(provider, batch, targets)\n\n\n\n\n\n\nprovider::AbstractDataProvider provider\n: the data provider.\n\n\nbatch::AbstractDataBatch batch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load label into.\n\n\n\n\nThe same as \nload_data!\n, except that this is for loading labels.\n\n\nsource", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/io/#data-providers", 
            "text": "Data providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/io/#abstractdataprovider-interface", 
            "text": "#  MXNet.mx.AbstractDataProvider     Type .  AbstractDataProvider  The root type for all data provider. A data provider should implement the following interfaces:   get_batch_size  provide_data  provide_label   As well as the Julia iterator interface (see  the Julia manual ). Normally this involves defining:   Base.eltype(provider) -  AbstractDataBatch  Base.start(provider) -  AbstractDataProviderState  Base.done(provider, state) -  Bool  Base.next(provider, state) -  (AbstractDataBatch, AbstractDataProvider)   source  The difference between  data  and  label  is that during training stage, both  data  and  label  will be feeded into the model, while during prediction stage, only  data  is loaded. Otherwise, they could be anything, with any names, and of any shapes. The provided data and label names here should match the input names in a target  SymbolicNode .  A data provider should also implement the Julia iteration interface, in order to allow iterating through the data set. The provider will be called in the following way:  for batch in eachbatch(provider)\n    data = get_data(provider, batch)\nend  which will be translated by Julia compiler into  state = Base.start(eachbatch(provider))\nwhile !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\nend  By default,  eachbatch  simply returns the provider itself, so the iterator interface is implemented on the provider type itself. But the extra layer of abstraction allows us to implement a data provider easily via a Julia  Task  coroutine. See the data provider defined in  the char-lstm example  for an example of using coroutine to define data providers.  The detailed interface functions for the iterator API is listed below:  Base.eltype(provider) -  AbstractDataBatch  Returns the specific subtype representing a data batch. See  AbstractDataBatch .    provider::AbstractDataProvider : the data provider.  Base.start(provider) -  AbstractDataProviderState    This function is always called before iterating into the dataset. It should initialize the iterator, reset the index, and do data shuffling if needed.    provider::AbstractDataProvider : the data provider.  Base.done(provider, state) -  Bool    True if there is no more data to iterate in this dataset.   provider::AbstractDataProvider : the data provider.   state::AbstractDataProviderState : the state returned by  Base.start  and  Base.next .  Base.next(provider) -  (AbstractDataBatch, AbstractDataProviderState)    Returns the current data batch, and the state for the next iteration.   provider::AbstractDataProvider : the data provider.   Note sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that   Base.start  will always be called, and called only once before the iteration starts.  Base.done  will always be called at the beginning of every iteration and always be called once.  If  Base.done  return true, the iteration will stop, until the next round, again, starting with a call to  Base.start .  Base.next  will always be called only once in each iteration. It will always be called after one and only one call to  Base.done ; but if  Base.done  returns true,  Base.next  will not be called.   With those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in  MXDataProvider  for example.   Note  Please do not use the one data provider simultaneously in two different places, either in parallel, or in a nested loop. For example, the behavior for the following code is undefined  ```julia\nfor batch in data\n    # updating the parameters  # now let's test the performance on the training set\nfor b2 in data\n    # ...\nend  end\n```   #  MXNet.mx.get_batch_size     Function .  get_batch_size(provider) -  Int  Arguments:   provider::AbstractDataProvider : the data provider.   Returns the mini-batch size of the provided data. All the provided data should have the same mini-batch size (i.e. the last dimension).  source  #  MXNet.mx.provide_data     Function .  provide_data(provider) -  Vector{Tuple{Base.Symbol, Tuple}}  Arguments:   provider::AbstractDataProvider : the data provider.   Returns a vector of (name, shape) pairs describing the names of the data it provides, and the corresponding shapes.  source  #  MXNet.mx.provide_label     Function .  provide_label(provider) -  Vector{Tuple{Base.Symbol, Tuple}}  Arguments:   provider::AbstractDataProvider : the data provider.   Returns a vector of (name, shape) pairs describing the names of the labels it provides, and the corresponding shapes.  source", 
            "title": "AbstractDataProvider interface"
        }, 
        {
            "location": "/api/io/#abstractdatabatch-interface", 
            "text": "#  MXNet.mx.AbstractDataProviderState     Type .  AbstractDataProviderState  Base type for data provider states.  source  #  MXNet.mx.count_samples     Function .  count_samples(provider, batch) -  Int  Arguments:   batch::AbstractDataBatch : the data batch object.   Returns the number of samples in this batch. This number should be greater than 0, but less than or equal to the batch size. This is used to indicate at the end of the data set, there might not be enough samples for a whole mini-batch.  source  #  MXNet.mx.get_data     Function .  get_data(provider, batch) -  Vector{NDArray}  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.   Returns a vector of data in this batch, should be in the same order as declared in  provide_data()  AbstractDataProvider.provide_data .  The last dimension of each  NDArray  should always match the batch_size, even when  count_samples  returns a value less than the batch size. In this case,      the data provider is free to pad the remaining contents with any value.  source  #  MXNet.mx.get_label     Function .  get_label(provider, batch) -  Vector{NDArray}  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.   Returns a vector of labels in this batch. Similar to  get_data .  source  #  Base.get     Function .  get(metric)  Get the accumulated metrics.  Returns  Vector{Tuple{Base.Symbol, Real}} , a list of name-value pairs. For example,  [(:accuracy, 0.9)] .  source  get(provider, batch, name) -  NDArray   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  name::Symbol : the name of the data to get, should be one of the names provided in either  provide_data()  AbstractDataProvider.provide_data  or  provide_label()  AbstractDataProvider.provide_label .   Returns the corresponding data array corresponding to that name.  source  #  MXNet.mx.load_data!     Function .  load_data!(provider, batch, targets)  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load data into.   The targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of  SlicedNDArray . This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where  slice  specify the range of samples in the mini-batch that should be loaded into the corresponding  ndarray .  This utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.  source  #  MXNet.mx.load_label!     Function .  load_label!(provider, batch, targets)   provider::AbstractDataProvider provider : the data provider.  batch::AbstractDataBatch batch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load label into.   The same as  load_data! , except that this is for loading labels.  source", 
            "title": "AbstractDataBatch interface"
        }, 
        {
            "location": "/api/io/#implemented-providers-and-other-methods", 
            "text": "#  MXNet.mx.AbstractDataBatch     Type .  AbstractDataBatch  Base type for a data mini-batch. It should implement the following interfaces:   count_samples  get_data  get_label   The following utility functions will be automatically defined:   get  load_data!  load_label!   source  #  MXNet.mx.ArrayDataProvider     Type .  ArrayDataProvider  A convenient tool to iterate  NDArray  or Julia  Array .  ArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)  Construct a data provider from  NDArray  or Julia Arrays.  Arguments:    data : the data, could be   a  NDArray , or a Julia Array. This is equivalent to  :data =  data .  a name-data pair, like  :mydata =  array , where  :mydata  is the name of the data  and  array  is an  NDArray  or a Julia Array.  a list of name-data pairs.  label : the same as the  data  parameter. When this argument is omitted, the constructed provider will provide no labels.  batch_size::Int : the batch size, default is 0, which means treating the whole array as a single mini-batch.  shuffle::Bool : turn on if the data should be shuffled at every epoch.  data_padding::Real : when the mini-batch goes beyond the dataset boundary, there might be less samples to include than a mini-batch. This value specify a scalar to pad the contents of all the missing data points.  label_padding::Real : the same as  data_padding , except for the labels.     TODO: remove  data_padding  and  label_padding , and implement rollover that copies the last or first several training samples to feed the padding.  source  #  MXNet.mx.DataBatch     Type .  DataBatch  A basic subclass of  AbstractDataBatch , that implement the interface by accessing member fields.  source  #  MXNet.mx.MXDataProvider     Type .  MXDataProvider  A data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.  source  #  MXNet.mx.SlicedNDArray     Type .  SlicedNDArray  A alias type of  Tuple{UnitRange{Int},NDArray} .  source  #  Base.get     Method .  get(provider, batch, name) -  NDArray   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  name::Symbol : the name of the data to get, should be one of the names provided in either  provide_data()  AbstractDataProvider.provide_data  or  provide_label()  AbstractDataProvider.provide_label .   Returns the corresponding data array corresponding to that name.  source  #  MXNet.mx.CSVIter     Method .  CSVIter(data_csv, data_shape, label_csv, label_shape, batch_size, round_batch, prefetch_buffer, dtype)  Can also be called with the alias  CSVProvider . Returns the CSV file iterator.  In this function, the  data_shape  parameter is used to set the shape of each line of the input data. If a row in an input file is  1,2,3,4,5,6``and data_shape` is (3,2), that row will be reshaped, yielding the array [[1,2],[3,4],[5,6]] of shape (3,2).  By default, the  CSVIter  has  round_batch  parameter set to $True$. So, if  batch_size  is 3 and there are 4 total rows in CSV file, 2 more examples are consumed at the first round. If  reset  function is called after first round, the call is ignored and remaining examples are returned in the second round.  If one wants all the instances in the second round after calling  reset , make sure to set  round_batch  to False.  If $data_csv = 'data/'$ is set, then all the files in this directory will be read.  $reset()$ is expected to be called only after a complete pass of data.  Examples::  // Contents of CSV file $data/data.csv$.   1,2,3   2,3,4   3,4,5   4,5,6  // Creates a  CSVIter  with  batch_size =2 and default  round_batch =True.   CSVIter = mx.io.CSVIter(data_csv = 'data/data.csv', data_shape = (3,),   batch_size = 2)  // Two batches read from the above iterator are as follows:   [[ 1.  2.  3.]   [ 2.  3.  4.]]   [[ 3.  4.  5.]   [ 4.  5.  6.]]  // Creates a  CSVIter  with default  round_batch  set to True.   CSVIter = mx.io.CSVIter(data_csv = 'data/data.csv', data_shape = (3,),   batch_size = 3)  // Two batches read from the above iterator in the first pass are as follows:   [[1.  2.  3.]   [2.  3.  4.]   [3.  4.  5.]]  [[4.  5.  6.]   [1.  2.  3.]   [2.  3.  4.]]  // Now,  reset  method is called.   CSVIter.reset()  // Batch read from the above iterator in the second pass is as follows:   [[ 3.  4.  5.]   [ 4.  5.  6.]   [ 1.  2.  3.]]  // Creates a  CSVIter  with  round_batch =False.   CSVIter = mx.io.CSVIter(data_csv = 'data/data.csv', data_shape = (3,),   batch_size = 3, round_batch=False)  // Contents of two batches read from the above iterator in both passes, after calling   //  reset  method before second pass, is as follows:   [[1.  2.  3.]   [2.  3.  4.]   [3.  4.  5.]]  [[4.  5.  6.]   [2.  3.  4.]   [3.  4.  5.]]  Defined in src/io/iter_csv.cc:L223  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  data_csv::string, required : The input CSV file or a directory path.  data_shape::Shape(tuple), required : The shape of one example.  label_csv::string, optional, default='NULL' : The input CSV file or a directory path. If NULL, all labels will be returned as 0.  label_shape::Shape(tuple), optional, default=[1] : The shape of one label.  batch_size::int (non-negative), required : Batch size.  round_batch::boolean, optional, default=1 : Whether to use round robin to handle overflow batch or not.  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageDetRecordIter     Method .  ImageDetRecordIter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, label_pad_width, label_pad_value, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop_prob, min_crop_scales, max_crop_scales, min_crop_aspect_ratios, max_crop_aspect_ratios, min_crop_overlaps, max_crop_overlaps, min_crop_sample_coverages, max_crop_sample_coverages, min_crop_object_coverages, max_crop_object_coverages, num_crop_sampler, crop_emit_mode, emit_overlap_thresh, max_crop_trials, rand_pad_prob, max_pad_scale, max_random_hue, random_hue_prob, max_random_saturation, random_saturation_prob, max_random_illumination, random_illumination_prob, max_random_contrast, random_contrast_prob, rand_mirror_prob, fill_value, inter_method, data_shape, resize_mode, seed, mean_img, mean_r, mean_g, mean_b, mean_a, std_r, std_g, std_b, std_a, scale, verbose)  Can also be called with the alias  ImageDetRecordProvider . Create iterator for image detection dataset packed in recordio.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Dataset Param: Path to image list.  path_imgrec::string, optional, default='./data/imgrec.rec' : Dataset Param: Path to image record file.  aug_seq::string, optional, default='det_aug_default' : Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters. Make sure you don't use normal augmenters for detection tasks.  label_width::int, optional, default='-1' : Dataset Param: How many labels for an image, -1 for variable label size.  data_shape::Shape(tuple), required : Dataset Param: Shape of each instance generated by the DataIter.  preprocess_threads::int, optional, default='4' : Backend Param: Number of thread to do preprocessing.  verbose::boolean, optional, default=1 : Auxiliary Param: Whether to output parser information.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  shuffle_chunk_size::long (non-negative), optional, default=0 : the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling  shuffle_chunk_seed::int, optional, default='0' : the seed for chunk shuffling  label_pad_width::int, optional, default='0' : pad output label width if set larger than 0, -1 for auto estimate  label_pad_value::float, optional, default=-1 : label padding value if enabled  shuffle::boolean, optional, default=0 : Augmentation Param: Whether to shuffle data.  seed::int, optional, default='0' : Augmentation Param: Random Seed.  batch_size::int (non-negative), required : Batch size.  round_batch::boolean, optional, default=1 : Whether to use round robin to handle overflow batch or not.  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.  resize::int, optional, default='-1' : Augmentation Param: scale shorter edge to size before applying other augmentations, -1 to disable.  rand_crop_prob::float, optional, default=0 : Augmentation Param: Probability of random cropping,  = 0 to disable  min_crop_scales::tuple of  float , optional, default=[0] : Augmentation Param: Min crop scales.  max_crop_scales::tuple of  float , optional, default=[1] : Augmentation Param: Max crop scales.  min_crop_aspect_ratios::tuple of  float , optional, default=[1] : Augmentation Param: Min crop aspect ratios.  max_crop_aspect_ratios::tuple of  float , optional, default=[1] : Augmentation Param: Max crop aspect ratios.  min_crop_overlaps::tuple of  float , optional, default=[0] : Augmentation Param: Minimum crop IOU between crop_box and ground-truths.  max_crop_overlaps::tuple of  float , optional, default=[1] : Augmentation Param: Maximum crop IOU between crop_box and ground-truth.  min_crop_sample_coverages::tuple of  float , optional, default=[0] : Augmentation Param: Minimum ratio of intersect/crop_area between crop box and ground-truths.  max_crop_sample_coverages::tuple of  float , optional, default=[1] : Augmentation Param: Maximum ratio of intersect/crop_area between crop box and ground-truths.  min_crop_object_coverages::tuple of  float , optional, default=[0] : Augmentation Param: Minimum ratio of intersect/gt_area between crop box and ground-truths.  max_crop_object_coverages::tuple of  float , optional, default=[1] : Augmentation Param: Maximum ratio of intersect/gt_area between crop box and ground-truths.  num_crop_sampler::int, optional, default='1' : Augmentation Param: Number of crop samplers.  crop_emit_mode::{'center', 'overlap'},optional, default='center' : Augmentation Param: Emition mode for invalid ground-truths after crop. center: emit if centroid of object is out of crop region; overlap: emit if overlap is less than emit_overlap_thresh.  emit_overlap_thresh::float, optional, default=0.3 : Augmentation Param: Emit overlap thresh for emit mode overlap only.  max_crop_trials::Shape(tuple), optional, default=[25] : Augmentation Param: Skip cropping if fail crop trail count exceeds this number.  rand_pad_prob::float, optional, default=0 : Augmentation Param: Probability for random padding.  max_pad_scale::float, optional, default=1 : Augmentation Param: Maximum padding scale.  max_random_hue::int, optional, default='0' : Augmentation Param: Maximum random value of H channel in HSL color space.  random_hue_prob::float, optional, default=0 : Augmentation Param: Probability to apply random hue.  max_random_saturation::int, optional, default='0' : Augmentation Param: Maximum random value of S channel in HSL color space.  random_saturation_prob::float, optional, default=0 : Augmentation Param: Probability to apply random saturation.  max_random_illumination::int, optional, default='0' : Augmentation Param: Maximum random value of L channel in HSL color space.  random_illumination_prob::float, optional, default=0 : Augmentation Param: Probability to apply random illumination.  max_random_contrast::float, optional, default=0 : Augmentation Param: Maximum random value of delta contrast.  random_contrast_prob::float, optional, default=0 : Augmentation Param: Probability to apply random contrast.  rand_mirror_prob::float, optional, default=0 : Augmentation Param: Probability to apply horizontal flip aka. mirror.  fill_value::int, optional, default='127' : Augmentation Param: Filled color value while padding.  inter_method::int, optional, default='1' : Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  resize_mode::{'fit', 'force', 'shrink'},optional, default='force' : Augmentation Param: How image data fit in data_shape. force: force reshape to data_shape regardless of aspect ratio; shrink: ensure each side fit in data_shape, preserve aspect ratio; fit: fit image to data_shape, preserve ratio, will upscale if applicable.  mean_img::string, optional, default='' : Augmentation Param: Mean Image to be subtracted.  mean_r::float, optional, default=0 : Augmentation Param: Mean value on R channel.  mean_g::float, optional, default=0 : Augmentation Param: Mean value on G channel.  mean_b::float, optional, default=0 : Augmentation Param: Mean value on B channel.  mean_a::float, optional, default=0 : Augmentation Param: Mean value on Alpha channel.  std_r::float, optional, default=0 : Augmentation Param: Standard deviation on R channel.  std_g::float, optional, default=0 : Augmentation Param: Standard deviation on G channel.  std_b::float, optional, default=0 : Augmentation Param: Standard deviation on B channel.  std_a::float, optional, default=0 : Augmentation Param: Standard deviation on Alpha channel.  scale::float, optional, default=1 : Augmentation Param: Scale in color space.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageRecordIter     Method .  ImageRecordIter(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, std_r, std_g, std_b, std_a, scale, max_random_contrast, max_random_illumination, verbose)  Can also be called with the alias  ImageRecordProvider . Iterates on image RecordIO files  Reads batches of images from .rec RecordIO files. One can use $im2rec.py$ tool (in tools/) to pack raw image files into RecordIO files. This iterator is less flexible to customization but is fast and has lot of language bindings. To iterate over raw images directly use $ImageIter$ instead (in Python).  Example::  data_iter = mx.io.ImageRecordIter(     path_imgrec=\"./sample.rec\", # The target record file.     data_shape=(3, 227, 227), # Output data shape; 227x227 region will be cropped from the original image.     batch_size=4, # Number of items per batch.     resize=256 # Resize the shorter edge to 256 before cropping.     # You can specify more augmentation options. Use help(mx.io.ImageRecordIter) to see all the options.     )  You can now use the data_iter to access batches of images.  batch = data_iter.next() # first batch.   images = batch.data[0] # This will contain 4 (=batch_size) images each of 3x227x227.  process the images  ...   data_iter.reset() # To restart the iterator from the beginning.  Defined in src/io/iter_image_recordio_2.cc:L748  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated):         .  path_imgrec::string, optional, default='' : Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.  path_imgidx::string, optional, default='' : Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.  aug_seq::string, optional, default='aug_default' : The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.  label_width::int, optional, default='1' : The number of labels per image.  data_shape::Shape(tuple), required : The shape of one output image in (channels, height, width) format.  preprocess_threads::int, optional, default='4' : The number of threads to do preprocessing.  verbose::boolean, optional, default=1 : If or not output verbose information.  num_parts::int, optional, default='1' : Virtually partition the data into these many parts.  part_index::int, optional, default='0' : The  i -th virtual partition to be read.  shuffle_chunk_size::long (non-negative), optional, default=0 : The data shuffle buffer size in MB. Only valid if shuffle is true.  shuffle_chunk_seed::int, optional, default='0' : The random seed for shuffling  shuffle::boolean, optional, default=0 : Whether to shuffle data randomly or not.  seed::int, optional, default='0' : The random seed.  batch_size::int (non-negative), required : Batch size.  round_batch::boolean, optional, default=1 : Whether to use round robin to handle overflow batch or not.  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.  resize::int, optional, default='-1' : Down scale the shorter edge to a new size  before applying other augmentations.  rand_crop::boolean, optional, default=0 : If or not randomly crop the image  max_rotate_angle::int, optional, default='0' : Rotate by a random degree in $[-v, v]$  max_aspect_ratio::float, optional, default=0 : Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$  max_shear_ratio::float, optional, default=0 : Apply a shear transformation (namely $(x,y)- (x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$  max_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  min_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  max_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  min_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  max_img_size::float, optional, default=1e+10 : Set the maximal width and height after all resize and rotate argumentation  are applied  min_img_size::float, optional, default=0 : Set the minimal width and height after all resize and rotate argumentation  are applied  random_h::int, optional, default='0' : Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.  random_s::int, optional, default='0' : Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.  random_l::int, optional, default='0' : Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.  rotate::int, optional, default='-1' : Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.  fill_value::int, optional, default='255' : Set the padding pixes value into $fill_value$.  inter_method::int, optional, default='1' : The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  pad::int, optional, default='0' : Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes  mirror::boolean, optional, default=0 : Whether to mirror the image or not. If true, images are flipped along the horizontal axis.  rand_mirror::boolean, optional, default=0 : Whether to randomly mirror images or not. If true, 50% of the images will be randomly mirrored (flipped along the horizontal axis)  mean_img::string, optional, default='' : Filename of the mean image.  mean_r::float, optional, default=0 : The mean value to be subtracted on the R channel  mean_g::float, optional, default=0 : The mean value to be subtracted on the G channel  mean_b::float, optional, default=0 : The mean value to be subtracted on the B channel  mean_a::float, optional, default=0 : The mean value to be subtracted on the alpha channel  std_r::float, optional, default=1 : Augmentation Param: Standard deviation on R channel.  std_g::float, optional, default=1 : Augmentation Param: Standard deviation on G channel.  std_b::float, optional, default=1 : Augmentation Param: Standard deviation on B channel.  std_a::float, optional, default=1 : Augmentation Param: Standard deviation on Alpha channel.  scale::float, optional, default=1 : Multiply the image with a scale value.  max_random_contrast::float, optional, default=0 : Change the contrast with a value randomly chosen from $[-max_random_contrast, max_random_contrast]$  max_random_illumination::float, optional, default=0 : Change the illumination with a value randomly chosen from $[-max_random_illumination, max_random_illumination]$   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageRecordIter_v1     Method .  ImageRecordIter_v1(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, std_r, std_g, std_b, std_a, scale, max_random_contrast, max_random_illumination, verbose)  Iterating on image RecordIO files  Read images batches from RecordIO files with a rich of data augmentation options.  One can use $tools/im2rec.py$ to pack individual image files into RecordIO files.  Defined in src/io/iter_image_recordio.cc:L347  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated):         .  path_imgrec::string, optional, default='' : Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.  path_imgidx::string, optional, default='' : Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.  aug_seq::string, optional, default='aug_default' : The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.  label_width::int, optional, default='1' : The number of labels per image.  data_shape::Shape(tuple), required : The shape of one output image in (channels, height, width) format.  preprocess_threads::int, optional, default='4' : The number of threads to do preprocessing.  verbose::boolean, optional, default=1 : If or not output verbose information.  num_parts::int, optional, default='1' : Virtually partition the data into these many parts.  part_index::int, optional, default='0' : The  i -th virtual partition to be read.  shuffle_chunk_size::long (non-negative), optional, default=0 : The data shuffle buffer size in MB. Only valid if shuffle is true.  shuffle_chunk_seed::int, optional, default='0' : The random seed for shuffling  shuffle::boolean, optional, default=0 : Whether to shuffle data randomly or not.  seed::int, optional, default='0' : The random seed.  batch_size::int (non-negative), required : Batch size.  round_batch::boolean, optional, default=1 : Whether to use round robin to handle overflow batch or not.  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.  resize::int, optional, default='-1' : Down scale the shorter edge to a new size  before applying other augmentations.  rand_crop::boolean, optional, default=0 : If or not randomly crop the image  max_rotate_angle::int, optional, default='0' : Rotate by a random degree in $[-v, v]$  max_aspect_ratio::float, optional, default=0 : Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$  max_shear_ratio::float, optional, default=0 : Apply a shear transformation (namely $(x,y)- (x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$  max_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  min_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  max_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  min_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  max_img_size::float, optional, default=1e+10 : Set the maximal width and height after all resize and rotate argumentation  are applied  min_img_size::float, optional, default=0 : Set the minimal width and height after all resize and rotate argumentation  are applied  random_h::int, optional, default='0' : Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.  random_s::int, optional, default='0' : Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.  random_l::int, optional, default='0' : Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.  rotate::int, optional, default='-1' : Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.  fill_value::int, optional, default='255' : Set the padding pixes value into $fill_value$.  inter_method::int, optional, default='1' : The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  pad::int, optional, default='0' : Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes  mirror::boolean, optional, default=0 : Whether to mirror the image or not. If true, images are flipped along the horizontal axis.  rand_mirror::boolean, optional, default=0 : Whether to randomly mirror images or not. If true, 50% of the images will be randomly mirrored (flipped along the horizontal axis)  mean_img::string, optional, default='' : Filename of the mean image.  mean_r::float, optional, default=0 : The mean value to be subtracted on the R channel  mean_g::float, optional, default=0 : The mean value to be subtracted on the G channel  mean_b::float, optional, default=0 : The mean value to be subtracted on the B channel  mean_a::float, optional, default=0 : The mean value to be subtracted on the alpha channel  std_r::float, optional, default=1 : Augmentation Param: Standard deviation on R channel.  std_g::float, optional, default=1 : Augmentation Param: Standard deviation on G channel.  std_b::float, optional, default=1 : Augmentation Param: Standard deviation on B channel.  std_a::float, optional, default=1 : Augmentation Param: Standard deviation on Alpha channel.  scale::float, optional, default=1 : Multiply the image with a scale value.  max_random_contrast::float, optional, default=0 : Change the contrast with a value randomly chosen from $[-max_random_contrast, max_random_contrast]$  max_random_illumination::float, optional, default=0 : Change the illumination with a value randomly chosen from $[-max_random_illumination, max_random_illumination]$   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageRecordUInt8Iter     Method .  ImageRecordUInt8Iter(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad)  Can also be called with the alias  ImageRecordUInt8Provider . Iterating on image RecordIO files  This iterator is identical to $ImageRecordIter$ except for using $uint8$ as the data type instead of $float$.  Defined in src/io/iter_image_recordio_2.cc:L765  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated):         .  path_imgrec::string, optional, default='' : Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.  path_imgidx::string, optional, default='' : Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.  aug_seq::string, optional, default='aug_default' : The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.  label_width::int, optional, default='1' : The number of labels per image.  data_shape::Shape(tuple), required : The shape of one output image in (channels, height, width) format.  preprocess_threads::int, optional, default='4' : The number of threads to do preprocessing.  verbose::boolean, optional, default=1 : If or not output verbose information.  num_parts::int, optional, default='1' : Virtually partition the data into these many parts.  part_index::int, optional, default='0' : The  i -th virtual partition to be read.  shuffle_chunk_size::long (non-negative), optional, default=0 : The data shuffle buffer size in MB. Only valid if shuffle is true.  shuffle_chunk_seed::int, optional, default='0' : The random seed for shuffling  shuffle::boolean, optional, default=0 : Whether to shuffle data randomly or not.  seed::int, optional, default='0' : The random seed.  batch_size::int (non-negative), required : Batch size.  round_batch::boolean, optional, default=1 : Whether to use round robin to handle overflow batch or not.  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.  resize::int, optional, default='-1' : Down scale the shorter edge to a new size  before applying other augmentations.  rand_crop::boolean, optional, default=0 : If or not randomly crop the image  max_rotate_angle::int, optional, default='0' : Rotate by a random degree in $[-v, v]$  max_aspect_ratio::float, optional, default=0 : Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$  max_shear_ratio::float, optional, default=0 : Apply a shear transformation (namely $(x,y)- (x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$  max_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  min_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  max_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  min_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  max_img_size::float, optional, default=1e+10 : Set the maximal width and height after all resize and rotate argumentation  are applied  min_img_size::float, optional, default=0 : Set the minimal width and height after all resize and rotate argumentation  are applied  random_h::int, optional, default='0' : Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.  random_s::int, optional, default='0' : Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.  random_l::int, optional, default='0' : Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.  rotate::int, optional, default='-1' : Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.  fill_value::int, optional, default='255' : Set the padding pixes value into $fill_value$.  inter_method::int, optional, default='1' : The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  pad::int, optional, default='0' : Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageRecordUInt8Iter_v1     Method .  ImageRecordUInt8Iter_v1(path_imglist, path_imgrec, path_imgidx, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle_chunk_size, shuffle_chunk_seed, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, resize, rand_crop, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad)  Iterating on image RecordIO files  This iterator is identical to $ImageRecordIter$ except for using $uint8$ as the data type instead of $float$.  Defined in src/io/iter_image_recordio.cc:L368  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Path to the image list (.lst) file. Generally created with tools/im2rec.py. Format (Tab separated):         .  path_imgrec::string, optional, default='' : Path to the image RecordIO (.rec) file or a directory path. Created with tools/im2rec.py.  path_imgidx::string, optional, default='' : Path to the image RecordIO index (.idx) file. Created with tools/im2rec.py.  aug_seq::string, optional, default='aug_default' : The augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.  label_width::int, optional, default='1' : The number of labels per image.  data_shape::Shape(tuple), required : The shape of one output image in (channels, height, width) format.  preprocess_threads::int, optional, default='4' : The number of threads to do preprocessing.  verbose::boolean, optional, default=1 : If or not output verbose information.  num_parts::int, optional, default='1' : Virtually partition the data into these many parts.  part_index::int, optional, default='0' : The  i -th virtual partition to be read.  shuffle_chunk_size::long (non-negative), optional, default=0 : The data shuffle buffer size in MB. Only valid if shuffle is true.  shuffle_chunk_seed::int, optional, default='0' : The random seed for shuffling  shuffle::boolean, optional, default=0 : Whether to shuffle data randomly or not.  seed::int, optional, default='0' : The random seed.  batch_size::int (non-negative), required : Batch size.  round_batch::boolean, optional, default=1 : Whether to use round robin to handle overflow batch or not.  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.  resize::int, optional, default='-1' : Down scale the shorter edge to a new size  before applying other augmentations.  rand_crop::boolean, optional, default=0 : If or not randomly crop the image  max_rotate_angle::int, optional, default='0' : Rotate by a random degree in $[-v, v]$  max_aspect_ratio::float, optional, default=0 : Change the aspect (namely width/height) to a random value in $[1 - max_aspect_ratio, 1 + max_aspect_ratio]$  max_shear_ratio::float, optional, default=0 : Apply a shear transformation (namely $(x,y)- (x+my,y)$) with $m$ randomly chose from $[-max_shear_ratio, max_shear_ratio]$  max_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  min_crop_size::int, optional, default='-1' : Crop both width and height into a random size in $[min_crop_size, max_crop_size]$  max_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  min_random_scale::float, optional, default=1 : Resize into $[width s, height s]$ with $s$ randomly chosen from $[min_random_scale, max_random_scale]$  max_img_size::float, optional, default=1e+10 : Set the maximal width and height after all resize and rotate argumentation  are applied  min_img_size::float, optional, default=0 : Set the minimal width and height after all resize and rotate argumentation  are applied  random_h::int, optional, default='0' : Add a random value in $[-random_h, random_h]$ to the H channel in HSL color space.  random_s::int, optional, default='0' : Add a random value in $[-random_s, random_s]$ to the S channel in HSL color space.  random_l::int, optional, default='0' : Add a random value in $[-random_l, random_l]$ to the L channel in HSL color space.  rotate::int, optional, default='-1' : Rotate by an angle. If set, it overwrites the $max_rotate_angle$ option.  fill_value::int, optional, default='255' : Set the padding pixes value into $fill_value$.  inter_method::int, optional, default='1' : The interpolation method: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  pad::int, optional, default='0' : Change size from $[width, height]$ into $[pad + width + pad, pad + height + pad]$ by padding pixes   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.LibSVMIter     Method .  LibSVMIter(data_libsvm, data_shape, label_libsvm, label_shape, num_parts, part_index, batch_size, round_batch, prefetch_buffer, dtype)  Can also be called with the alias  LibSVMProvider . Returns the LibSVM iterator which returns data with  csr  storage type. This iterator is experimental and should be used with care.  The input data is stored in a format similar to LibSVM file format, except that the  indices are expected to be zero-based instead of one-based, and the column indices for each row are expected to be sorted in ascending order . Details of the LibSVM format are available  here.  https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ _  The  data_shape  parameter is used to set the shape of each line of the data. The dimension of both  data_shape  and  label_shape  are expected to be 1.  The  data_libsvm  parameter is used to set the path input LibSVM file. When it is set to a directory, all the files in the directory will be read.  When  label_libsvm  is set to $NULL$, both data and label are read from the file specified by  data_libsvm . In this case, the data is stored in  csr  storage type, while the label is a 1D dense array.  The  LibSVMIter  only support  round_batch  parameter set to $True$. Therefore, if  batch_size  is 3 and there are 4 total rows in libsvm file, 2 more examples are consumed at the first round.  When  num_parts  and  part_index  are provided, the data is split into  num_parts  partitions, and the iterator only reads the  part_index -th partition. However, the partitions are not guaranteed to be even.  $reset()$ is expected to be called only after a complete pass of data.  Example::  Contents of libsvm file $data.t$.  1.0 0:0.5 2:1.2   -2.0   -3.0 0:0.6 1:2.4 2:1.2   4 2:-1.2  Creates a  LibSVMIter  with  batch_size =3.     data_iter = mx.io.LibSVMIter(data_libsvm = 'data.t', data_shape = (3,), batch_size = 3)     The data of the first batch is stored in csr storage type     batch = data_iter.next() csr = batch.data[0]         csr.asnumpy()     [[ 0.5        0.          1.2 ]   [ 0.          0.          0.  ]   [ 0.6         2.4         1.2]]  The label of first batch     label = batch.label[0] label     [ 1. -2. -3.]        second_batch = data_iter.next()     The data of the second batch     second_batch.data[0].asnumpy()     [[ 0.          0.         -1.2 ]    [ 0.5         0.          1.2 ]    [ 0.          0.          0. ]]  The label of the second batch     second_batch.label[0].asnumpy()     [ 4.  1. -2.]     data_iter.reset()     To restart the iterator for the second pass of the data  When  label_libsvm  is set to the path to another LibSVM file, data is read from  data_libsvm  and label from  label_libsvm . In this case, both data and label are stored in the csr format. If the label column in the  data_libsvm  file is ignored.  Example::  Contents of libsvm file $label.t$  1.0   -2.0 0:0.125   -3.0 2:1.2   4 1:1.0 2:-1.2  Creates a  LibSVMIter  with specified label file     data_iter = mx.io.LibSVMIter(data_libsvm = 'data.t', data_shape = (3,),                    label_libsvm = 'label.t', label_shape = (3,), batch_size = 3)  Both data and label are in csr storage type     batch = data_iter.next() csr_data = batch.data[0]         csr_data.asnumpy()     [[ 0.5         0.          1.2  ]    [ 0.          0.          0.   ]    [ 0.6         2.4         1.2 ]]     csr_label = batch.label[0]         csr_label.asnumpy()     [[ 0.          0.          0.   ]    [ 0.125       0.          0.   ]    [ 0.          0.          1.2 ]]  Defined in src/io/iter_libsvm.cc:L298  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  data_libsvm::string, required : The input zero-base indexed LibSVM data file or a directory path.  data_shape::Shape(tuple), required : The shape of one example.  label_libsvm::string, optional, default='NULL' : The input LibSVM label file or a directory path. If NULL, all labels will be read from $data_libsvm$.  label_shape::Shape(tuple), optional, default=[1] : The shape of one label.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  batch_size::int (non-negative), required : Batch size.  round_batch::boolean, optional, default=1 : Whether to use round robin to handle overflow batch or not.  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.MNISTIter     Method .  MNISTIter(image, label, batch_size, shuffle, flat, seed, silent, num_parts, part_index, prefetch_buffer, dtype)  Can also be called with the alias  MNISTProvider . Iterating on the MNIST dataset.  One can download the dataset from http://yann.lecun.com/exdb/mnist/  Defined in src/io/iter_mnist.cc:L265  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  image::string, optional, default='./train-images-idx3-ubyte' : Dataset Param: Mnist image path.  label::string, optional, default='./train-labels-idx1-ubyte' : Dataset Param: Mnist label path.  batch_size::int, optional, default='128' : Batch Param: Batch Size.  shuffle::boolean, optional, default=1 : Augmentation Param: Whether to shuffle data.  flat::boolean, optional, default=0 : Augmentation Param: Whether to flat the data into 1D.  seed::int, optional, default='0' : Augmentation Param: Random Seed.  silent::boolean, optional, default=0 : Auxiliary Param: Whether to print out data info.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  prefetch_buffer::long (non-negative), optional, default=4 : Maximum number of batches to prefetch.  dtype::{None, 'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='None' : Output data type. $None$ means no change.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.eachbatch     Method .  eachbatch(provider::AbstractDataProvider)  Allows you to perform operations on data every epoch. This is especially useful when you need to perform real-time augmentation of the data.  Arguments:   provider : an instance of the custom DataProvider type. You must return this   instance after modifying its fields.  source  #  MXNet.mx.load_data!     Method .  load_data!(provider, batch, targets)  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load data into.   The targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of  SlicedNDArray . This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where  slice  specify the range of samples in the mini-batch that should be loaded into the corresponding  ndarray .  This utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.  source  #  MXNet.mx.load_label!     Method .  load_label!(provider, batch, targets)   provider::AbstractDataProvider provider : the data provider.  batch::AbstractDataBatch batch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load label into.   The same as  load_data! , except that this is for loading labels.  source", 
            "title": "Implemented providers and other methods"
        }, 
        {
            "location": "/api/ndarray/", 
            "text": "NDArray API\n\n\n\n\nArithmetic Operations\n\n\nIn the following example \ny\n can be a \nReal\n value or another \nNDArray\n\n\n\n\n\n\n\n\nAPI\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n+\n\n\nx .+ y\n\n\nElementwise summation\n\n\n\n\n\n\n-\n\n\nx .- y\n\n\nElementwise minus\n\n\n\n\n\n\n*\n\n\nx .* y\n\n\nElementwise multiplication\n\n\n\n\n\n\n/\n\n\nx ./ y\n\n\nElementwise division\n\n\n\n\n\n\n^\n\n\nx .^ y\n\n\nElementwise power\n\n\n\n\n\n\n%\n\n\nx .% y\n\n\nElementwise modulo\n\n\n\n\n\n\n\n\n\n\nTrigonometric Functions\n\n\n\n\n\n\n\n\nAPI\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\nsin\n\n\nsin.(x)\n\n\nElementwise sine\n\n\n\n\n\n\ncos\n\n\ncos.(x)\n\n\nElementwise cosine\n\n\n\n\n\n\ntan\n\n\ntan.(x)\n\n\nElementwise tangent\n\n\n\n\n\n\nasin\n\n\nasin.(x)\n\n\nElementwise inverse sine\n\n\n\n\n\n\nacos\n\n\nacos.(x)\n\n\nElementwise inverse cosine\n\n\n\n\n\n\natan\n\n\natan.(x)\n\n\nElementwise inverse tangent\n\n\n\n\n\n\n\n\n\n\nHyperbolic Functions\n\n\n\n\n\n\n\n\nAPI\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\nsinh\n\n\nsinh.(x)\n\n\nElementwise hyperbolic sine\n\n\n\n\n\n\ncosh\n\n\ncosh.(x)\n\n\nElementwise hyperbolic cosine\n\n\n\n\n\n\ntanh\n\n\ntanh.(x)\n\n\nElementwise hyperbolic tangent\n\n\n\n\n\n\nasinh\n\n\nasinh.(x)\n\n\nElementwise inverse hyperbolic sine\n\n\n\n\n\n\nacosh\n\n\nacosh.(x)\n\n\nElementwise inverse hyperbolic cosine\n\n\n\n\n\n\natanh\n\n\natanh.(x)\n\n\nElementwise inverse hyperbolic tangent\n\n\n\n\n\n\n\n\n\n\nActivation Functions\n\n\n\n\n\n\n\n\nAPI\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\u03c3\n\n\n\u03c3.(x)\n\n\nSigmoid function\n\n\n\n\n\n\nsigmoid\n\n\nsigmoid.(x)\n\n\nSigmoid function\n\n\n\n\n\n\nrelu\n\n\nrelu.(x)\n\n\nReLU function\n\n\n\n\n\n\nsoftmax\n\n\nsoftmax.(x)\n\n\nSoftmax function\n\n\n\n\n\n\nlog_softmax\n\n\nlog_softmax.(x)\n\n\nSoftmax followed by log\n\n\n\n\n\n\n\n\n\n\nReference\n\n\n#\n\n\nMXNet.mx.NDArray\n \n \nType\n.\n\n\nNDArray\n\n\n\n\nWrapper of the \nNDArray\n type in \nlibmxnet\n. This is the basic building block of tensor-based computation.\n\n\n\n\nNote\n\n\nsince C/C++ use row-major ordering for arrays while Julia follows a   column-major ordering. To keep things consistent, we keep the underlying data   in their original layout, but use \nlanguage-native\n convention when we talk   about shapes. For example, a mini-batch of 100 MNIST images is a tensor of   C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory   have shape (28,28,1,100).\n\n\n\n\nsource\n\n\n#\n\n\nBase.cos\n \n \nFunction\n.\n\n\ncos.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L63\n\n\n\n\nsource\n\n\n#\n\n\nBase.cosh\n \n \nFunction\n.\n\n\ncosh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L216\n\n\n\n\nsource\n\n\n#\n\n\nBase.reshape\n \n \nMethod\n.\n\n\nreshape(arr::NDArray, dim; reverse=false)\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L164\n\n\nsource\n\n\n#\n\n\nBase.reshape\n \n \nMethod\n.\n\n\nreshape(arr::NDArray, dim...; reverse=false)\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L164\n\n\nsource\n\n\n#\n\n\nBase.sin\n \n \nFunction\n.\n\n\nsin.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L46\n\n\n\n\nsource\n\n\n#\n\n\nBase.sinh\n \n \nFunction\n.\n\n\nsinh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L201\n\n\n\n\nsource\n\n\n#\n\n\nBase.tan\n \n \nFunction\n.\n\n\ntan.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L83\n\n\n\n\nsource\n\n\n#\n\n\nBase.tanh\n \n \nFunction\n.\n\n\ntanh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L234\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.clip!\n \n \nMethod\n.\n\n\nclip(x::NDArray, min, max)\nclip!(x::NDArray, min, max)\n\n\n\n\nClips (limits) the values in \nNDArray\n. Given an interval, values outside the interval are clipped to the interval edges. Clipping \nx\n between \nmin\n and \nx\n would be:\n\n\nclip(x, min_, max_) = max(min(x, max_), min_))\n\n\n\n\njulia\n x = NDArray(1:9);\n\njulia\n mx.clip(x, 2, 8)'\n1\u00d79 mx.NDArray{Int64,2} @ CPU0:\n 2  2  3  4  5  6  7  8  8\n\n\n\n\nThe storage type of clip output depends on storage types of inputs and the \nmin\n, \nmax\n parameter values:\n\n\n\n\nclip(default) = default\n\n\nclip(row_sparse, min \n= 0, max \n= 0) = row_sparse\n\n\nclip(csr, min \n= 0, max \n= 0) = csr\n\n\nclip(row_sparse, min \n 0, max \n 0) = default\n\n\nclip(row_sparse, min \n 0, max \n 0) = default\n\n\nclip(csr, min \n 0, max \n 0) = csr\n\n\nclip(csr, min \n 0, max \n 0) = csr\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L486\n\n\nsource\n\n\n#\n\n\nMXNet.mx.clip\n \n \nMethod\n.\n\n\nclip(x::NDArray, min, max)\nclip!(x::NDArray, min, max)\n\n\n\n\nClips (limits) the values in \nNDArray\n. Given an interval, values outside the interval are clipped to the interval edges. Clipping \nx\n between \nmin\n and \nx\n would be:\n\n\nclip(x, min_, max_) = max(min(x, max_), min_))\n\n\n\n\njulia\n x = NDArray(1:9);\n\njulia\n mx.clip(x, 2, 8)'\n1\u00d79 mx.NDArray{Int64,2} @ CPU0:\n 2  2  3  4  5  6  7  8  8\n\n\n\n\nThe storage type of clip output depends on storage types of inputs and the \nmin\n, \nmax\n parameter values:\n\n\n\n\nclip(default) = default\n\n\nclip(row_sparse, min \n= 0, max \n= 0) = row_sparse\n\n\nclip(csr, min \n= 0, max \n= 0) = csr\n\n\nclip(row_sparse, min \n 0, max \n 0) = default\n\n\nclip(row_sparse, min \n 0, max \n 0) = default\n\n\nclip(csr, min \n 0, max \n 0) = csr\n\n\nclip(csr, min \n 0, max \n 0) = csr\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L486\n\n\nsource\n\n\n#\n\n\nMXNet.mx.context\n \n \nMethod\n.\n\n\ncontext(arr::NDArray)\n\n\n\n\nGet the context that this \nNDArray\n lives on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\nempty(dims::Tuple[, ctx::Context = cpu()])\nempty(dim1, dim2, ...)\n\n\n\n\nAllocate memory for an uninitialized \nNDArray\n with specific shape of type Float32.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\nempty(DType, dims[, ctx::Context = cpu()])\nempty(DType, dims)\nempty(DType, dim1, dim2, ...)\n\n\n\n\nAllocate memory for an uninitialized \nNDArray\n with a specified type.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.expand_dims\n \n \nMethod\n.\n\n\nexpand_dims(x::NDArray, dim)\n\n\n\n\nInsert a new axis into \ndim\n.\n\n\njulia\n x\n4 mx.NDArray{Float64,1} @ CPU0:\n 1.0\n 2.0\n 3.0\n 4.0\n\njulia\n mx.expand_dims(x, 1)\n1\u00d74 mx.NDArray{Float64,2} @ CPU0:\n 1.0  2.0  3.0  4.0\n\njulia\n mx.expand_dims(x, 2)\n4\u00d71 mx.NDArray{Float64,2} @ CPU0:\n 1.0\n 2.0\n 3.0\n 4.0\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L289\n\n\nsource\n\n\n#\n\n\nMXNet.mx.log_softmax\n \n \nFunction\n.\n\n\nlog_softmax.(x::NDArray, [dim = ndims(x)])\n\n\n\n\nComputes the log softmax of the input. This is equivalent to computing softmax followed by log.\n\n\njulia\n x 2\u00d73 mx.NDArray{Float64,2} @ CPU0:  1.0  2.0  0.1  0.1  2.0  1.0\n\n\njulia\n mx.log_softmax.(x) 2\u00d73 mx.NDArray{Float64,2} @ CPU0:  -1.41703  -0.41703  -2.31703  -2.31703  -0.41703  -1.41703\n\n\nsource\n\n\n#\n\n\nMXNet.mx.relu\n \n \nFunction\n.\n\n\nrelu.(x::NDArray)\n\n\n\n\nComputes rectified linear.\n\n\n\n\n\n\\max(x, 0)\n\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L84\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sigmoid\n \n \nFunction\n.\n\n\n\u03c3.(x::NDArray)\nsigmoid.(x::NDArray)\n\n\n\n\nComputes sigmoid of x element-wise.\n\n\n\n\n\n\u03c3(x) = \\frac{1}{(1 + exp(-x))}\n\n\n\n\n\nThe storage type of \nsigmoid\n output is always dense.\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L103\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax\n \n \nFunction\n.\n\n\nsoftmax.(x::NDArray, [dim = ndims(x)])\n\n\n\n\nApplies the softmax function.\n\n\nThe resulting array contains elements in the range \n(0, 1)\n and the elements along the given axis sum up to 1.\n\n\n\n\n\nsoftmax(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\n\n\n\n\n\nDefined in src/operator/nn/softmax.cc:L54\n\n\nsource\n\n\n#\n\n\nMXNet.mx.\u03c3\n \n \nFunction\n.\n\n\n\u03c3.(x::NDArray)\nsigmoid.(x::NDArray)\n\n\n\n\nComputes sigmoid of x element-wise.\n\n\n\n\n\n\u03c3(x) = \\frac{1}{(1 + exp(-x))}\n\n\n\n\n\nThe storage type of \nsigmoid\n output is always dense.\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L103\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@inplace\n \n \nMacro\n.\n\n\n@inplace\n\n\n\n\nJulia does not support re-definiton of \n+=\n operator (like \n__iadd__\n in python), When one write \na += b\n, it gets translated to \na = a+b\n. \na+b\n will allocate new memory for the results, and the newly allocated \nNDArray\n object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.\n\n\nThis macro is a simple utility to implement this behavior. Write\n\n\n  @mx.inplace a += b\n\n\n\n\nwill translate into\n\n\n  mx.add_to!(a, b)\n\n\n\n\nwhich will do inplace adding of the contents of \nb\n into \na\n.\n\n\nsource\n\n\n#\n\n\nBase.Iterators.Flatten\n \n \nMethod\n.\n\n\nFlatten(data)\n\n\n\n\nFlattens the input array into a 2-D array by collapsing the higher dimensions.\n\n\n.. note:: \nFlatten\n is deprecated. Use \nflatten\n instead.\n\n\nFor an input array with shape $(d1, d2, ..., dk)$, \nflatten\n operation reshapes the input array into an output array of shape $(d1, d2\n...\ndk)$.\n\n\nExample::\n\n\nx = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L208\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.:%\n \n \nMethod\n.\n\n\n.%(x::NDArray, y::NDArray)\n.%(x::NDArray, y::Real)\n.%(x::Real, y::NDArray)\n\n\n\n\nElementwise modulo for \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.:*\n \n \nMethod\n.\n\n\n.*(x, y)\n\n\n\n\nElementwise multiplication for \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.:*\n \n \nMethod\n.\n\n\n*(A::NDArray, B::NDArray)\n\n\n\n\nMatrix/tensor multiplication.\n\n\nsource\n\n\n#\n\n\nBase.:+\n \n \nMethod\n.\n\n\n+(args...)\n.+(args...)\n\n\n\n\nSummation. Multiple arguments of either scalar or \nNDArray\n could be added together. Note at least the first or second argument needs to be an \nNDArray\n to avoid ambiguity of built-in summation.\n\n\nsource\n\n\n#\n\n\nBase.:-\n \n \nMethod\n.\n\n\n-(x::NDArray)\n-(x, y)\n.-(x, y)\n\n\n\n\nSubtraction \nx - y\n, of scalar types or \nNDArray\n. Or create the negative of \nx\n.\n\n\nsource\n\n\n#\n\n\nBase.:/\n \n \nMethod\n.\n\n\n./(x::NDArray, y::NDArray)\n./(x::NDArray, y::Real)\n./(x::Real, y::NDArray)\n\n\n\n\n\n\nElementwise dividing an \nNDArray\n by a scalar or another \nNDArray\n\n\n\n\nof the same shape.\n\n\n\n\nElementwise divide a scalar by an \nNDArray\n.\n\n\nMatrix division (solving linear systems) is not implemented yet.\n\n\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.dot\n \n \nMethod\n.\n\n\ndot(x::NDArray, y::NDArray)\n\n\n\n\nDefined in src/operator/tensor/dot.cc:L62\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.norm\n \n \nMethod\n.\n\n\nnorm(data)\n\n\n\n\nFlattens the input array and then computes the l2 norm.\n\n\nExamples::\n\n\nx = [[1, 2],        [3, 4]]\n\n\nnorm(x) = [5.47722578]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L257\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\n\n\nsource\n\n\n#\n\n\nBase.Math.cbrt\n \n \nMethod\n.\n\n\ncbrt(data)\n\n\n\n\nReturns element-wise cube-root value of the input.\n\n\n.. math::    cbrt(x) = \\sqrt[3]{x}\n\n\nExample::\n\n\ncbrt([1, 8, -125]) = [1, 2, -5]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L602\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.Math.gamma\n \n \nMethod\n.\n\n\ngamma(data)\n\n\n\n\nReturns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.\n\n\nThe storage type of $gamma$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase._div\n \n \nMethod\n.\n\n\n_div(lhs, rhs)\n\n\n\n\n_div is an alias of elemwise_div.\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nBase._sub\n \n \nMethod\n.\n\n\n_sub(lhs, rhs)\n\n\n\n\n_sub is an alias of elemwise_sub.\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nBase.abs\n \n \nMethod\n.\n\n\nabs(data)\n\n\n\n\nReturns element-wise absolute value of the input.\n\n\nExample::\n\n\nabs([-2, 0, 3]) = [2, 0, 3]\n\n\nThe storage type of $abs$ output depends upon the input storage type:\n\n\n\n\nabs(default) = default\n\n\nabs(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L386\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.acos\n \n \nFunction\n.\n\n\nacos.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L123\n\n\n\n\nsource\n\n\n#\n\n\nBase.acosh\n \n \nFunction\n.\n\n\nacosh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L264\n\n\n\n\nsource\n\n\n#\n\n\nBase.asin\n \n \nFunction\n.\n\n\nasin.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L104\n\n\n\n\nsource\n\n\n#\n\n\nBase.asinh\n \n \nFunction\n.\n\n\nasinh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L250\n\n\n\n\nsource\n\n\n#\n\n\nBase.atan\n \n \nFunction\n.\n\n\natan.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L144\n\n\n\n\nsource\n\n\n#\n\n\nBase.atanh\n \n \nFunction\n.\n\n\natanh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L281\n\n\n\n\nsource\n\n\n#\n\n\nBase.cat\n \n \nMethod\n.\n\n\ncat(dim, xs::NDArray...)\n\n\n\n\nConcate the \nNDArray\ns which have the same element type along the \ndim\n. Building a diagonal matrix is not supported yet.\n\n\nsource\n\n\n#\n\n\nBase.ceil\n \n \nMethod\n.\n\n\nceil(data)\n\n\n\n\nReturns element-wise ceiling of the input.\n\n\nThe ceil of the scalar x is the smallest integer i, such that i \n= x.\n\n\nExample::\n\n\nceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]\n\n\nThe storage type of $ceil$ output depends upon the input storage type:\n\n\n\n\nceil(default) = default\n\n\nceil(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L464\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.convert\n \n \nMethod\n.\n\n\nconvert(::Type{Array{\n:Real}}, x::NDArray)\n\n\n\n\nConvert an \nNDArray\n into a Julia \nArray\n of specific type. Data will be copied.\n\n\nsource\n\n\n#\n\n\nBase.copy!\n \n \nMethod\n.\n\n\ncopy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})\n\n\n\n\nCopy contents of \nsrc\n into \ndst\n.\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\ncopy(arr :: NDArray)\ncopy(arr :: NDArray, ctx :: Context)\ncopy(arr :: Array, ctx :: Context)\n\n\n\n\nCreate a copy of an array. When no \nContext\n is given, create a Julia \nArray\n. Otherwise, create an \nNDArray\n on the specified context.\n\n\nsource\n\n\n#\n\n\nBase.deepcopy\n \n \nMethod\n.\n\n\ndeepcopy(arr::NDArray)\n\n\n\n\nGet a deep copy of the data blob in the form of an NDArray of default storage type. This function blocks. Do not use it in performance critical code.\n\n\nsource\n\n\n#\n\n\nBase.eltype\n \n \nMethod\n.\n\n\neltype(x::NDArray)\n\n\n\n\nGet the element type of an \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nMethod\n.\n\n\nexp(data)\n\n\n\n\nReturns element-wise exponential value of the input.\n\n\n.. math::    exp(x) = e^x \\approx 2.718^x\n\n\nExample::\n\n\nexp([0, 1, 2]) = [1., 2.71828175, 7.38905621]\n\n\nThe storage type of $exp$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L642\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.expm1\n \n \nMethod\n.\n\n\nexpm1(data)\n\n\n\n\nReturns $exp(x) - 1$ computed element-wise on the input.\n\n\nThis function provides greater precision than $exp(x) - 1$ for small values of $x$.\n\n\nThe storage type of $expm1$ output depends upon the input storage type:\n\n\n\n\nexpm1(default) = default\n\n\nexpm1(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L721\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.fill!\n \n \nMethod\n.\n\n\nfill!(arr::NDArray, x)\n\n\n\n\nCreate an \nNDArray\n filled with the value \nx\n, like \nBase.fill!\n.\n\n\nsource\n\n\n#\n\n\nBase.floor\n \n \nMethod\n.\n\n\nfloor(data)\n\n\n\n\nReturns element-wise floor of the input.\n\n\nThe floor of the scalar x is the largest integer i, such that i \n= x.\n\n\nExample::\n\n\nfloor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]\n\n\nThe storage type of $floor$ output depends upon the input storage type:\n\n\n\n\nfloor(default) = default\n\n\nfloor(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L483\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(arr::NDArray, idx)\n\n\n\n\nShortcut for \nslice\n. A typical use is to write\n\n\n  arr[:] += 5\n\n\n\n\nwhich translates into\n\n\n  arr[:] = arr[:] + 5\n\n\n\n\nwhich furthur translates into\n\n\n  setindex!(getindex(arr, Colon()), 5, Colon())\n\n\n\n\n\n\nNote\n\n\nThe behavior is quite different from indexing into Julia's \nArray\n. For example, \narr[2:5]\n create a \ncopy\n of the sub-array for Julia \nArray\n, while for \nNDArray\n, this is a \nslice\n that shares the memory.\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\nShortcut for \nslice\n. \nNOTE\n the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call \nslice\n, which shares the underlying memory.\n\n\nsource\n\n\n#\n\n\nBase.hcat\n \n \nMethod\n.\n\n\nhcat(x::NDArray...)\n\n\n\n\nsource\n\n\n#\n\n\nBase.identity\n \n \nMethod\n.\n\n\nidentity(data)\n\n\n\n\nidentity is an alias of _copy.\n\n\nReturns a copy of the input.\n\n\nFrom:src/operator/tensor/elemwise_unary_op_basic.cc:112\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.length\n \n \nMethod\n.\n\n\nlength(x::NDArray)\n\n\n\n\nGet the number of elements in an \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nMethod\n.\n\n\nlog(data)\n\n\n\n\nReturns element-wise Natural logarithmic value of the input.\n\n\nThe natural logarithm is logarithm in base \ne\n, so that $log(exp(x)) = x$\n\n\nThe storage type of $log$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L654\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log10\n \n \nMethod\n.\n\n\nlog10(data)\n\n\n\n\nReturns element-wise Base-10 logarithmic value of the input.\n\n\n$10**log10(x) = x$\n\n\nThe storage type of $log10$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L666\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log1p\n \n \nMethod\n.\n\n\nlog1p(data)\n\n\n\n\nReturns element-wise $log(1 + x)$ value of the input.\n\n\nThis function is more accurate than $log(1 + x)$  for small $x$ so that :math:\n1+x\\approx 1\n\n\nThe storage type of $log1p$ output depends upon the input storage type:\n\n\n\n\nlog1p(default) = default\n\n\nlog1p(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L703\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log2\n \n \nMethod\n.\n\n\nlog2(data)\n\n\n\n\nReturns element-wise Base-2 logarithmic value of the input.\n\n\n$2**log2(x) = x$\n\n\nThe storage type of $log2$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L678\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.maximum\n \n \nMethod\n.\n\n\nmaximum(arr::NDArray, dims)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L160\n\n\nsource\n\n\n#\n\n\nBase.maximum\n \n \nMethod\n.\n\n\nmaximum(arr::NDArray)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L160\n\n\nsource\n\n\n#\n\n\nBase.mean\n \n \nMethod\n.\n\n\nmean(arr::NDArray, region)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L101\n\n\nsource\n\n\n#\n\n\nBase.mean\n \n \nMethod\n.\n\n\nmean(arr::NDArray)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L101\n\n\nsource\n\n\n#\n\n\nBase.minimum\n \n \nMethod\n.\n\n\nminimum(arr::NDArray, dims)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L174\n\n\nsource\n\n\n#\n\n\nBase.minimum\n \n \nMethod\n.\n\n\nminimum(arr::NDArray)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L174\n\n\nsource\n\n\n#\n\n\nBase.ndims\n \n \nMethod\n.\n\n\nndims(x::NDArray)\n\n\n\n\nGet the number of dimensions of an \nNDArray\n. Is equivalent to \nlength(size(arr))\n.\n\n\nsource\n\n\n#\n\n\nBase.permutedims\n \n \nMethod\n.\n\n\npermutedims(arr::NDArray, axes)\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L253\n\n\nsource\n\n\n#\n\n\nBase.prod\n \n \nMethod\n.\n\n\nprod(arr::NDArray, dims)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L116\n\n\nsource\n\n\n#\n\n\nBase.prod\n \n \nMethod\n.\n\n\nprod(arr::NDArray)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L116\n\n\nsource\n\n\n#\n\n\nBase.repeat\n \n \nMethod\n.\n\n\nrepeat(data, repeats, axis)\n\n\n\n\nRepeats elements of an array.\n\n\nBy default, $repeat$ flattens the input array into 1-D and then repeats the elements::\n\n\nx = [[ 1, 2],        [ 3, 4]]\n\n\nrepeat(x, repeats=2) = [ 1.,  1.,  2.,  2.,  3.,  3.,  4.,  4.]\n\n\nThe parameter $axis$ specifies the axis along which to perform repeat::\n\n\nrepeat(x, repeats=2, axis=1) = [[ 1.,  1.,  2.,  2.],                                   [ 3.,  3.,  4.,  4.]]\n\n\nrepeat(x, repeats=2, axis=0) = [[ 1.,  2.],                                   [ 1.,  2.],                                   [ 3.,  4.],                                   [ 3.,  4.]]\n\n\nrepeat(x, repeats=2, axis=-1) = [[ 1.,  1.,  2.,  2.],                                    [ 3.,  3.,  4.,  4.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L560\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\nrepeats::int, required\n: The number of repetitions for each element.\n\n\naxis::int or None, optional, default='None'\n: The axis along which to repeat values. The negative numbers are interpreted counting from the backward. By default, use the flattened input array, and return a flat output array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.reverse\n \n \nMethod\n.\n\n\nreverse(data, axis)\n\n\n\n\nReverses the order of elements along given axis while preserving array shape.\n\n\nNote: reverse and flip are equivalent. We use reverse in the following examples.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]\n\n\nreverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]\n\n\nreverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L662\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\naxis::Shape(tuple), required\n: The axis which to reverse elements.\n\n\n\n\nsource\n\n\n#\n\n\nBase.round\n \n \nMethod\n.\n\n\nround(data)\n\n\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\nExample::\n\n\nround([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]\n\n\nThe storage type of $round$ output depends upon the input storage type:\n\n\n\n\nround(default) = default\n\n\nround(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L424\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.setindex!\n \n \nMethod\n.\n\n\nsetindex!(arr::NDArray, val, idx)\n\n\n\n\nAssign values to an \nNDArray\n. The following scenarios are supported\n\n\n\n\nsingle value assignment via linear indexing: \narr[42] = 24\n\n\narr[:] = val\n: whole array assignment, \nval\n could be a scalar or an array (Julia \nArray\n or \nNDArray\n) of the same shape.\n\n\narr[start:stop] = val\n: assignment to a \nslice\n, \nval\n could be a scalar or an array of the same shape to the slice. See also \nslice\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sign\n \n \nMethod\n.\n\n\nsign(data)\n\n\n\n\nReturns element-wise sign of the input.\n\n\nExample::\n\n\nsign([-2, 0, 3]) = [-1, 0, 1]\n\n\nThe storage type of $sign$ output depends upon the input storage type:\n\n\n\n\nsign(default) = default\n\n\nsign(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L405\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.similar\n \n \nMethod\n.\n\n\nsimilar(x::NDArray)\n\n\n\n\nCreate an \nNDArray\n with similar shape, data type, and context with the given one. Note that the returned \nNDArray\n is uninitialized.\n\n\nsource\n\n\n#\n\n\nBase.size\n \n \nMethod\n.\n\n\nsize(x::NDArray)\nsize(x::NDArray, dims...)\n\n\n\n\nGet the shape of an \nNDArray\n. The shape is in Julia's column-major convention. See also the notes on NDArray shapes \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.sort\n \n \nMethod\n.\n\n\nsort(data, axis, is_ascend)\n\n\n\n\nReturns a sorted copy of an input array along the given axis.\n\n\nExamples::\n\n\nx = [[ 1, 4],        [ 3, 1]]\n\n\n// sorts along the last axis   sort(x) = [[ 1.,  4.],              [ 1.,  3.]]\n\n\n// flattens and then sorts   sort(x) = [ 1.,  1.,  3.,  4.]\n\n\n// sorts along the first axis   sort(x, axis=0) = [[ 1.,  1.],                      [ 3.,  4.]]\n\n\n// in a descend order   sort(x, is_ascend=0) = [[ 4.,  1.],                           [ 3.,  1.]]\n\n\nDefined in src/operator/tensor/ordering_op.cc:L126\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=1\n: Whether to sort in ascending or descending order.\n\n\n\n\nsource\n\n\n#\n\n\nBase.split\n \n \nMethod\n.\n\n\nsplit(data, num_outputs, axis, squeeze_axis)\n\n\n\n\nsplit is an alias of SliceChannel.\n\n\nSplits an array along a particular axis into multiple sub-arrays.\n\n\n.. note:: $SliceChannel$ is deprecated. Use $split$ instead.\n\n\nNote\n that \nnum_outputs\n should evenly divide the length of the axis along which to split the array.\n\n\nExample::\n\n\nx  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)\n\n\ny = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]\n\n\n   [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]\n\n\n\n\ny[0].shape = (3, 1, 1)\n\n\nz = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]\n\n\n   [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]\n\n\n\n\nz[0].shape = (1, 2, 1)\n\n\nsqueeze_axis=1\n removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $1$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to true only if $input.shape[axis] == num_outputs$.\n\n\nExample::\n\n\nz = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]\n\n\n   [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]\n\n\n\n\nz[0].shape = (2 ,1 )\n\n\nDefined in src/operator/slice_channel.cc:L107\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nnum_outputs::int, required\n: Number of splits. Note that this should evenly divide the length of the \naxis\n.\n\n\naxis::int, optional, default='1'\n: Axis along which to split.\n\n\nsqueeze_axis::boolean, optional, default=0\n: If true, Removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $true$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to $true$ only if $input.shape[axis] == num_outputs$.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sqrt\n \n \nMethod\n.\n\n\nsqrt(data)\n\n\n\n\nReturns element-wise square-root value of the input.\n\n\n.. math::    \\textrm{sqrt}(x) = \\sqrt{x}\n\n\nExample::\n\n\nsqrt([4, 9, 16]) = [2, 3, 4]\n\n\nThe storage type of $sqrt$ output depends upon the input storage type:\n\n\n\n\nsqrt(default) = default\n\n\nsqrt(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L565\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\nsum(arr::NDArray, dims)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L85\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\nsum(arr::NDArray)\n\n\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L85\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\ntranspose(arr::NDArray{T, 1}) where T\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L164\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\ntranspose(arr::NDArray{T, 2}) where T\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L253\n\n\nsource\n\n\n#\n\n\nBase.trunc\n \n \nMethod\n.\n\n\ntrunc(data)\n\n\n\n\nReturn the element-wise truncated value of the input.\n\n\nThe truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.\n\n\nExample::\n\n\ntrunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]\n\n\nThe storage type of $trunc$ output depends upon the input storage type:\n\n\n\n\ntrunc(default) = default\n\n\ntrunc(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L503\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nBase.vcat\n \n \nMethod\n.\n\n\nvcat(x::NDArray...)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Activation\n \n \nMethod\n.\n\n\nActivation(data, act_type)\n\n\n\n\nApplies an activation function element-wise to the input.\n\n\nThe following activation functions are supported:\n\n\n\n\nrelu\n: Rectified Linear Unit, :math:\ny = max(x, 0)\n\n\nsigmoid\n: :math:\ny = \\frac{1}{1 + exp(-x)}\n\n\ntanh\n: Hyperbolic tangent, :math:\ny = \\frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}\n\n\nsoftrelu\n: Soft ReLU, or SoftPlus, :math:\ny = log(1 + exp(x))\n\n\n\n\nDefined in src/operator/nn/activation.cc:L92\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to activation function.\n\n\nact_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required\n: Activation function to be applied.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm\n \n \nMethod\n.\n\n\nBatchNorm(data, gamma, beta, moving_mean, moving_var, eps, momentum, fix_gamma, use_global_stats, output_mean_var, axis, cudnn_off)\n\n\n\n\nBatch normalization.\n\n\nNormalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.\n\n\nAssume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:\n\n\n.. math::\n\n\ndata_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])\n\n\nThen compute the normalized output, which has the same shape as input, as following:\n\n\n.. math::\n\n\nout[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]\n\n\nBoth \nmean\n and \nvar\n returns a scalar by treating the input as a vector.\n\n\nAssume the input has size \nk\n on axis 1, then both $gamma$ and $beta$ have shape \n(k,)\n. If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.\n\n\nBesides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are \nk\n-length vectors. They are global statistics for the whole dataset, which are updated by::\n\n\nmoving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)\n\n\nIf $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.\n\n\nThe parameter $axis$ specifies which axis of the input shape denotes the 'channel' (separately normalized groups).  The default is 1.  Specifying -1 sets the channel axis to be the last item in the input shape.\n\n\nBoth $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.\n\n\nDefined in src/operator/nn/batch_norm.cc:L400\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to batch normalization\n\n\ngamma::NDArray-or-SymbolicNode\n: gamma array\n\n\nbeta::NDArray-or-SymbolicNode\n: beta array\n\n\nmoving_mean::NDArray-or-SymbolicNode\n: running mean of input\n\n\nmoving_var::NDArray-or-SymbolicNode\n: running variance of input\n\n\neps::double, optional, default=0.001\n: Epsilon to prevent div 0. Must be no less than CUDNN_BN_MIN_EPSILON defined in cudnn.h when using cudnn (usually 1e-5)\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=1\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=0\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\noutput_mean_var::boolean, optional, default=0\n: Output All,normal mean and var\n\n\naxis::int, optional, default='1'\n: Specify which shape axis the channel is specified\n\n\ncudnn_off::boolean, optional, default=0\n: Do not select CUDNN operator, if available\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm_v1\n \n \nMethod\n.\n\n\nBatchNorm_v1(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)\n\n\n\n\nBatch normalization.\n\n\nNormalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.\n\n\nAssume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:\n\n\n.. math::\n\n\ndata_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])\n\n\nThen compute the normalized output, which has the same shape as input, as following:\n\n\n.. math::\n\n\nout[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]\n\n\nBoth \nmean\n and \nvar\n returns a scalar by treating the input as a vector.\n\n\nAssume the input has size \nk\n on axis 1, then both $gamma$ and $beta$ have shape \n(k,)\n. If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.\n\n\nBesides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are \nk\n-length vectors. They are global statistics for the whole dataset, which are updated by::\n\n\nmoving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)\n\n\nIf $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.\n\n\nBoth $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.\n\n\nDefined in src/operator/batch_norm_v1.cc:L90\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to batch normalization\n\n\ngamma::NDArray-or-SymbolicNode\n: gamma array\n\n\nbeta::NDArray-or-SymbolicNode\n: beta array\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent div 0\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=1\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=0\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\noutput_mean_var::boolean, optional, default=0\n: Output All,normal mean and var\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BilinearSampler\n \n \nMethod\n.\n\n\nBilinearSampler(data, grid)\n\n\n\n\nApplies bilinear sampling to input feature map.\n\n\nBilinear Sampling is the key of  [NIPS2015] \\\"Spatial Transformer Networks\\\". The usage of the operator is very similar to remap function in OpenCV, except that the operator has the backward pass.\n\n\nGiven :math:\ndata\n and :math:\ngrid\n, then the output is computed by\n\n\n.. math::   x_{src} = grid[batch, 0, y_{dst}, x_{dst}] \\\n  y_{src} = grid[batch, 1, y_{dst}, x_{dst}] \\\n  output[batch, channel, y_{dst}, x_{dst}] = G(data[batch, channel, y_{src}, x_{src})\n\n\n:math:\nx_{dst}\n, :math:\ny_{dst}\n enumerate all spatial locations in :math:\noutput\n, and :math:\nG()\n denotes the bilinear interpolation kernel. The out-boundary points will be padded with zeros.The shape of the output will be (data.shape[0], data.shape[1], grid.shape[2], grid.shape[3]).\n\n\nThe operator assumes that :math:\ndata\n has 'NCHW' layout and :math:\ngrid\n has been normalized to [-1, 1].\n\n\nBilinearSampler often cooperates with GridGenerator which generates sampling grids for BilinearSampler. GridGenerator supports two kinds of transformation: $affine$ and $warp$. If users want to design a CustomOp to manipulate :math:\ngrid\n, please firstly refer to the code of GridGenerator.\n\n\nExample 1::\n\n\nZoom out data two times\n\n\ndata = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])\n\n\naffine_matrix = array([[2, 0, 0],                          [0, 2, 0]])\n\n\naffine_matrix = reshape(affine_matrix, shape=(1, 6))\n\n\ngrid = GridGenerator(data=affine_matrix, transform_type='affine', target_shape=(4, 4))\n\n\nout = BilinearSampler(data, grid)\n\n\nout   [[[[ 0,   0,     0,   0],      [ 0,   3.5,   6.5, 0],      [ 0,   1.25,  2.5, 0],      [ 0,   0,     0,   0]]]\n\n\nExample 2::\n\n\nshift data horizontally by -1 pixel\n\n\ndata = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])\n\n\nwarp_maxtrix = array([[[[1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1]],                          [[0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0]]]])\n\n\ngrid = GridGenerator(data=warp_matrix, transform_type='warp')   out = BilinearSampler(data, grid)\n\n\nout   [[[[ 4,  3,  6,  0],      [ 8,  8,  9,  0],      [ 4,  1,  5,  0],      [ 0,  1,  3,  0]]]\n\n\nDefined in src/operator/bilinear_sampler.cc:L245\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the BilinearsamplerOp.\n\n\ngrid::NDArray-or-SymbolicNode\n: Input grid to the BilinearsamplerOp.grid has two channels: x_src, y_src\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BlockGrad\n \n \nMethod\n.\n\n\nBlockGrad(data)\n\n\n\n\nStops gradient computation.\n\n\nStops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.\n\n\nExample::\n\n\nv1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)\n\n\nexecutor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]\n\n\nexecutor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L167\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Cast\n \n \nMethod\n.\n\n\nCast(data, dtype)\n\n\n\n\nCasts all elements of the input to a new type.\n\n\n.. note:: $Cast$ is deprecated. Use $cast$ instead.\n\n\nExample::\n\n\ncast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L311\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Output data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Concat\n \n \nMethod\n.\n\n\nConcat(data, num_args, dim)\n\n\n\n\nNote\n: Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.\n\n\nJoins input arrays along a given axis.\n\n\n.. note:: \nConcat\n is deprecated. Use \nconcat\n instead.\n\n\nThe dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.\n\n\nExample::\n\n\nx = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]\n\n\nconcat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]\n\n\nNote that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.\n\n\nconcat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]\n\n\nDefined in src/operator/concat.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: List of arrays to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution\n \n \nMethod\n.\n\n\nConvolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nCompute \nN\n-D convolution on \n(N+2)\n-D input.\n\n\nIn the 2-D convolution, given input data with shape \n(batch_size, channel, height, width)\n, the output is computed by\n\n\n.. math::\n\n\nout[n,i,:,:] = bias[i] + \\sum_{j=0}^{channel} data[n,j,:,:] \\star    weight[i,j,:,:]\n\n\nwhere :math:\n\\star\n is the 2-D cross-correlation operator.\n\n\nFor general 2-D convolution, the shapes are\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0], kernel[1])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n.\n\n\n\n\nDefine::\n\n\nf(x,k,p,s,d) = floor((x+2\np-d\n(k-1)-1)/s)+1\n\n\nthen we have::\n\n\nout_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])\n\n\nIf $no_bias$ is set to be true, then the $bias$ term is ignored.\n\n\nThe default data $layout$ is \nNCHW\n, namely \n(batch_size, channel, height, width)\n. We can choose other layouts such as \nNHWC\n.\n\n\nIf $num_group$ is larger than 1, denoted by \ng\n, then split the input $data$ evenly into \ng\n parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the \ni\n-th part of the data with the \ni\n-th weight part. The output is obtained by concatenating all the \ng\n results.\n\n\n1-D convolution does not have \nheight\n dimension but only \nwidth\n in space.\n\n\n\n\ndata\n: \n(batch_size, channel, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_width)\n.\n\n\n\n\n3-D convolution adds an additional \ndepth\n dimension besides \nheight\n and \nwidth\n. The shapes are\n\n\n\n\ndata\n: \n(batch_size, channel, depth, height, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0], kernel[1], kernel[2])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_depth, out_height, out_width)\n.\n\n\n\n\nBoth $weight$ and $bias$ are learnable parameters.\n\n\nThere are other options to tune the performance.\n\n\n\n\n\n\ncudnn_tune\n: enable this option leads to higher startup time but may give faster speed. Options are\n\n\n\n\noff\n: no tuning\n\n\nlimited_workspace\n:run test and pick the fastest algorithm that doesn't exceed workspace limit.\n\n\nfastest\n: pick the fastest algorithm and ignore workspace limit.\n\n\nNone\n (default): the behavior is determined by environment variable $MXNET_CUDNN_AUTOTUNE_DEFAULT$. 0 for off, 1 for limited workspace (default), 2 for fastest.\n\n\nworkspace\n: A large number leads to more (GPU) memory usage but may improve the performance.\n\n\n\n\n\n\n\n\nDefined in src/operator/nn/convolution.cc:L170\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: Convolution kernel size: (w,), (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=[]\n: Convolution stride: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\ndilate::Shape(tuple), optional, default=[]\n: Convolution dilate: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: Zero pad for convolution: (w,), (h, w) or (d, h, w). Defaults to no padding.\n\n\nnum_filter::int (non-negative), required\n: Convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions.\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum temporary workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algo by running performance test.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution_v1\n \n \nMethod\n.\n\n\nConvolution_v1(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nThis operator is DEPRECATED. Apply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ConvolutionV1Op.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: convolution kernel size: (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=[]\n: convolution stride: (h, w) or (d, h, w)\n\n\ndilate::Shape(tuple), optional, default=[]\n: convolution dilate: (h, w) or (d, h, w)\n\n\npad::Shape(tuple), optional, default=[]\n: pad for convolution: (h, w) or (d, h, w)\n\n\nnum_filter::int (non-negative), required\n: convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum tmp workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Correlation\n \n \nMethod\n.\n\n\nCorrelation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)\n\n\n\n\nApplies correlation to inputs.\n\n\nThe correlation layer performs multiplicative patch comparisons between two feature maps.\n\n\nGiven two multi-channel feature maps :math:\nf_{1}, f_{2}\n, with :math:\nw\n, :math:\nh\n, and :math:\nc\n being their width, height, and number of channels, the correlation layer lets the network compare each patch from :math:\nf_{1}\n with each patch from :math:\nf_{2}\n.\n\n\nFor now we consider only a single comparison of two patches. The 'correlation' of two patches centered at :math:\nx_{1}\n in the first map and :math:\nx_{2}\n in the second map is then defined as:\n\n\n.. math::    c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k] \\times [-k,k]} \n\n\nfor a square patch of size :math:\nK:=2k+1\n.\n\n\nNote that the equation above is identical to one step of a convolution in neural networks, but instead of convolving data with a filter, it convolves data with other data. For this reason, it has no training weights.\n\n\nComputing :math:\nc(x_{1}, x_{2})\n involves :math:\nc * K^{2}\n multiplications. Comparing all patch combinations involves :math:\nw^{2}*h^{2}\n such computations.\n\n\nGiven a maximum displacement :math:\nd\n, for each location :math:\nx_{1}\n it computes correlations :math:\nc(x_{1}, x_{2})\n only in a neighborhood of size :math:\nD:=2d+1\n, by limiting the range of :math:\nx_{2}\n. We use strides :math:\ns_{1}, s_{2}\n, to quantize :math:\nx_{1}\n globally and to quantize :math:\nx_{2}\n within the neighborhood centered around :math:\nx_{1}\n.\n\n\nThe final output is defined by the following expression:\n\n\n.. math::   out[n, q, i, j] = c(x_{i, j}, x_{q})\n\n\nwhere :math:\ni\n and :math:\nj\n enumerate spatial locations in :math:\nf_{1}\n, and :math:\nq\n denotes the :math:\nq^{th}\n neighborhood of :math:\nx_{i,j}\n.\n\n\nDefined in src/operator/correlation.cc:L192\n\n\nArguments\n\n\n\n\ndata1::NDArray-or-SymbolicNode\n: Input data1 to the correlation.\n\n\ndata2::NDArray-or-SymbolicNode\n: Input data2 to the correlation.\n\n\nkernel_size::int (non-negative), optional, default=1\n: kernel size for Correlation must be an odd number\n\n\nmax_displacement::int (non-negative), optional, default=1\n: Max displacement of Correlation\n\n\nstride1::int (non-negative), optional, default=1\n: stride1 quantize data1 globally\n\n\nstride2::int (non-negative), optional, default=1\n: stride2 quantize data2 within the neighborhood centered around data1\n\n\npad_size::int (non-negative), optional, default=0\n: pad for Correlation\n\n\nis_multiply::boolean, optional, default=1\n: operation type is either multiplication or subduction\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Crop\n \n \nMethod\n.\n\n\nCrop(data, num_args, offset, h_w, center_crop)\n\n\n\n\nNote\n: Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.\n\n\n.. note:: \nCrop\n is deprecated. Use \nslice\n instead.\n\n\nCrop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used\n\n\nDefined in src/operator/crop.cc:L50\n\n\nArguments\n\n\n\n\ndata::SymbolicNode or SymbolicNode[]\n: Tensor or List of Tensors, the second input will be used as crop_like shape reference\n\n\nnum_args::int, required\n: Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here\n\n\noffset::Shape(tuple), optional, default=[0,0]\n: crop offset coordinate: (y, x)\n\n\nh_w::Shape(tuple), optional, default=[0,0]\n: crop height and width: (h, w)\n\n\ncenter_crop::boolean, optional, default=0\n: If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Custom\n \n \nMethod\n.\n\n\nCustom(data, op_type)\n\n\n\n\nApply a custom operator implemented in a frontend language (like Python).\n\n\nCustom operators should override required methods like \nforward\n and \nbackward\n. The custom operator must be registered before it can be used. Please check the tutorial here: http://mxnet.io/how_to/new_op.html.\n\n\nDefined in src/operator/custom/custom.cc:L378\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Input data for the custom operator.\n\n\nop_type::string\n: Name of the custom operator. This is the name that is passed to \nmx.operator.register\n to register the operator.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Deconvolution\n \n \nMethod\n.\n\n\nDeconvolution(data, weight, bias, kernel, stride, dilate, pad, adj, target_shape, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nComputes 1D or 2D transposed convolution (aka fractionally strided convolution) of the input tensor. This operation can be seen as the gradient of Convolution operation with respect to its input. Convolution usually reduces the size of the input. Transposed convolution works the other way, going from a smaller input to a larger output while preserving the connectivity pattern.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input tensor to the deconvolution operation.\n\n\nweight::NDArray-or-SymbolicNode\n: Weights representing the kernel.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias added to the result after the deconvolution operation.\n\n\nkernel::Shape(tuple), required\n: Deconvolution kernel size: (w,), (h, w) or (d, h, w). This is same as the kernel size used for the corresponding convolution\n\n\nstride::Shape(tuple), optional, default=[]\n: The stride used for the corresponding convolution: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\ndilate::Shape(tuple), optional, default=[]\n: Dilation factor for each dimension of the input: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: The amount of implicit zero padding added during convolution for each dimension of the input: (w,), (h, w) or (d, h, w). $(kernel-1)/2$ is usually a good choice. If \ntarget_shape\n is set, \npad\n will be ignored and a padding that will generate the target shape will be used. Defaults to no padding.\n\n\nadj::Shape(tuple), optional, default=[]\n: Adjustment for output shape: (w,), (h, w) or (d, h, w). If \ntarget_shape\n is set, \nadj\n will be ignored and computed accordingly.\n\n\ntarget_shape::Shape(tuple), optional, default=[]\n: Shape of the output tensor: (w,), (h, w) or (d, h, w).\n\n\nnum_filter::int (non-negative), required\n: Number of output filters.\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of groups partition.\n\n\nworkspace::long (non-negative), optional, default=512\n: Maximum temporal workspace allowed for deconvolution (MB).\n\n\nno_bias::boolean, optional, default=1\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algorithm by running performance test.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for default layout, NCW for 1d, NCHW for 2d and NCDHW for 3d.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Dropout\n \n \nMethod\n.\n\n\nDropout(data, p, mode)\n\n\n\n\nApplies dropout operation to input array.\n\n\n\n\nDuring training, each element of the input is set to zero with probability p. The whole array is rescaled by :math:\n1/(1-p)\n to keep the expected sum of the input unchanged.\n\n\nDuring testing, this operator does not change the input if mode is 'training'. If mode is 'always', the same computaion as during training will be applied.\n\n\n\n\nExample::\n\n\nrandom.seed(998)   input_array = array([[3., 0.5,  -0.5,  2., 7.],                       [2., -0.4,   7.,  3., 0.2]])   a = symbol.Variable('a')   dropout = symbol.Dropout(a, p = 0.2)   executor = dropout.simple_bind(a = input_array.shape)\n\n\nIf training\n\n\nexecutor.forward(is_train = True, a = input_array)   executor.outputs   [[ 3.75   0.625 -0.     2.5    8.75 ]    [ 2.5   -0.5    8.75   3.75   0.   ]]\n\n\nIf testing\n\n\nexecutor.forward(is_train = False, a = input_array)   executor.outputs   [[ 3.     0.5   -0.5    2.     7.   ]    [ 2.    -0.4    7.     3.     0.2  ]]\n\n\nDefined in src/operator/nn/dropout.cc:L78\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to which dropout will be applied.\n\n\np::float, optional, default=0.5\n: Fraction of the input that gets dropped out during training time.\n\n\nmode::{'always', 'training'},optional, default='training'\n: Whether to only turn on dropout during training or to also turn on for inference.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ElementWiseSum\n \n \nMethod\n.\n\n\nElementWiseSum(args)\n\n\n\n\nElementWiseSum is an alias of add_n.\n\n\nNote\n: ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Embedding\n \n \nMethod\n.\n\n\nEmbedding(data, weight, input_dim, output_dim, dtype)\n\n\n\n\nMaps integer indices to vector representations (embeddings).\n\n\nThis operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.\n\n\nFor an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).\n\n\nIf the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).\n\n\nBy default, if any index mentioned is too large, it is replaced by the index that addresses the last vector in an embedding matrix.\n\n\nExamples::\n\n\ninput_dim = 4   output_dim = 5\n\n\n// Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]\n\n\n// Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]\n\n\n// Mapped input x to its vector representation y.   Embedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                             [ 15.,  16.,  17.,  18.,  19.]],\n\n\n                       [[  0.,   1.,   2.,   3.,   4.],\n                        [ 10.,  11.,  12.,  13.,  14.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L185\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array to the embedding operator.\n\n\nweight::NDArray-or-SymbolicNode\n: The embedding weight matrix.\n\n\ninput_dim::int, required\n: Vocabulary size of the input indices.\n\n\noutput_dim::int, required\n: Dimension of the embedding vectors.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Data type of weight.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FullyConnected\n \n \nMethod\n.\n\n\nFullyConnected(data, weight, bias, num_hidden, no_bias, flatten)\n\n\n\n\nApplies a linear transformation: :math:\nY = XW^T + b\n.\n\n\nIf $flatten$ is set to be true, then the shapes are:\n\n\n\n\ndata\n: \n(batch_size, x1, x2, ..., xn)\n\n\nweight\n: \n(num_hidden, x1 * x2 * ... * xn)\n\n\nbias\n: \n(num_hidden,)\n\n\nout\n: \n(batch_size, num_hidden)\n\n\n\n\nIf $flatten$ is set to be false, then the shapes are:\n\n\n\n\ndata\n: \n(x1, x2, ..., xn, input_dim)\n\n\nweight\n: \n(num_hidden, input_dim)\n\n\nbias\n: \n(num_hidden,)\n\n\nout\n: \n(x1, x2, ..., xn, num_hidden)\n\n\n\n\nThe learnable parameters include both $weight$ and $bias$.\n\n\nIf $no_bias$ is set to be true, then the $bias$ term is ignored.\n\n\nDefined in src/operator/nn/fully_connected.cc:L98\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nnum_hidden::int, required\n: Number of hidden nodes of the output.\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\nflatten::boolean, optional, default=1\n: Whether to collapse all but the first axis of the input data tensor.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.GridGenerator\n \n \nMethod\n.\n\n\nGridGenerator(data, transform_type, target_shape)\n\n\n\n\nGenerates 2D sampling grid for bilinear sampling.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\ntransform_type::{'affine', 'warp'}, required\n: The type of transformation. For \naffine\n, input data should be an affine matrix of size (batch, 6). For \nwarp\n, input data should be an optical flow of size (batch, 2, h, w).\n\n\ntarget_shape::Shape(tuple), optional, default=[0,0]\n: Specifies the output shape (H, W). This is required if transformation type is \naffine\n. If transformation type is \nwarp\n, this parameter is ignored.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\nIdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)\n\n\n\n\nApply a sparse regularization to the output a sigmoid activation function.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nsparseness_target::float, optional, default=0.1\n: The sparseness target\n\n\npenalty::float, optional, default=0.001\n: The tradeoff parameter for the sparseness penalty\n\n\nmomentum::float, optional, default=0.9\n: The momentum for running average\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.InstanceNorm\n \n \nMethod\n.\n\n\nInstanceNorm(data, gamma, beta, eps)\n\n\n\n\nApplies instance normalization to the n-dimensional input array.\n\n\nThis operator takes an n-dimensional input array where (n\n2) and normalizes the input using the following formula:\n\n\n.. math::\n\n\nout = \\frac{x - mean[data]}{ \\sqrt{Var[data]} + \\epsilon} * gamma + beta\n\n\nThis layer is similar to batch normalization layer (\nBatchNorm\n) with two differences: first, the normalization is carried out per example (instance), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as \ncontrast normalization\n.\n\n\nIf the input data is of shape [batch, channel, spacial_dim1, spacial_dim2, ...], \ngamma\n and \nbeta\n parameters must be vectors of shape [channel].\n\n\nThis implementation is based on paper:\n\n\n.. [1] Instance Normalization: The Missing Ingredient for Fast Stylization,    D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2).\n\n\nExamples::\n\n\n// Input of shape (2,1,2)   x = [[[ 1.1,  2.2]],        [[ 3.3,  4.4]]]\n\n\n// gamma parameter of length 1   gamma = [1.5]\n\n\n// beta parameter of length 1   beta = [0.5]\n\n\n// Instance normalization is calculated with the above formula   InstanceNorm(x,gamma,beta) = [[[-0.997527  ,  1.99752665]],                                 [[-0.99752653,  1.99752724]]]\n\n\nDefined in src/operator/instance_norm.cc:L95\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: An n-dimensional input array (n \n 2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].\n\n\ngamma::NDArray-or-SymbolicNode\n: A vector of length 'channel', which multiplies the normalized input.\n\n\nbeta::NDArray-or-SymbolicNode\n: A vector of length 'channel', which is added to the product of the normalized input and the weight.\n\n\neps::float, optional, default=0.001\n: An \nepsilon\n parameter to prevent division by 0.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.L2Normalization\n \n \nMethod\n.\n\n\nL2Normalization(data, eps, mode)\n\n\n\n\nNormalize the input array using the L2 norm.\n\n\nFor 1-D NDArray, it computes::\n\n\nout = data / sqrt(sum(data ** 2) + eps)\n\n\nFor N-D NDArray, if the input array has shape (N, N, ..., N),\n\n\nwith $mode$ = $instance$, it normalizes each instance in the multidimensional array by its L2 norm.::\n\n\nfor i in 0...N     out[i,:,:,...,:] = data[i,:,:,...,:] / sqrt(sum(data[i,:,:,...,:] ** 2) + eps)\n\n\nwith $mode$ = $channel$, it normalizes each channel in the array by its L2 norm.::\n\n\nfor i in 0...N     out[:,i,:,...,:] = data[:,i,:,...,:] / sqrt(sum(data[:,i,:,...,:] ** 2) + eps)\n\n\nwith $mode$ = $spatial$, it normalizes the cross channel norm for each position in the array by its L2 norm.::\n\n\nfor dim in 2...N     for i in 0...N       out[.....,i,...] = take(out, indices=i, axis=dim) / sqrt(sum(take(out, indices=i, axis=dim) ** 2) + eps)           -dim-\n\n\nExample::\n\n\nx = [[[1,2],         [3,4]],        [[2,2],         [5,6]]]\n\n\nL2Normalization(x, mode='instance')   =[[[ 0.18257418  0.36514837]      [ 0.54772252  0.73029673]]     [[ 0.24077171  0.24077171]      [ 0.60192931  0.72231513]]]\n\n\nL2Normalization(x, mode='channel')   =[[[ 0.31622776  0.44721359]      [ 0.94868326  0.89442718]]     [[ 0.37139067  0.31622776]      [ 0.92847669  0.94868326]]]\n\n\nL2Normalization(x, mode='spatial')   =[[[ 0.44721359  0.89442718]      [ 0.60000002  0.80000001]]     [[ 0.70710677  0.70710677]      [ 0.6401844   0.76822126]]]\n\n\nDefined in src/operator/l2_normalization.cc:L93\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to normalize.\n\n\neps::float, optional, default=1e-10\n: A small constant for numerical stability.\n\n\nmode::{'channel', 'instance', 'spatial'},optional, default='instance'\n: Specify the dimension along which to compute L2 norm.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LRN\n \n \nMethod\n.\n\n\nLRN(data, alpha, beta, knorm, nsize)\n\n\n\n\nApplies local response normalization to the input.\n\n\nThe local response normalization layer performs \"lateral inhibition\" by normalizing over local input regions.\n\n\nIf :math:\na_{x,y}^{i}\n is the activity of a neuron computed by applying kernel :math:\ni\n at position :math:\n(x, y)\n and then applying the ReLU nonlinearity, the response-normalized activity :math:\nb_{x,y}^{i}\n is given by the expression:\n\n\n.. math::    b_{x,y}^{i} = \\frac{a_{x,y}^{i}}{\\Bigg({k + \\alpha \\sum_{j=max(0, i-\\frac{n}{2})}^{min(N-1, i+\\frac{n}{2})} (a_{x,y}^{j})^{2}}\\Bigg)^{\\beta}}\n\n\nwhere the sum runs over :math:\nn\n \"adjacent\" kernel maps at the same spatial position, and :math:\nN\n is the total number of kernels in the layer.\n\n\nDefined in src/operator/lrn.cc:L73\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nalpha::float, optional, default=0.0001\n: The variance scaling parameter :math:\n\u0007lpha\n in the LRN expression.\n\n\nbeta::float, optional, default=0.75\n: The power parameter :math:\n\beta\n in the LRN expression.\n\n\nknorm::float, optional, default=2\n: The parameter :math:\nk\n in the LRN expression.\n\n\nnsize::int (non-negative), required\n: normalization window width in elements.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LeakyReLU\n \n \nMethod\n.\n\n\nLeakyReLU(data, act_type, slope, lower_bound, upper_bound)\n\n\n\n\nApplies Leaky rectified linear unit activation element-wise to the input.\n\n\nLeaky ReLUs attempt to fix the \"dying ReLU\" problem by allowing a small \nslope\n when the input is negative and has a slope of one when input is positive.\n\n\nThe following modified ReLU Activation functions are supported:\n\n\n\n\nelu\n: Exponential Linear Unit. \ny = x \n 0 ? x : slope * (exp(x)-1)\n\n\nleaky\n: Leaky ReLU. \ny = x \n 0 ? x : slope * x\n\n\nprelu\n: Parametric ReLU. This is same as \nleaky\n except that \nslope\n is learnt during training.\n\n\nrrelu\n: Randomized ReLU. same as \nleaky\n but the \nslope\n is uniformly and randomly chosen from \n[lower_bound, upper_bound)\n for training, while fixed to be \n(lower_bound+upper_bound)/2\n for inference.\n\n\n\n\nDefined in src/operator/leaky_relu.cc:L58\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'\n: Activation function to be applied.\n\n\nslope::float, optional, default=0.25\n: Init slope for the activation. (For leaky and elu only)\n\n\nlower_bound::float, optional, default=0.125\n: Lower bound of random slope. (For rrelu only)\n\n\nupper_bound::float, optional, default=0.334\n: Upper bound of random slope. (For rrelu only)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LinearRegressionOutput\n \n \nMethod\n.\n\n\nLinearRegressionOutput(data, label, grad_scale)\n\n\n\n\nComputes and optimizes for squared loss during backward propagation. Just outputs $data$ during forward propagation.\n\n\nIf :math:\n\\hat{y}_i\n is the predicted value of the i-th sample, and :math:\ny_i\n is the corresponding target value, then the squared loss estimated over :math:\nn\n samples is defined as\n\n\n:math:\n\\text{SquaredLoss}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left( y_i - \\hat{y}_i \\right)^2\n\n\n.. note::    Use the LinearRegressionOutput as the final output layer of a net.\n\n\nBy default, gradients of this loss function are scaled by factor \n1/n\n, where n is the number of training examples. The parameter \ngrad_scale\n can be used to change this scale to \ngrad_scale/n\n.\n\n\nDefined in src/operator/regression_output.cc:L70\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label to the function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LogisticRegressionOutput\n \n \nMethod\n.\n\n\nLogisticRegressionOutput(data, label, grad_scale)\n\n\n\n\nApplies a logistic function to the input.\n\n\nThe logistic function, also known as the sigmoid function, is computed as :math:\n\\frac{1}{1+exp(-x)}\n.\n\n\nCommonly, the sigmoid is used to squash the real-valued output of a linear model :math:wTx+b into the [0,1] range so that it can be interpreted as a probability. It is suitable for binary classification or probability prediction tasks.\n\n\n.. note::    Use the LogisticRegressionOutput as the final output layer of a net.\n\n\nBy default, gradients of this loss function are scaled by factor \n1/n\n, where n is the number of training examples. The parameter \ngrad_scale\n can be used to change this scale to \ngrad_scale/n\n.\n\n\nDefined in src/operator/regression_output.cc:L112\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label to the function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MAERegressionOutput\n \n \nMethod\n.\n\n\nMAERegressionOutput(data, label, grad_scale)\n\n\n\n\nComputes mean absolute error of the input.\n\n\nMAE is a risk metric corresponding to the expected value of the absolute error.\n\n\nIf :math:\n\\hat{y}_i\n is the predicted value of the i-th sample, and :math:\ny_i\n is the corresponding target value, then the mean absolute error (MAE) estimated over :math:\nn\n samples is defined as\n\n\n:math:\n\\text{MAE}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left| y_i - \\hat{y}_i \\right|\n\n\n.. note::    Use the MAERegressionOutput as the final output layer of a net.\n\n\nBy default, gradients of this loss function are scaled by factor \n1/n\n, where n is the number of training examples. The parameter \ngrad_scale\n can be used to change this scale to \ngrad_scale/n\n.\n\n\nDefined in src/operator/regression_output.cc:L91\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label to the function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MakeLoss\n \n \nMethod\n.\n\n\nMakeLoss(data, grad_scale, valid_thresh, normalization)\n\n\n\n\nMake your own loss function in network construction.\n\n\nThis operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.\n\n\nFor example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::\n\n\ncross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = MakeLoss(cross_entropy)\n\n\nWe will need to use $MakeLoss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.\n\n\nIn addition, we can give a scale to the loss by setting $grad_scale$, so that the gradient of the loss will be rescaled in the backpropagation.\n\n\n.. note:: This operator should be used as a Symbol instead of NDArray.\n\n\nDefined in src/operator/make_loss.cc:L71\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\ngrad_scale::float, optional, default=1\n: Gradient scale as a supplement to unary and binary operators\n\n\nvalid_thresh::float, optional, default=0\n: clip each element in the array to 0 when it is less than $valid_thresh$. This is used when $normalization$ is set to $'valid'$.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If this is set to null, the output gradient will not be normalized. If this is set to batch, the output gradient will be divided by the batch size. If this is set to valid, the output gradient will be divided by the number of valid input elements.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pad\n \n \nMethod\n.\n\n\nPad(data, mode, pad_width, constant_value)\n\n\n\n\nPads an input array with a constant or edge values of the array.\n\n\n.. note:: \nPad\n is deprecated. Use \npad\n instead.\n\n\n.. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in \npad_width\n to be zero.\n\n\nThis operation pads an input array with either a \nconstant_value\n or edge values along each axis of the input array. The amount of padding is specified by \npad_width\n.\n\n\npad_width\n is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The \npad_width\n should be of length $2*N$ where $N$ is the number of dimensions of the array.\n\n\nFor dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.\n\n\nExample::\n\n\nx = [[[[  1.   2.   3.]           [  4.   5.   6.]]\n\n\n     [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]\n\n\n\n\npad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]\n\n\n\n\npad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]\n\n\n\n\nDefined in src/operator/pad.cc:L766\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: An n-dimensional input array.\n\n\nmode::{'constant', 'edge', 'reflect'}, required\n: Padding type to use. \"constant\" pads with \nconstant_value\n \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.\n\n\npad_width::Shape(tuple), required\n: Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.\n\n\nconstant_value::double, optional, default=0\n: The value used for padding when \nmode\n is \"constant\".\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling\n \n \nMethod\n.\n\n\nPooling(data, global_pool, cudnn_off, kernel, pool_type, pooling_convention, stride, pad)\n\n\n\n\nPerforms pooling on the input.\n\n\nThe shapes for 1-D pooling are\n\n\n\n\ndata\n: \n(batch_size, channel, width)\n,\n\n\nout\n: \n(batch_size, num_filter, out_width)\n.\n\n\n\n\nThe shapes for 2-D pooling are\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n, with::\n\n\nout_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])\n\n\n\n\n\n\nThe definition of \nf\n depends on $pooling_convention$, which has two options:\n\n\n\n\n\n\nvalid\n (default)::\n\n\nf(x, k, p, s) = floor((x+2*p-k)/s)+1\n  * \nfull\n, which is compatible with Caffe::\n\n\nf(x, k, p, s) = ceil((x+2*p-k)/s)+1\n\n\n\n\n\n\nBut $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.\n\n\nThree pooling options are supported by $pool_type$:\n\n\n\n\navg\n: average pooling\n\n\nmax\n: max pooling\n\n\nsum\n: sum pooling\n\n\n\n\nFor 3-D pooling, an additional \ndepth\n dimension is added before \nheight\n. Namely the input data will have shape \n(batch_size, channel, depth, height, width)\n.\n\n\nDefined in src/operator/nn/pooling.cc:L133\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=0\n: Ignore kernel size, do global pooling based on current input feature map.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn pooling and use MXNet pooling operator.\n\n\nkernel::Shape(tuple), required\n: Pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\npooling_convention::{'full', 'valid'},optional, default='valid'\n: Pooling convention to be applied.\n\n\nstride::Shape(tuple), optional, default=[]\n: Stride: for pooling (y, x) or (d, y, x). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: Pad for pooling: (y, x) or (d, y, x). Defaults to no padding.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling_v1\n \n \nMethod\n.\n\n\nPooling_v1(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)\n\n\n\n\nThis operator is DEPRECATED. Perform pooling on the input.\n\n\nThe shapes for 2-D pooling is\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n, with::\n\n\nout_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])\n\n\n\n\n\n\nThe definition of \nf\n depends on $pooling_convention$, which has two options:\n\n\n\n\n\n\nvalid\n (default)::\n\n\nf(x, k, p, s) = floor((x+2*p-k)/s)+1\n  * \nfull\n, which is compatible with Caffe::\n\n\nf(x, k, p, s) = ceil((x+2*p-k)/s)+1\n\n\n\n\n\n\nBut $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.\n\n\nThree pooling options are supported by $pool_type$:\n\n\n\n\navg\n: average pooling\n\n\nmax\n: max pooling\n\n\nsum\n: sum pooling\n\n\n\n\n1-D pooling is special case of 2-D pooling with \nweight=1\n and \nkernel[1]=1\n.\n\n\nFor 3-D pooling, an additional \ndepth\n dimension is added before \nheight\n. Namely the input data will have shape \n(batch_size, channel, depth, height, width)\n.\n\n\nDefined in src/operator/pooling_v1.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=0\n: Ignore kernel size, do global pooling based on current input feature map.\n\n\nkernel::Shape(tuple), required\n: pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\npooling_convention::{'full', 'valid'},optional, default='valid'\n: Pooling convention to be applied.\n\n\nstride::Shape(tuple), optional, default=[]\n: stride: for pooling (y, x) or (d, y, x)\n\n\npad::Shape(tuple), optional, default=[]\n: pad for pooling: (y, x) or (d, y, x)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.RNN\n \n \nMethod\n.\n\n\nRNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)\n\n\n\n\nApplies a recurrent layer to input.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to RNN\n\n\nparameters::NDArray-or-SymbolicNode\n: Vector of all RNN trainable parameters concatenated\n\n\nstate::NDArray-or-SymbolicNode\n: initial hidden state of the RNN\n\n\nstate_cell::NDArray-or-SymbolicNode\n: initial cell state for LSTM networks (only for LSTM)\n\n\nstate_size::int (non-negative), required\n: size of the state for each layer\n\n\nnum_layers::int (non-negative), required\n: number of stacked layers\n\n\nbidirectional::boolean, optional, default=0\n: whether to use bidirectional recurrent layers\n\n\nmode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required\n: the type of RNN to compute\n\n\np::float, optional, default=0\n: Dropout probability, fraction of the input that gets dropped out at training time\n\n\nstate_outputs::boolean, optional, default=0\n: Whether to have the states as symbol outputs.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ROIPooling\n \n \nMethod\n.\n\n\nROIPooling(data, rois, pooled_size, spatial_scale)\n\n\n\n\nPerforms region of interest(ROI) pooling on the input array.\n\n\nROI pooling is a variant of a max pooling layer, in which the output size is fixed and region of interest is a parameter. Its purpose is to perform max pooling on the inputs of non-uniform sizes to obtain fixed-size feature maps. ROI pooling is a neural-net layer mostly used in training a \nFast R-CNN\n network for object detection.\n\n\nThis operator takes a 4D feature map as an input array and region proposals as \nrois\n, then it pools over sub-regions of input and produces a fixed-sized output array regardless of the ROI size.\n\n\nTo crop the feature map accordingly, you can resize the bounding box coordinates by changing the parameters \nrois\n and \nspatial_scale\n.\n\n\nThe cropped feature maps are pooled by standard max pooling operation to a fixed size output indicated by a \npooled_size\n parameter. batch_size will change to the number of region bounding boxes after \nROIPooling\n.\n\n\nThe size of each region of interest doesn't have to be perfectly divisible by the number of pooling sections(\npooled_size\n).\n\n\nExample::\n\n\nx = [[[[  0.,   1.,   2.,   3.,   4.,   5.],          [  6.,   7.,   8.,   9.,  10.,  11.],          [ 12.,  13.,  14.,  15.,  16.,  17.],          [ 18.,  19.,  20.,  21.,  22.,  23.],          [ 24.,  25.,  26.,  27.,  28.,  29.],          [ 30.,  31.,  32.,  33.,  34.,  35.],          [ 36.,  37.,  38.,  39.,  40.,  41.],          [ 42.,  43.,  44.,  45.,  46.,  47.]]]]\n\n\n// region of interest i.e. bounding box coordinates.   y = [[0,0,0,4,4]]\n\n\n// returns array of shape (2,2) according to the given roi with max pooling.   ROIPooling(x, y, (2,2), 1.0) = [[[[ 14.,  16.],                                     [ 26.,  28.]]]]\n\n\n// region of interest is changed due to the change in \nspacial_scale\n parameter.   ROIPooling(x, y, (2,2), 0.7) = [[[[  7.,   9.],                                     [ 19.,  21.]]]]\n\n\nDefined in src/operator/roi_pooling.cc:L287\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array to the pooling operator,  a 4D Feature maps\n\n\nrois::NDArray-or-SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]], where (x1, y1) and (x2, y2) are top left and bottom right corners of designated region of interest. \nbatch_index\n indicates the index of corresponding image in the input array\n\n\npooled_size::Shape(tuple), required\n: ROI pooling output shape (h,w)\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SVMOutput\n \n \nMethod\n.\n\n\nSVMOutput(data, label, margin, regularization_coefficient, use_linear)\n\n\n\n\nComputes support vector machine based transformation of the input.\n\n\nThis tutorial demonstrates using SVM as output layer for classification instead of softmax: https://github.com/dmlc/mxnet/tree/master/example/svm_mnist.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data for SVM transformation.\n\n\nlabel::NDArray-or-SymbolicNode\n: Class label for the input data.\n\n\nmargin::float, optional, default=1\n: The loss function penalizes outputs that lie outside this margin. Default margin is 1.\n\n\nregularization_coefficient::float, optional, default=1\n: Regularization parameter for the SVM. This balances the tradeoff between coefficient size and error.\n\n\nuse_linear::boolean, optional, default=0\n: Whether to use L1-SVM objective. L2-SVM objective is used by default.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceLast\n \n \nMethod\n.\n\n\nSequenceLast(data, sequence_length, use_sequence_length)\n\n\n\n\nTakes the last element of a sequence.\n\n\nThis function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns a (n-1)-dimensional array of the form [batch_size, other_feature_dims].\n\n\nParameter \nsequence_length\n is used to handle variable-length sequences. \nsequence_length\n should be an input array of positive ints of dimension [batch_size]. To use this parameter, set \nuse_sequence_length\n to \nTrue\n, otherwise each example in the batch is assumed to have the max sequence length.\n\n\n.. note:: Alternatively, you can also use \ntake\n operator.\n\n\nExample::\n\n\nx = [[[  1.,   2.,   3.],          [  4.,   5.,   6.],          [  7.,   8.,   9.]],\n\n\n    [[ 10.,   11.,   12.],\n     [ 13.,   14.,   15.],\n     [ 16.,   17.,   18.]],\n\n    [[  19.,   20.,   21.],\n     [  22.,   23.,   24.],\n     [  25.,   26.,   27.]]]\n\n\n\n\n// returns last sequence when sequence_length parameter is not used    SequenceLast(x) = [[  19.,   20.,   21.],                       [  22.,   23.,   24.],                       [  25.,   26.,   27.]]\n\n\n// sequence_length y is used    SequenceLast(x, y=[1,1,1], use_sequence_length=True) =             [[  1.,   2.,   3.],              [  4.,   5.,   6.],              [  7.,   8.,   9.]]\n\n\n// sequence_length y is used    SequenceLast(x, y=[1,2,3], use_sequence_length=True) =             [[  1.,    2.,   3.],              [  13.,  14.,  15.],              [  25.,  26.,  27.]]\n\n\nDefined in src/operator/sequence_last.cc:L92\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n\n2\n\n\nsequence_length::NDArray-or-SymbolicNode\n: vector of sequence lengths of the form [batch_size]\n\n\nuse_sequence_length::boolean, optional, default=0\n: If set to true, this layer takes in an extra input parameter \nsequence_length\n to specify variable length sequence\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceMask\n \n \nMethod\n.\n\n\nSequenceMask(data, sequence_length, use_sequence_length, value)\n\n\n\n\nSets all elements outside the sequence to a constant value.\n\n\nThis function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.\n\n\nParameter \nsequence_length\n is used to handle variable-length sequences. \nsequence_length\n should be an input array of positive ints of dimension [batch_size]. To use this parameter, set \nuse_sequence_length\n to \nTrue\n, otherwise each example in the batch is assumed to have the max sequence length and this operator works as the \nidentity\n operator.\n\n\nExample::\n\n\nx = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],\n\n\n    [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]\n\n\n\n\n// Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]\n\n\n// Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]\n\n\n// works as identity operator when sequence_length parameter is not used    SequenceMask(x) = [[[  1.,   2.,   3.],                        [  4.,   5.,   6.]],\n\n\n                  [[  7.,   8.,   9.],\n                   [ 10.,  11.,  12.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]\n\n\n\n\n// sequence_length [1,1] means 1 of each batch will be kept    // and other rows are masked with default mask value = 0    SequenceMask(x, y=[1,1], use_sequence_length=True) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],\n\n\n             [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]],\n\n             [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]]]\n\n\n\n\n// sequence_length [2,3] means 2 of batch B1 and 3 of batch B2 will be kept    // and other rows are masked with value = 1    SequenceMask(x, y=[2,3], use_sequence_length=True, value=1) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],\n\n\n             [[  7.,   8.,   9.],\n              [  10.,  11.,  12.]],\n\n             [[   1.,   1.,   1.],\n              [  16.,  17.,  18.]]]\n\n\n\n\nDefined in src/operator/sequence_mask.cc:L114\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n\n2\n\n\nsequence_length::NDArray-or-SymbolicNode\n: vector of sequence lengths of the form [batch_size]\n\n\nuse_sequence_length::boolean, optional, default=0\n: If set to true, this layer takes in an extra input parameter \nsequence_length\n to specify variable length sequence\n\n\nvalue::float, optional, default=0\n: The value to be used as a mask.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceReverse\n \n \nMethod\n.\n\n\nSequenceReverse(data, sequence_length, use_sequence_length)\n\n\n\n\nReverses the elements of each sequence.\n\n\nThis function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.\n\n\nParameter \nsequence_length\n is used to handle variable-length sequences. \nsequence_length\n should be an input array of positive ints of dimension [batch_size]. To use this parameter, set \nuse_sequence_length\n to \nTrue\n, otherwise each example in the batch is assumed to have the max sequence length.\n\n\nExample::\n\n\nx = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],\n\n\n    [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]\n\n\n\n\n// Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]\n\n\n// Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]\n\n\n// returns reverse sequence when sequence_length parameter is not used    SequenceReverse(x) = [[[ 13.,  14.,   15.],                           [ 16.,  17.,   18.]],\n\n\n                     [[  7.,   8.,   9.],\n                      [ 10.,  11.,  12.]],\n\n                     [[  1.,   2.,   3.],\n                      [  4.,   5.,   6.]]]\n\n\n\n\n// sequence_length [2,2] means 2 rows of    // both batch B1 and B2 will be reversed.    SequenceReverse(x, y=[2,2], use_sequence_length=True) =                      [[[  7.,   8.,   9.],                        [ 10.,  11.,  12.]],\n\n\n                  [[  1.,   2.,   3.],\n                   [  4.,   5.,   6.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]\n\n\n\n\n// sequence_length [2,3] means 2 of batch B2 and 3 of batch B3    // will be reversed.    SequenceReverse(x, y=[2,3], use_sequence_length=True) =                     [[[  7.,   8.,   9.],                       [ 16.,  17.,  18.]],\n\n\n                 [[  1.,   2.,   3.],\n                  [ 10.,  11.,  12.]],\n\n                 [[ 13.,  14,   15.],\n                  [  4.,   5.,   6.]]]\n\n\n\n\nDefined in src/operator/sequence_reverse.cc:L113\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: n-dimensional input array of the form [max_sequence_length, batch_size, other dims] where n\n2\n\n\nsequence_length::NDArray-or-SymbolicNode\n: vector of sequence lengths of the form [batch_size]\n\n\nuse_sequence_length::boolean, optional, default=0\n: If set to true, this layer takes in an extra input parameter \nsequence_length\n to specify variable length sequence\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SliceChannel\n \n \nMethod\n.\n\n\nSliceChannel(data, num_outputs, axis, squeeze_axis)\n\n\n\n\nSplits an array along a particular axis into multiple sub-arrays.\n\n\n.. note:: $SliceChannel$ is deprecated. Use $split$ instead.\n\n\nNote\n that \nnum_outputs\n should evenly divide the length of the axis along which to split the array.\n\n\nExample::\n\n\nx  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)\n\n\ny = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]\n\n\n   [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]\n\n\n\n\ny[0].shape = (3, 1, 1)\n\n\nz = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]\n\n\n   [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]\n\n\n\n\nz[0].shape = (1, 2, 1)\n\n\nsqueeze_axis=1\n removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $1$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to true only if $input.shape[axis] == num_outputs$.\n\n\nExample::\n\n\nz = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]\n\n\n   [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]\n\n\n\n\nz[0].shape = (2 ,1 )\n\n\nDefined in src/operator/slice_channel.cc:L107\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nnum_outputs::int, required\n: Number of splits. Note that this should evenly divide the length of the \naxis\n.\n\n\naxis::int, optional, default='1'\n: Axis along which to split.\n\n\nsqueeze_axis::boolean, optional, default=0\n: If true, Removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $true$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to $true$ only if $input.shape[axis] == num_outputs$.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxActivation\n \n \nMethod\n.\n\n\nSoftmaxActivation(data, mode)\n\n\n\n\nApplies softmax activation to input. This is intended for internal layers.\n\n\n.. note::\n\n\nThis operator has been deprecated, please use \nsoftmax\n.\n\n\nIf \nmode\n = $instance$, this operator will compute a softmax for each instance in the batch. This is the default mode.\n\n\nIf \nmode\n = $channel$, this operator will compute a k-class softmax at each position of each instance, where \nk\n = $num_channel$. This mode can only be used when the input array has at least 3 dimensions. This can be used for \nfully convolutional network\n, \nimage segmentation\n, etc.\n\n\nExample::\n\n\n\n\n\n\n\n\ninput_array = mx.nd.array([[3., 0.5, -0.5, 2., 7.],                            [2., -.4, 7.,   3., 0.2]]) softmax_act = mx.nd.SoftmaxActivation(input_array) print softmax_act.asnumpy()\n\n\n\n\n\n\n\n\n[[  1.78322066e-02   1.46375655e-03   5.38485940e-04   6.56010211e-03   9.73605454e-01]    [  6.56221947e-03   5.95310994e-04   9.73919690e-01   1.78379621e-02   1.08472735e-03]]\n\n\nDefined in src/operator/nn/softmax_activation.cc:L67\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to activation function.\n\n\nmode::{'channel', 'instance'},optional, default='instance'\n: Specifies how to compute the softmax. If set to $instance$, it computes softmax for each instance. If set to $channel$, It computes cross channel softmax for each position of each instance.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxOutput\n \n \nMethod\n.\n\n\nSoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad, smooth_alpha)\n\n\n\n\nComputes the gradient of cross entropy loss with respect to softmax output.\n\n\n\n\n\n\nThis operator computes the gradient in two steps. The cross entropy loss does not actually need to be computed.\n\n\n\n\nApplies softmax function on the input array.\n\n\nComputes and returns the gradient of cross entropy loss w.r.t. the softmax output.\n\n\n\n\nThe softmax function, cross entropy loss and gradient is given by:\n\n\n\n\n\n\nSoftmax Function:\n\n\n.. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n      * Cross Entropy Function:\n\n\n.. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)\n      * The gradient of cross entropy loss w.r.t softmax output:\n\n\n.. math:: \\text{gradient} = \\text{output} - \\text{label}\n  * During forward propagation, the softmax function is computed for each instance in the input array.\n\n\n\n\n\n\nFor general \nN\n-D input arrays with shape :math:\n(d_1, d_2, ..., d_n)\n. The size is :math:\ns=d_1 \\cdot d_2 \\cdot \\cdot \\cdot d_n\n. We can use the parameters \npreserve_shape\n and \nmulti_output\n to specify the way to compute softmax:\n\n\n\n\nBy default, \npreserve_shape\n is $false$. This operator will reshape the input array into a 2-D array with shape :math:\n(d_1, \\frac{s}{d_1})\n and then compute the softmax function for each row in the reshaped array, and afterwards reshape it back to the original shape :math:\n(d_1, d_2, ..., d_n)\n.\n\n\nIf \npreserve_shape\n is $true$, the softmax function will be computed along the last axis (\naxis\n = $-1$).\n\n\nIf \nmulti_output\n is $true$, the softmax function will be computed along the second axis (\naxis\n = $1$).\n\n\n\n\nDuring backward propagation, the gradient of cross-entropy loss w.r.t softmax output array is computed. The provided label can be a one-hot label array or a probability label array.\n\n\n\n\n\n\nIf the parameter \nuse_ignore\n is $true$, \nignore_label\n can specify input instances with a particular label to be ignored during backward propagation. \nThis has no effect when softmax \noutput\n has same shape as \nlabel\n.\n\n\nExample::\n\n\ndata = [[1,2,3,4],[2,2,2,2],[3,3,3,3],[4,4,4,4]]   label = [1,0,2,3]   ignore_label = 1   SoftmaxOutput(data=data, label = label,\n                multi_output=true, use_ignore=true,\n                ignore_label=ignore_label)\n\n\nforward softmax output\n\n\n[[ 0.0320586   0.08714432  0.23688284  0.64391428]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]]\n\n\nbackward gradient output\n\n\n[[ 0.    0.    0.    0.  ]    [-0.75  0.25  0.25  0.25]    [ 0.25  0.25 -0.75  0.25]    [ 0.25  0.25  0.25 -0.75]]\n\n\nnotice that the first row is all 0 because label[0] is 1, which is equal to ignore_label.\n\n\n  * The parameter `grad_scale` can be used to rescale the gradient, which is often used to give each loss function different weights.\n  * This operator also supports various ways to normalize the gradient by `normalization`, The `normalization` is applied if softmax output has different shape than the labels. The `normalization` mode can be set to the followings:\n\n\n\n\n\n$'null'$: do nothing.\n\n\n$'batch'$: divide the gradient by the batch size.\n\n\n$'valid'$: divide the gradient by the number of instances which are not ignored.\n\n\n\n\n\n\n\n\n\n\n\n\nDefined in src/operator/softmax_output.cc:L123\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\nlabel::NDArray-or-SymbolicNode\n: Ground truth label.\n\n\ngrad_scale::float, optional, default=1\n: Scales the gradient by a float factor.\n\n\nignore_label::float, optional, default=-1\n: The instances whose \nlabels\n == \nignore_label\n will be ignored during backward, if \nuse_ignore\n is set to $true$).\n\n\nmulti_output::boolean, optional, default=0\n: If set to $true$, the softmax function will be computed along axis $1$. This is applied when the shape of input array differs from the shape of label array.\n\n\nuse_ignore::boolean, optional, default=0\n: If set to $true$, the \nignore_label\n value will not contribute to the backward gradient.\n\n\npreserve_shape::boolean, optional, default=0\n: If set to $true$, the softmax function will be computed along the last axis ($-1$).\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: Normalizes the gradient.\n\n\nout_grad::boolean, optional, default=0\n: Multiplies gradient with output gradient element-wise.\n\n\nsmooth_alpha::float, optional, default=0\n: Constant for computing a label smoothed version of cross-entropyfor the backwards pass.  This constant gets subtracted from theone-hot encoding of the gold label and distributed uniformly toall other labels.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SpatialTransformer\n \n \nMethod\n.\n\n\nSpatialTransformer(data, loc, target_shape, transform_type, sampler_type)\n\n\n\n\nApplies a spatial transformer to input feature map.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the SpatialTransformerOp.\n\n\nloc::NDArray-or-SymbolicNode\n: localisation net, the output dim should be 6 when transform_type is affine. You shold initialize the weight and bias with identity tranform.\n\n\ntarget_shape::Shape(tuple), optional, default=[0,0]\n: output shape(h, w) of spatial transformer: (y, x)\n\n\ntransform_type::{'affine'}, required\n: transformation type\n\n\nsampler_type::{'bilinear'}, required\n: sampling type\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SwapAxis\n \n \nMethod\n.\n\n\nSwapAxis(data, dim1, dim2)\n\n\n\n\nInterchanges two axes of an array.\n\n\nExamples::\n\n\nx = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]\n\n\nx = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array\n\n\nswapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]\n\n\nDefined in src/operator/swapaxis.cc:L70\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UpSampling\n \n \nMethod\n.\n\n\nUpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)\n\n\n\n\nNote\n: UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.\n\n\nPerforms nearest neighbor/bilinear up sampling to inputs.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Array of tensors to upsample\n\n\nscale::int (non-negative), required\n: Up sampling scale\n\n\nnum_filter::int (non-negative), optional, default=0\n: Input filter. Only used by bilinear sample_type.\n\n\nsample_type::{'bilinear', 'nearest'}, required\n: upsampling method\n\n\nmulti_input_mode::{'concat', 'sum'},optional, default='concat'\n: How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.\n\n\nnum_args::int, required\n: Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale\nh_0,scale\nw_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CachedOp\n \n \nMethod\n.\n\n\n_CachedOp()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CrossDeviceCopy\n \n \nMethod\n.\n\n\n_CrossDeviceCopy()\n\n\n\n\nSpecial op to copy data cross device\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CustomFunction\n \n \nMethod\n.\n\n\n_CustomFunction()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Div\n \n \nMethod\n.\n\n\n_Div(lhs, rhs)\n\n\n\n\n_Div is an alias of elemwise_div.\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._DivScalar\n \n \nMethod\n.\n\n\n_DivScalar(data, scalar)\n\n\n\n\n_DivScalar is an alias of _div_scalar.\n\n\nDivide an array with a scalar.\n\n\n$_div_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Equal\n \n \nMethod\n.\n\n\n_Equal(lhs, rhs)\n\n\n\n\n_Equal is an alias of _equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._EqualScalar\n \n \nMethod\n.\n\n\n_EqualScalar(data, scalar)\n\n\n\n\n_EqualScalar is an alias of _equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater\n \n \nMethod\n.\n\n\n_Greater(lhs, rhs)\n\n\n\n\n_Greater is an alias of _greater.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterEqualScalar\n \n \nMethod\n.\n\n\n_GreaterEqualScalar(data, scalar)\n\n\n\n\n_GreaterEqualScalar is an alias of _greater_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterScalar\n \n \nMethod\n.\n\n\n_GreaterScalar(data, scalar)\n\n\n\n\n_GreaterScalar is an alias of _greater_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater_Equal\n \n \nMethod\n.\n\n\n_Greater_Equal(lhs, rhs)\n\n\n\n\n_Greater_Equal is an alias of _greater_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Hypot\n \n \nMethod\n.\n\n\n_Hypot(lhs, rhs)\n\n\n\n\n_Hypot is an alias of _hypot.\n\n\nGiven the \"legs\" of a right triangle, return its hypotenuse.\n\n\nDefined in src/operator/tensor/elemwise_binary_op_extended.cc:L79\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._HypotScalar\n \n \nMethod\n.\n\n\n_HypotScalar(data, scalar)\n\n\n\n\n_HypotScalar is an alias of _hypot_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser\n \n \nMethod\n.\n\n\n_Lesser(lhs, rhs)\n\n\n\n\n_Lesser is an alias of _lesser.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserEqualScalar\n \n \nMethod\n.\n\n\n_LesserEqualScalar(data, scalar)\n\n\n\n\n_LesserEqualScalar is an alias of _lesser_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserScalar\n \n \nMethod\n.\n\n\n_LesserScalar(data, scalar)\n\n\n\n\n_LesserScalar is an alias of _lesser_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser_Equal\n \n \nMethod\n.\n\n\n_Lesser_Equal(lhs, rhs)\n\n\n\n\n_Lesser_Equal is an alias of _lesser_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Maximum\n \n \nMethod\n.\n\n\n_Maximum(lhs, rhs)\n\n\n\n\n_Maximum is an alias of _maximum.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MaximumScalar\n \n \nMethod\n.\n\n\n_MaximumScalar(data, scalar)\n\n\n\n\n_MaximumScalar is an alias of _maximum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minimum\n \n \nMethod\n.\n\n\n_Minimum(lhs, rhs)\n\n\n\n\n_Minimum is an alias of _minimum.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinimumScalar\n \n \nMethod\n.\n\n\n_MinimumScalar(data, scalar)\n\n\n\n\n_MinimumScalar is an alias of _minimum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinusScalar\n \n \nMethod\n.\n\n\n_MinusScalar(data, scalar)\n\n\n\n\n_MinusScalar is an alias of _minus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._ModScalar\n \n \nMethod\n.\n\n\n_ModScalar(data, scalar)\n\n\n\n\n_ModScalar is an alias of _mod_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Mul\n \n \nMethod\n.\n\n\n_Mul(lhs, rhs)\n\n\n\n\n_Mul is an alias of elemwise_mul.\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MulScalar\n \n \nMethod\n.\n\n\n_MulScalar(data, scalar)\n\n\n\n\n_MulScalar is an alias of _mul_scalar.\n\n\nMultiply an array with a scalar.\n\n\n$_mul_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NDArray\n \n \nMethod\n.\n\n\n_NDArray(data, info)\n\n\n\n\nStub for implementing an operator implemented in native frontend language with ndarray.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Input data for the custom operator.\n\n\ninfo::ptr, required\n:\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Native\n \n \nMethod\n.\n\n\n_Native(data, info, need_top_grad)\n\n\n\n\nStub for implementing an operator implemented in native frontend language.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Input data for the custom operator.\n\n\ninfo::ptr, required\n:\n\n\nneed_top_grad::boolean, optional, default=1\n: Whether this layer needs out grad for backward. Should be false for loss layers.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NoGradient\n \n \nMethod\n.\n\n\n_NoGradient()\n\n\n\n\nPlace holder for variable who cannot perform gradient\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NotEqualScalar\n \n \nMethod\n.\n\n\n_NotEqualScalar(data, scalar)\n\n\n\n\n_NotEqualScalar is an alias of _not_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Not_Equal\n \n \nMethod\n.\n\n\n_Not_Equal(lhs, rhs)\n\n\n\n\n_Not_Equal is an alias of _not_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PlusScalar\n \n \nMethod\n.\n\n\n_PlusScalar(data, scalar)\n\n\n\n\n_PlusScalar is an alias of _plus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Power\n \n \nMethod\n.\n\n\n_Power(lhs, rhs)\n\n\n\n\n_Power is an alias of _power.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PowerScalar\n \n \nMethod\n.\n\n\n_PowerScalar(data, scalar)\n\n\n\n\n_PowerScalar is an alias of _power_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RDivScalar\n \n \nMethod\n.\n\n\n_RDivScalar(data, scalar)\n\n\n\n\n_RDivScalar is an alias of _rdiv_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RMinusScalar\n \n \nMethod\n.\n\n\n_RMinusScalar(data, scalar)\n\n\n\n\n_RMinusScalar is an alias of _rminus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RModScalar\n \n \nMethod\n.\n\n\n_RModScalar(data, scalar)\n\n\n\n\n_RModScalar is an alias of _rmod_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RPowerScalar\n \n \nMethod\n.\n\n\n_RPowerScalar(data, scalar)\n\n\n\n\n_RPowerScalar is an alias of _rpower_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._add\n \n \nMethod\n.\n\n\n_add(lhs, rhs)\n\n\n\n\n_add is an alias of elemwise_add.\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._arange\n \n \nMethod\n.\n\n\n_arange(start, stop, step, repeat, ctx, dtype)\n\n\n\n\nReturn evenly spaced values within a given interval. Similar to Numpy\n\n\nArguments\n\n\n\n\nstart::double, required\n: Start of interval. The interval includes this value. The default start value is 0.\n\n\nstop::double or None, optional, default=None\n: End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.\n\n\nstep::double, optional, default=1\n: Spacing between values.\n\n\nrepeat::int, optional, default='1'\n: The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013\n a, a, a.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'int64', 'uint8'},optional, default='float32'\n: Target data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Activation\n \n \nMethod\n.\n\n\n_backward_Activation()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BatchNorm\n \n \nMethod\n.\n\n\n_backward_BatchNorm()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BatchNorm_v1\n \n \nMethod\n.\n\n\n_backward_BatchNorm_v1()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BilinearSampler\n \n \nMethod\n.\n\n\n_backward_BilinearSampler()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_CachedOp\n \n \nMethod\n.\n\n\n_backward_CachedOp()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Concat\n \n \nMethod\n.\n\n\n_backward_Concat()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Convolution\n \n \nMethod\n.\n\n\n_backward_Convolution()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Convolution_v1\n \n \nMethod\n.\n\n\n_backward_Convolution_v1()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Correlation\n \n \nMethod\n.\n\n\n_backward_Correlation()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Crop\n \n \nMethod\n.\n\n\n_backward_Crop()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Custom\n \n \nMethod\n.\n\n\n_backward_Custom()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_CustomFunction\n \n \nMethod\n.\n\n\n_backward_CustomFunction()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Deconvolution\n \n \nMethod\n.\n\n\n_backward_Deconvolution()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Dropout\n \n \nMethod\n.\n\n\n_backward_Dropout()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Embedding\n \n \nMethod\n.\n\n\n_backward_Embedding()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_FullyConnected\n \n \nMethod\n.\n\n\n_backward_FullyConnected()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_GridGenerator\n \n \nMethod\n.\n\n\n_backward_GridGenerator()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\n_backward_IdentityAttachKLSparseReg()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_InstanceNorm\n \n \nMethod\n.\n\n\n_backward_InstanceNorm()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_L2Normalization\n \n \nMethod\n.\n\n\n_backward_L2Normalization()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LRN\n \n \nMethod\n.\n\n\n_backward_LRN()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LeakyReLU\n \n \nMethod\n.\n\n\n_backward_LeakyReLU()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LinearRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LinearRegressionOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LogisticRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LogisticRegressionOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MAERegressionOutput\n \n \nMethod\n.\n\n\n_backward_MAERegressionOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MakeLoss\n \n \nMethod\n.\n\n\n_backward_MakeLoss()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pad\n \n \nMethod\n.\n\n\n_backward_Pad()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pooling\n \n \nMethod\n.\n\n\n_backward_Pooling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pooling_v1\n \n \nMethod\n.\n\n\n_backward_Pooling_v1()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_RNN\n \n \nMethod\n.\n\n\n_backward_RNN()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_ROIPooling\n \n \nMethod\n.\n\n\n_backward_ROIPooling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SVMOutput\n \n \nMethod\n.\n\n\n_backward_SVMOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceLast\n \n \nMethod\n.\n\n\n_backward_SequenceLast()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceMask\n \n \nMethod\n.\n\n\n_backward_SequenceMask()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceReverse\n \n \nMethod\n.\n\n\n_backward_SequenceReverse()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SliceChannel\n \n \nMethod\n.\n\n\n_backward_SliceChannel()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Softmax\n \n \nMethod\n.\n\n\n_backward_Softmax()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxActivation\n \n \nMethod\n.\n\n\n_backward_SoftmaxActivation()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxOutput\n \n \nMethod\n.\n\n\n_backward_SoftmaxOutput()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SparseEmbedding\n \n \nMethod\n.\n\n\n_backward_SparseEmbedding()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SpatialTransformer\n \n \nMethod\n.\n\n\n_backward_SpatialTransformer()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SwapAxis\n \n \nMethod\n.\n\n\n_backward_SwapAxis()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_UpSampling\n \n \nMethod\n.\n\n\n_backward_UpSampling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__CrossDeviceCopy\n \n \nMethod\n.\n\n\n_backward__CrossDeviceCopy()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__NDArray\n \n \nMethod\n.\n\n\n_backward__NDArray()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__Native\n \n \nMethod\n.\n\n\n_backward__Native()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_CTCLoss\n \n \nMethod\n.\n\n\n_backward__contrib_CTCLoss()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_DeformableConvolution\n \n \nMethod\n.\n\n\n_backward__contrib_DeformableConvolution()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_DeformablePSROIPooling\n \n \nMethod\n.\n\n\n_backward__contrib_DeformablePSROIPooling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiBoxDetection\n \n \nMethod\n.\n\n\n_backward__contrib_MultiBoxDetection()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiBoxPrior\n \n \nMethod\n.\n\n\n_backward__contrib_MultiBoxPrior()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiBoxTarget\n \n \nMethod\n.\n\n\n_backward__contrib_MultiBoxTarget()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiProposal\n \n \nMethod\n.\n\n\n_backward__contrib_MultiProposal()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_PSROIPooling\n \n \nMethod\n.\n\n\n_backward__contrib_PSROIPooling()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_Proposal\n \n \nMethod\n.\n\n\n_backward__contrib_Proposal()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_count_sketch\n \n \nMethod\n.\n\n\n_backward__contrib_count_sketch()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_fft\n \n \nMethod\n.\n\n\n_backward__contrib_fft()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_ifft\n \n \nMethod\n.\n\n\n_backward__contrib_ifft()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_abs\n \n \nMethod\n.\n\n\n_backward_abs(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_add\n \n \nMethod\n.\n\n\n_backward_add()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccos\n \n \nMethod\n.\n\n\n_backward_arccos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccosh\n \n \nMethod\n.\n\n\n_backward_arccosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsin\n \n \nMethod\n.\n\n\n_backward_arcsin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsinh\n \n \nMethod\n.\n\n\n_backward_arcsinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctan\n \n \nMethod\n.\n\n\n_backward_arctan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctanh\n \n \nMethod\n.\n\n\n_backward_arctanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_batch_dot\n \n \nMethod\n.\n\n\n_backward_batch_dot()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_add\n \n \nMethod\n.\n\n\n_backward_broadcast_add()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_div\n \n \nMethod\n.\n\n\n_backward_broadcast_div()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_hypot\n \n \nMethod\n.\n\n\n_backward_broadcast_hypot()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_maximum\n \n \nMethod\n.\n\n\n_backward_broadcast_maximum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_minimum\n \n \nMethod\n.\n\n\n_backward_broadcast_minimum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_mod\n \n \nMethod\n.\n\n\n_backward_broadcast_mod()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_mul\n \n \nMethod\n.\n\n\n_backward_broadcast_mul()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_power\n \n \nMethod\n.\n\n\n_backward_broadcast_power()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_sub\n \n \nMethod\n.\n\n\n_backward_broadcast_sub()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cast\n \n \nMethod\n.\n\n\n_backward_cast()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cbrt\n \n \nMethod\n.\n\n\n_backward_cbrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_clip\n \n \nMethod\n.\n\n\n_backward_clip()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_contrib_bipartite_matching\n \n \nMethod\n.\n\n\n_backward_contrib_bipartite_matching(is_ascend, threshold, topk)\n\n\n\n\nArguments\n\n\n\n\nis_ascend::boolean, optional, default=0\n: Use ascend order for scores instead of descending. Please set threshold accordingly.\n\n\nthreshold::float, required\n: Ignore matching when score \n thresh, if is_ascend=false, or ignore score \n thresh, if is_ascend=true.\n\n\ntopk::int, optional, default='-1'\n: Limit the number of matches to topk, set -1 for no limit\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_contrib_box_iou\n \n \nMethod\n.\n\n\n_backward_contrib_box_iou(format)\n\n\n\n\nArguments\n\n\n\n\nformat::{'center', 'corner'},optional, default='corner'\n: The box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_contrib_box_nms\n \n \nMethod\n.\n\n\n_backward_contrib_box_nms(overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)\n\n\n\n\nArguments\n\n\n\n\noverlap_thresh::float, optional, default=0.5\n: Overlapping(IoU) threshold to suppress object with smaller score.\n\n\ntopk::int, optional, default='-1'\n: Apply nms to topk boxes with descending scores, -1 to no restriction.\n\n\ncoord_start::int, optional, default='2'\n: Start index of the consecutive 4 coordinates.\n\n\nscore_index::int, optional, default='1'\n: Index of the scores/confidence of boxes.\n\n\nid_index::int, optional, default='-1'\n: Optional, index of the class categories, -1 to disable.\n\n\nforce_suppress::boolean, optional, default=0\n: Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category\n\n\nin_format::{'center', 'corner'},optional, default='corner'\n: The input box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nout_format::{'center', 'corner'},optional, default='corner'\n: The output box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_copy\n \n \nMethod\n.\n\n\n_backward_copy()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cos\n \n \nMethod\n.\n\n\n_backward_cos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cosh\n \n \nMethod\n.\n\n\n_backward_cosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_degrees\n \n \nMethod\n.\n\n\n_backward_degrees(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_div\n \n \nMethod\n.\n\n\n_backward_div()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_div_scalar\n \n \nMethod\n.\n\n\n_backward_div_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_dot\n \n \nMethod\n.\n\n\n_backward_dot(transpose_a, transpose_b)\n\n\n\n\nArguments\n\n\n\n\ntranspose_a::boolean, optional, default=0\n: If true then transpose the first input before dot.\n\n\ntranspose_b::boolean, optional, default=0\n: If true then transpose the second input before dot.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_expm1\n \n \nMethod\n.\n\n\n_backward_expm1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gamma\n \n \nMethod\n.\n\n\n_backward_gamma(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gammaln\n \n \nMethod\n.\n\n\n_backward_gammaln(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot\n \n \nMethod\n.\n\n\n_backward_hypot()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot_scalar\n \n \nMethod\n.\n\n\n_backward_hypot_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_gelqf\n \n \nMethod\n.\n\n\n_backward_linalg_gelqf()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_gemm\n \n \nMethod\n.\n\n\n_backward_linalg_gemm()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_gemm2\n \n \nMethod\n.\n\n\n_backward_linalg_gemm2()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_potrf\n \n \nMethod\n.\n\n\n_backward_linalg_potrf()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_potri\n \n \nMethod\n.\n\n\n_backward_linalg_potri()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_sumlogdiag\n \n \nMethod\n.\n\n\n_backward_linalg_sumlogdiag()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_syevd\n \n \nMethod\n.\n\n\n_backward_linalg_syevd()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_syrk\n \n \nMethod\n.\n\n\n_backward_linalg_syrk()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_trmm\n \n \nMethod\n.\n\n\n_backward_linalg_trmm()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_trsm\n \n \nMethod\n.\n\n\n_backward_linalg_trsm()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log\n \n \nMethod\n.\n\n\n_backward_log(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log10\n \n \nMethod\n.\n\n\n_backward_log10(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log1p\n \n \nMethod\n.\n\n\n_backward_log1p(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log2\n \n \nMethod\n.\n\n\n_backward_log2(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log_softmax\n \n \nMethod\n.\n\n\n_backward_log_softmax(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_max\n \n \nMethod\n.\n\n\n_backward_max()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum\n \n \nMethod\n.\n\n\n_backward_maximum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum_scalar\n \n \nMethod\n.\n\n\n_backward_maximum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mean\n \n \nMethod\n.\n\n\n_backward_mean()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_min\n \n \nMethod\n.\n\n\n_backward_min()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum\n \n \nMethod\n.\n\n\n_backward_minimum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum_scalar\n \n \nMethod\n.\n\n\n_backward_minimum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mod\n \n \nMethod\n.\n\n\n_backward_mod()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mod_scalar\n \n \nMethod\n.\n\n\n_backward_mod_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mul\n \n \nMethod\n.\n\n\n_backward_mul()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mul_scalar\n \n \nMethod\n.\n\n\n_backward_mul_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nanprod\n \n \nMethod\n.\n\n\n_backward_nanprod()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nansum\n \n \nMethod\n.\n\n\n_backward_nansum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_pick\n \n \nMethod\n.\n\n\n_backward_pick()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power\n \n \nMethod\n.\n\n\n_backward_power()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power_scalar\n \n \nMethod\n.\n\n\n_backward_power_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_prod\n \n \nMethod\n.\n\n\n_backward_prod()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_radians\n \n \nMethod\n.\n\n\n_backward_radians(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rcbrt\n \n \nMethod\n.\n\n\n_backward_rcbrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rdiv_scalar\n \n \nMethod\n.\n\n\n_backward_rdiv_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_reciprocal\n \n \nMethod\n.\n\n\n_backward_reciprocal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_relu\n \n \nMethod\n.\n\n\n_backward_relu(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_repeat\n \n \nMethod\n.\n\n\n_backward_repeat()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_reverse\n \n \nMethod\n.\n\n\n_backward_reverse()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rmod_scalar\n \n \nMethod\n.\n\n\n_backward_rmod_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rpower_scalar\n \n \nMethod\n.\n\n\n_backward_rpower_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rsqrt\n \n \nMethod\n.\n\n\n_backward_rsqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sample_multinomial\n \n \nMethod\n.\n\n\n_backward_sample_multinomial()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sigmoid\n \n \nMethod\n.\n\n\n_backward_sigmoid(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sign\n \n \nMethod\n.\n\n\n_backward_sign(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sin\n \n \nMethod\n.\n\n\n_backward_sin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sinh\n \n \nMethod\n.\n\n\n_backward_sinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_slice\n \n \nMethod\n.\n\n\n_backward_slice()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_slice_axis\n \n \nMethod\n.\n\n\n_backward_slice_axis()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_smooth_l1\n \n \nMethod\n.\n\n\n_backward_smooth_l1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_softmax\n \n \nMethod\n.\n\n\n_backward_softmax(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_softmax_cross_entropy\n \n \nMethod\n.\n\n\n_backward_softmax_cross_entropy()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sparse_retain\n \n \nMethod\n.\n\n\n_backward_sparse_retain()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sqrt\n \n \nMethod\n.\n\n\n_backward_sqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_square\n \n \nMethod\n.\n\n\n_backward_square(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_square_sum\n \n \nMethod\n.\n\n\n_backward_square_sum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_stack\n \n \nMethod\n.\n\n\n_backward_stack()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sub\n \n \nMethod\n.\n\n\n_backward_sub()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sum\n \n \nMethod\n.\n\n\n_backward_sum()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_take\n \n \nMethod\n.\n\n\n_backward_take()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tan\n \n \nMethod\n.\n\n\n_backward_tan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tanh\n \n \nMethod\n.\n\n\n_backward_tanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tile\n \n \nMethod\n.\n\n\n_backward_tile()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_topk\n \n \nMethod\n.\n\n\n_backward_topk()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_where\n \n \nMethod\n.\n\n\n_backward_where()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast_backward\n \n \nMethod\n.\n\n\n_broadcast_backward()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_CTCLoss\n \n \nMethod\n.\n\n\n_contrib_CTCLoss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)\n\n\n\n\nConnectionist Temporal Classification Loss.\n\n\nThe shapes of the inputs and outputs:\n\n\n\n\ndata\n: \n(sequence_length, batch_size, alphabet_size)\n\n\nlabel\n: \n(batch_size, label_sequence_length)\n\n\nout\n: \n(batch_size)\n\n\n\n\nThe \ndata\n tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When \nblank_label\n is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.\n\n\n$label$ is an index matrix of integers. When \nblank_label\n is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when \nblank_label\n is $\"last\"$, the value \n(alphabet_size-1)\n is reserved for blank label.\n\n\nIf a sequence of labels is shorter than \nlabel_sequence_length\n, use the special padding value at the end of the sequence to conform it to the correct length. The padding value is \n0\n when \nblank_label\n is $\"first\"$, and \n-1\n otherwise.\n\n\nFor example, suppose the vocabulary is \n[a, b, c]\n, and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When \nblank_label\n is $\"first\"$, we can index the labels as \n{'a': 1, 'b': 2, 'c': 3}\n, and we reserve the 0-th channel for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]\n\n\nWhen \nblank_label\n is $\"last\"$, we can index the labels as \n{'a': 0, 'b': 1, 'c': 2}\n, and we reserve the channel index 3 for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]\n\n\n$out$ is a list of CTC loss values, one per example in the batch.\n\n\nSee \nConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks\n, A. Graves \net al\n. for more information on the definition and the algorithm.\n\n\nDefined in src/operator/contrib/ctc_loss.cc:L115\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ctc_loss op.\n\n\nlabel::NDArray-or-SymbolicNode\n: Ground-truth labels for the loss.\n\n\ndata_lengths::NDArray-or-SymbolicNode\n: Lengths of data for each of the samples. Only required when use_data_lengths is true.\n\n\nlabel_lengths::NDArray-or-SymbolicNode\n: Lengths of labels for each of the samples. Only required when use_label_lengths is true.\n\n\nuse_data_lengths::boolean, optional, default=0\n: Whether the data lenghts are decided by \ndata_lengths\n. If false, the lengths are equal to the max sequence length.\n\n\nuse_label_lengths::boolean, optional, default=0\n: Whether the label lenghts are decided by \nlabel_lengths\n, or derived from \npadding_mask\n. If false, the lengths are derived from the first occurrence of the value of \npadding_mask\n. The value of \npadding_mask\n is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See \nblank_label\n.\n\n\nblank_label::{'first', 'last'},optional, default='first'\n: Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_DeformableConvolution\n \n \nMethod\n.\n\n\n_contrib_DeformableConvolution(data, offset, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, num_deformable_group, workspace, no_bias, layout)\n\n\n\n\nCompute 2-D deformable convolution on 4-D input.\n\n\nThe deformable convolution operation is described in https://arxiv.org/abs/1703.06211\n\n\nFor 2-D deformable convolution, the shapes are\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\noffset\n: \n(batch_size, num_deformable_group * kernel[0] * kernel[1], height, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0], kernel[1])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n.\n\n\n\n\nDefine::\n\n\nf(x,k,p,s,d) = floor((x+2\np-d\n(k-1)-1)/s)+1\n\n\nthen we have::\n\n\nout_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])\n\n\nIf $no_bias$ is set to be true, then the $bias$ term is ignored.\n\n\nThe default data $layout$ is \nNCHW\n, namely \n(batch_size, channle, height, width)\n.\n\n\nIf $num_group$ is larger than 1, denoted by \ng\n, then split the input $data$ evenly into \ng\n parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the \ni\n-th part of the data with the \ni\n-th weight part. The output is obtained by concating all the \ng\n results.\n\n\nIf $num_deformable_group$ is larger than 1, denoted by \ndg\n, then split the input $offset$ evenly into \ndg\n parts along the channel axis, and also evenly split $out$ evenly into \ndg\n parts along the channel axis. Next compute the deformable convolution, apply the \ni\n-th part of the offset part on the \ni\n-th out.\n\n\nBoth $weight$ and $bias$ are learnable parameters.\n\n\nDefined in src/operator/contrib/deformable_convolution.cc:L100\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the DeformableConvolutionOp.\n\n\noffset::NDArray-or-SymbolicNode\n: Input offset to the DeformableConvolutionOp.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: Convolution kernel size: (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=[]\n: Convolution stride: (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\ndilate::Shape(tuple), optional, default=[]\n: Convolution dilate: (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: Zero pad for convolution: (h, w) or (d, h, w). Defaults to no padding.\n\n\nnum_filter::int (non-negative), required\n: Convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions.\n\n\nnum_deformable_group::int (non-negative), optional, default=1\n: Number of deformable group partitions.\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum temperal workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NCW'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_DeformablePSROIPooling\n \n \nMethod\n.\n\n\n_contrib_DeformablePSROIPooling(data, rois, trans, spatial_scale, output_dim, group_size, pooled_size, part_size, sample_per_part, trans_std, no_trans)\n\n\n\n\nPerforms deformable position-sensitive region-of-interest pooling on inputs. The DeformablePSROIPooling operation is described in https://arxiv.org/abs/1703.06211 .batch_size will change to the number of region bounding boxes after DeformablePSROIPooling\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\ntrans::SymbolicNode\n: transition parameter\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\noutput_dim::int, required\n: fix output dim\n\n\ngroup_size::int, required\n: fix group size\n\n\npooled_size::int, required\n: fix pooled size\n\n\npart_size::int, optional, default='0'\n: fix part size\n\n\nsample_per_part::int, optional, default='1'\n: fix samples per part\n\n\ntrans_std::float, optional, default=0\n: fix transition std\n\n\nno_trans::boolean, optional, default=0\n: Whether to disable trans parameter.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiBoxDetection\n \n \nMethod\n.\n\n\n_contrib_MultiBoxDetection(cls_prob, loc_pred, anchor, clip, threshold, background_id, nms_threshold, force_suppress, variances, nms_topk)\n\n\n\n\nConvert multibox detection predictions.\n\n\nArguments\n\n\n\n\ncls_prob::NDArray-or-SymbolicNode\n: Class probabilities.\n\n\nloc_pred::NDArray-or-SymbolicNode\n: Location regression predictions.\n\n\nanchor::NDArray-or-SymbolicNode\n: Multibox prior anchor boxes\n\n\nclip::boolean, optional, default=1\n: Clip out-of-boundary boxes.\n\n\nthreshold::float, optional, default=0.01\n: Threshold to be a positive prediction.\n\n\nbackground_id::int, optional, default='0'\n: Background id.\n\n\nnms_threshold::float, optional, default=0.5\n: Non-maximum suppression threshold.\n\n\nforce_suppress::boolean, optional, default=0\n: Suppress all detections regardless of class_id.\n\n\nvariances::tuple of \nfloat\n, optional, default=[0.1,0.1,0.2,0.2]\n: Variances to be decoded from box regression output.\n\n\nnms_topk::int, optional, default='-1'\n: Keep maximum top k detections before nms, -1 for no limit.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiBoxPrior\n \n \nMethod\n.\n\n\n_contrib_MultiBoxPrior(data, sizes, ratios, clip, steps, offsets)\n\n\n\n\nGenerate prior(anchor) boxes from data, sizes and ratios.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nsizes::tuple of \nfloat\n, optional, default=[1]\n: List of sizes of generated MultiBoxPriores.\n\n\nratios::tuple of \nfloat\n, optional, default=[1]\n: List of aspect ratios of generated MultiBoxPriores.\n\n\nclip::boolean, optional, default=0\n: Whether to clip out-of-boundary boxes.\n\n\nsteps::tuple of \nfloat\n, optional, default=[-1,-1]\n: Priorbox step across y and x, -1 for auto calculation.\n\n\noffsets::tuple of \nfloat\n, optional, default=[0.5,0.5]\n: Priorbox center offsets, y and x respectively\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiBoxTarget\n \n \nMethod\n.\n\n\n_contrib_MultiBoxTarget(anchor, label, cls_pred, overlap_threshold, ignore_label, negative_mining_ratio, negative_mining_thresh, minimum_negative_samples, variances)\n\n\n\n\nCompute Multibox training targets\n\n\nArguments\n\n\n\n\nanchor::NDArray-or-SymbolicNode\n: Generated anchor boxes.\n\n\nlabel::NDArray-or-SymbolicNode\n: Object detection labels.\n\n\ncls_pred::NDArray-or-SymbolicNode\n: Class predictions.\n\n\noverlap_threshold::float, optional, default=0.5\n: Anchor-GT overlap threshold to be regarded as a positive match.\n\n\nignore_label::float, optional, default=-1\n: Label for ignored anchors.\n\n\nnegative_mining_ratio::float, optional, default=-1\n: Max negative to positive samples ratio, use -1 to disable mining\n\n\nnegative_mining_thresh::float, optional, default=0.5\n: Threshold used for negative mining.\n\n\nminimum_negative_samples::int, optional, default='0'\n: Minimum number of negative samples.\n\n\nvariances::tuple of \nfloat\n, optional, default=[0.1,0.1,0.2,0.2]\n: Variances to be encoded in box regression target.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiProposal\n \n \nMethod\n.\n\n\n_contrib_MultiProposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)\n\n\n\n\nGenerate region proposals via RPN\n\n\nArguments\n\n\n\n\ncls_score::NDArray-or-SymbolicNode\n: Score of how likely proposal is object.\n\n\nbbox_pred::NDArray-or-SymbolicNode\n: BBox Predicted deltas from anchors for proposals\n\n\nim_info::NDArray-or-SymbolicNode\n: Image size and scale.\n\n\nrpn_pre_nms_top_n::int, optional, default='6000'\n: Number of top scoring boxes to keep after applying NMS to RPN proposals\n\n\nrpn_post_nms_top_n::int, optional, default='300'\n: Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU \n= this threshold\n\n\nthreshold::float, optional, default=0.7\n: NMS value, below which to suppress.\n\n\nrpn_min_size::int, optional, default='16'\n: Minimum height or width in proposal\n\n\nscales::tuple of \nfloat\n, optional, default=[4,8,16,32]\n: Used to generate anchor windows by enumerating scales\n\n\nratios::tuple of \nfloat\n, optional, default=[0.5,1,2]\n: Used to generate anchor windows by enumerating ratios\n\n\nfeature_stride::int, optional, default='16'\n: The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.\n\n\noutput_score::boolean, optional, default=0\n: Add score to outputs\n\n\niou_loss::boolean, optional, default=0\n: Usage of IoU Loss\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_PSROIPooling\n \n \nMethod\n.\n\n\n_contrib_PSROIPooling(data, rois, spatial_scale, output_dim, pooled_size, group_size)\n\n\n\n\nPerforms region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after PSROIPooling\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\noutput_dim::int, required\n: fix output dim\n\n\npooled_size::int, required\n: fix pooled size\n\n\ngroup_size::int, optional, default='0'\n: fix group size\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_Proposal\n \n \nMethod\n.\n\n\n_contrib_Proposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)\n\n\n\n\nGenerate region proposals via RPN\n\n\nArguments\n\n\n\n\ncls_score::NDArray-or-SymbolicNode\n: Score of how likely proposal is object.\n\n\nbbox_pred::NDArray-or-SymbolicNode\n: BBox Predicted deltas from anchors for proposals\n\n\nim_info::NDArray-or-SymbolicNode\n: Image size and scale.\n\n\nrpn_pre_nms_top_n::int, optional, default='6000'\n: Number of top scoring boxes to keep after applying NMS to RPN proposals\n\n\nrpn_post_nms_top_n::int, optional, default='300'\n: Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU \n= this threshold\n\n\nthreshold::float, optional, default=0.7\n: NMS value, below which to suppress.\n\n\nrpn_min_size::int, optional, default='16'\n: Minimum height or width in proposal\n\n\nscales::tuple of \nfloat\n, optional, default=[4,8,16,32]\n: Used to generate anchor windows by enumerating scales\n\n\nratios::tuple of \nfloat\n, optional, default=[0.5,1,2]\n: Used to generate anchor windows by enumerating ratios\n\n\nfeature_stride::int, optional, default='16'\n: The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.\n\n\noutput_score::boolean, optional, default=0\n: Add score to outputs\n\n\niou_loss::boolean, optional, default=0\n: Usage of IoU Loss\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_SparseEmbedding\n \n \nMethod\n.\n\n\n_contrib_SparseEmbedding(data, weight, input_dim, output_dim, dtype)\n\n\n\n\nMaps integer indices to vector representations (embeddings).\n\n\nThis operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.\n\n\nFor an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).\n\n\nIf the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).\n\n\nThe storage type of weight must be \nrow_sparse\n, and the gradient of the weight will be of \nrow_sparse\n storage type, too.\n\n\n.. Note::\n\n\n`SparseEmbedding` is designed for the use case where `input_dim` is very large (e.g. 100k).\nThe operator is available on both CPU and GPU.\n\n\n\n\nExamples::\n\n\ninput_dim = 4   output_dim = 5\n\n\n// Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]\n\n\n// Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]\n\n\n// Mapped input x to its vector representation y.   SparseEmbedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                                  [ 15.,  16.,  17.,  18.,  19.]],\n\n\n                            [[  0.,   1.,   2.,   3.,   4.],\n                             [ 10.,  11.,  12.,  13.,  14.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L254\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array to the embedding operator.\n\n\nweight::NDArray-or-SymbolicNode\n: The embedding weight matrix.\n\n\ninput_dim::int, required\n: Vocabulary size of the input indices.\n\n\noutput_dim::int, required\n: Dimension of the embedding vectors.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Data type of weight.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_bipartite_matching\n \n \nMethod\n.\n\n\n_contrib_bipartite_matching(data, is_ascend, threshold, topk)\n\n\n\n\nCompute bipartite matching.   The matching is performed on score matrix with shape [B, N, M]\n\n\n\n\nB: batch_size\n\n\nN: number of rows to match\n\n\nM: number of columns as reference to be matched against.\n\n\n\n\nReturns:   x : matched column indices. -1 indicating non-matched elements in rows.   y : matched row indices.\n\n\nNote::\n\n\nZero gradients are back-propagated in this op for now.\n\n\n\n\nExample::\n\n\ns = [[0.5, 0.6], [0.1, 0.2], [0.3, 0.4]]\nx, y = bipartite_matching(x, threshold=1e-12, is_ascend=False)\nx = [1, -1, 0]\ny = [2\uff0c 0]\n\n\n\n\nDefined in src/operator/contrib/bounding_box.cc:L169\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nis_ascend::boolean, optional, default=0\n: Use ascend order for scores instead of descending. Please set threshold accordingly.\n\n\nthreshold::float, required\n: Ignore matching when score \n thresh, if is_ascend=false, or ignore score \n thresh, if is_ascend=true.\n\n\ntopk::int, optional, default='-1'\n: Limit the number of matches to topk, set -1 for no limit\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_box_iou\n \n \nMethod\n.\n\n\n_contrib_box_iou(lhs, rhs, format)\n\n\n\n\nBounding box overlap of two arrays.   The overlap is defined as Intersection-over-Union, aka, IOU.\n\n\n\n\nlhs: (a_1, a_2, ..., a_n, 4) array\n\n\nrhs: (b_1, b_2, ..., b_n, 4) array\n\n\noutput: (a_1, a_2, ..., a_n, b_1, b_2, ..., b_n) array\n\n\n\n\nNote::\n\n\nZero gradients are back-propagated in this op for now.\n\n\n\n\nExample::\n\n\nx = [[0.5, 0.5, 1.0, 1.0], [0.0, 0.0, 0.5, 0.5]]\ny = [0.25, 0.25, 0.75, 0.75]\nbox_iou(x, y, format='corner') = [[0.1428], [0.1428]]\n\n\n\n\nDefined in src/operator/contrib/bounding_box.cc:L123\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: The first input\n\n\nrhs::NDArray-or-SymbolicNode\n: The second input\n\n\nformat::{'center', 'corner'},optional, default='corner'\n: The box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_box_nms\n \n \nMethod\n.\n\n\n_contrib_box_nms(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)\n\n\n\n\nApply non-maximum suppression to input.\n\n\nThe output will be sorted in descending order according to \nscore\n. Boxes with overlaps larger than \noverlap_thresh\n and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.\n\n\nDuring back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.\n\n\nInput requirements:\n\n\n\n\nInput tensor have at least 2 dimensions, (n, k), any higher dims will be regarded\n\n\n\n\nas batch, e.g. (a, b, c, d, n, k) == (a\nb\nc*d, n, k)\n\n\n\n\nn is the number of boxes in each batch\n\n\nk is the width of each box item.\n\n\n\n\nBy default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.\n\n\n\n\nid_index\n: optional, use -1 to ignore, useful if \nforce_suppress=False\n, which means\n\n\n\n\nwe will skip highly overlapped boxes if one is \napple\n while the other is \ncar\n.\n\n\n\n\ncoord_start\n: required, default=2, the starting index of the 4 coordinates.\n\n\n\n\nTwo formats are supported:   \ncorner\n: [xmin, ymin, xmax, ymax]   \ncenter\n: [x, y, width, height]\n\n\n\n\nscore_index\n: required, default=1, box score/confidence.\n\n\n\n\nWhen two boxes overlap IOU \n \noverlap_thresh\n, the one with smaller score will be suppressed.\n\n\n\n\nin_format\n and \nout_format\n: default='corner', specify in/out box formats.\n\n\n\n\nExamples::\n\n\nx = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]\n\n\nexe.backward\n\n\nin_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]\n\n\nDefined in src/operator/contrib/bounding_box.cc:L82\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\noverlap_thresh::float, optional, default=0.5\n: Overlapping(IoU) threshold to suppress object with smaller score.\n\n\ntopk::int, optional, default='-1'\n: Apply nms to topk boxes with descending scores, -1 to no restriction.\n\n\ncoord_start::int, optional, default='2'\n: Start index of the consecutive 4 coordinates.\n\n\nscore_index::int, optional, default='1'\n: Index of the scores/confidence of boxes.\n\n\nid_index::int, optional, default='-1'\n: Optional, index of the class categories, -1 to disable.\n\n\nforce_suppress::boolean, optional, default=0\n: Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category\n\n\nin_format::{'center', 'corner'},optional, default='corner'\n: The input box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nout_format::{'center', 'corner'},optional, default='corner'\n: The output box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_box_non_maximum_suppression\n \n \nMethod\n.\n\n\n_contrib_box_non_maximum_suppression(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)\n\n\n\n\n_contrib_box_non_maximum_suppression is an alias of _contrib_box_nms.\n\n\nApply non-maximum suppression to input.\n\n\nThe output will be sorted in descending order according to \nscore\n. Boxes with overlaps larger than \noverlap_thresh\n and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.\n\n\nDuring back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.\n\n\nInput requirements:\n\n\n\n\nInput tensor have at least 2 dimensions, (n, k), any higher dims will be regarded\n\n\n\n\nas batch, e.g. (a, b, c, d, n, k) == (a\nb\nc*d, n, k)\n\n\n\n\nn is the number of boxes in each batch\n\n\nk is the width of each box item.\n\n\n\n\nBy default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.\n\n\n\n\nid_index\n: optional, use -1 to ignore, useful if \nforce_suppress=False\n, which means\n\n\n\n\nwe will skip highly overlapped boxes if one is \napple\n while the other is \ncar\n.\n\n\n\n\ncoord_start\n: required, default=2, the starting index of the 4 coordinates.\n\n\n\n\nTwo formats are supported:   \ncorner\n: [xmin, ymin, xmax, ymax]   \ncenter\n: [x, y, width, height]\n\n\n\n\nscore_index\n: required, default=1, box score/confidence.\n\n\n\n\nWhen two boxes overlap IOU \n \noverlap_thresh\n, the one with smaller score will be suppressed.\n\n\n\n\nin_format\n and \nout_format\n: default='corner', specify in/out box formats.\n\n\n\n\nExamples::\n\n\nx = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]\n\n\nexe.backward\n\n\nin_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]\n\n\nDefined in src/operator/contrib/bounding_box.cc:L82\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\noverlap_thresh::float, optional, default=0.5\n: Overlapping(IoU) threshold to suppress object with smaller score.\n\n\ntopk::int, optional, default='-1'\n: Apply nms to topk boxes with descending scores, -1 to no restriction.\n\n\ncoord_start::int, optional, default='2'\n: Start index of the consecutive 4 coordinates.\n\n\nscore_index::int, optional, default='1'\n: Index of the scores/confidence of boxes.\n\n\nid_index::int, optional, default='-1'\n: Optional, index of the class categories, -1 to disable.\n\n\nforce_suppress::boolean, optional, default=0\n: Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category\n\n\nin_format::{'center', 'corner'},optional, default='corner'\n: The input box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nout_format::{'center', 'corner'},optional, default='corner'\n: The output box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_count_sketch\n \n \nMethod\n.\n\n\n_contrib_count_sketch(data, h, s, out_dim, processing_batch_size)\n\n\n\n\nApply CountSketch to input: map a d-dimension data to k-dimension data\"\n\n\n.. note:: \ncount_sketch\n is only available on GPU.\n\n\nAssume input data has shape (N, d), sign hash table s has shape (N, d), index hash table h has shape (N, d) and mapping dimension out_dim = k, each element in s is either +1 or -1, each element in h is random integer from 0 to k-1. Then the operator computs:\n\n\n.. math::    out[h[i]] += data[i] * s[i]\n\n\nExample::\n\n\nout_dim = 5    x = [[1.2, 2.5, 3.4],[3.2, 5.7, 6.6]]    h = [[0, 3, 4]]    s = [[1, -1, 1]]    mx.contrib.ndarray.count_sketch(data=x, h=h, s=s, out_dim = 5) = [[1.2, 0, 0, -2.5, 3.4],                                                                      [3.2, 0, 0, -5.7, 6.6]]\n\n\nDefined in src/operator/contrib/count_sketch.cc:L67\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the CountSketchOp.\n\n\nh::NDArray-or-SymbolicNode\n: The index vector\n\n\ns::NDArray-or-SymbolicNode\n: The sign vector\n\n\nout_dim::int, required\n: The output dimension.\n\n\nprocessing_batch_size::int, optional, default='32'\n: How many sketch vectors to process at one time.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_ctc_loss\n \n \nMethod\n.\n\n\n_contrib_ctc_loss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)\n\n\n\n\n_contrib_ctc_loss is an alias of _contrib_CTCLoss.\n\n\nConnectionist Temporal Classification Loss.\n\n\nThe shapes of the inputs and outputs:\n\n\n\n\ndata\n: \n(sequence_length, batch_size, alphabet_size)\n\n\nlabel\n: \n(batch_size, label_sequence_length)\n\n\nout\n: \n(batch_size)\n\n\n\n\nThe \ndata\n tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When \nblank_label\n is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.\n\n\n$label$ is an index matrix of integers. When \nblank_label\n is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when \nblank_label\n is $\"last\"$, the value \n(alphabet_size-1)\n is reserved for blank label.\n\n\nIf a sequence of labels is shorter than \nlabel_sequence_length\n, use the special padding value at the end of the sequence to conform it to the correct length. The padding value is \n0\n when \nblank_label\n is $\"first\"$, and \n-1\n otherwise.\n\n\nFor example, suppose the vocabulary is \n[a, b, c]\n, and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When \nblank_label\n is $\"first\"$, we can index the labels as \n{'a': 1, 'b': 2, 'c': 3}\n, and we reserve the 0-th channel for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]\n\n\nWhen \nblank_label\n is $\"last\"$, we can index the labels as \n{'a': 0, 'b': 1, 'c': 2}\n, and we reserve the channel index 3 for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]\n\n\n$out$ is a list of CTC loss values, one per example in the batch.\n\n\nSee \nConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks\n, A. Graves \net al\n. for more information on the definition and the algorithm.\n\n\nDefined in src/operator/contrib/ctc_loss.cc:L115\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ctc_loss op.\n\n\nlabel::NDArray-or-SymbolicNode\n: Ground-truth labels for the loss.\n\n\ndata_lengths::NDArray-or-SymbolicNode\n: Lengths of data for each of the samples. Only required when use_data_lengths is true.\n\n\nlabel_lengths::NDArray-or-SymbolicNode\n: Lengths of labels for each of the samples. Only required when use_label_lengths is true.\n\n\nuse_data_lengths::boolean, optional, default=0\n: Whether the data lenghts are decided by \ndata_lengths\n. If false, the lengths are equal to the max sequence length.\n\n\nuse_label_lengths::boolean, optional, default=0\n: Whether the label lenghts are decided by \nlabel_lengths\n, or derived from \npadding_mask\n. If false, the lengths are derived from the first occurrence of the value of \npadding_mask\n. The value of \npadding_mask\n is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See \nblank_label\n.\n\n\nblank_label::{'first', 'last'},optional, default='first'\n: Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_dequantize\n \n \nMethod\n.\n\n\n_contrib_dequantize(input, min_range, max_range, out_type)\n\n\n\n\nDequantize the input tensor into a float tensor. [min_range, max_range] are scalar floats that spcify the range for the output data.\n\n\nEach value of the tensor will undergo the following:\n\n\nout[i] = min_range + (in[i] * (max_range - min_range) / range(INPUT_TYPE))\n\n\nhere \nrange(T) = numeric_limits\nT\n::max() - numeric_limits\nT\n::min()\n\n\nDefined in src/operator/contrib/dequantize.cc:L41\n\n\nArguments\n\n\n\n\ninput::NDArray-or-SymbolicNode\n: A ndarray/symbol of type \nuint8\n\n\nmin_range::NDArray-or-SymbolicNode\n: The minimum scalar value possibly produced for the input\n\n\nmax_range::NDArray-or-SymbolicNode\n: The maximum scalar value possibly produced for the input\n\n\nout_type::{'float32'}, required\n: Output data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_fft\n \n \nMethod\n.\n\n\n_contrib_fft(data, compute_size)\n\n\n\n\nApply 1D FFT to input\"\n\n\n.. note:: \nfft\n is only available on GPU.\n\n\nCurrently accept 2 input data shapes: (N, d) or (N1, N2, N3, d), data can only be real numbers. The output data has shape: (N, 2\nd) or (N1, N2, N3, 2\nd). The format is: [real0, imag0, real1, imag1, ...].\n\n\nExample::\n\n\ndata = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.fft(data = mx.nd.array(data,ctx = mx.gpu(0)))\n\n\nDefined in src/operator/contrib/fft.cc:L56\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the FFTOp.\n\n\ncompute_size::int, optional, default='128'\n: Maximum size of sub-batch to be forwarded at one time\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_ifft\n \n \nMethod\n.\n\n\n_contrib_ifft(data, compute_size)\n\n\n\n\nApply 1D ifft to input\"\n\n\n.. note:: \nifft\n is only available on GPU.\n\n\nCurrently accept 2 input data shapes: (N, d) or (N1, N2, N3, d). Data is in format: [real0, imag0, real1, imag1, ...]. Last dimension must be an even number. The output data has shape: (N, d/2) or (N1, N2, N3, d/2). It is only the real part of the result.\n\n\nExample::\n\n\ndata = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.ifft(data = mx.nd.array(data,ctx = mx.gpu(0)))\n\n\nDefined in src/operator/contrib/ifft.cc:L58\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the IFFTOp.\n\n\ncompute_size::int, optional, default='128'\n: Maximum size of sub-batch to be forwarded at one time\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_quantize\n \n \nMethod\n.\n\n\n_contrib_quantize(input, min_range, max_range, out_type)\n\n\n\n\nQuantize a input tensor from float to \nout_type\n, with user-specified \nmin_range\n and \nmax_range\n.\n\n\n[min_range, max_range] are scalar floats that spcify the range for the input data. Each value of the tensor will undergo the following:\n\n\nout[i] = (in[i] - min_range) * range(OUTPUT_TYPE) / (max_range - min_range)\n\n\nhere \nrange(T) = numeric_limits\nT\n::max() - numeric_limits\nT\n::min()\n\n\nDefined in src/operator/contrib/quantize.cc:L41\n\n\nArguments\n\n\n\n\ninput::NDArray-or-SymbolicNode\n: A ndarray/symbol of type \nfloat32\n\n\nmin_range::NDArray-or-SymbolicNode\n: The minimum scalar value possibly produced for the input\n\n\nmax_range::NDArray-or-SymbolicNode\n: The maximum scalar value possibly produced for the input\n\n\nout_type::{'uint8'},optional, default='uint8'\n: Output data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copy\n \n \nMethod\n.\n\n\n_copy(data)\n\n\n\n\nReturns a copy of the input.\n\n\nFrom:src/operator/tensor/elemwise_unary_op_basic.cc:112\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copyto\n \n \nMethod\n.\n\n\n_copyto(data)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: input data\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign\n \n \nMethod\n.\n\n\n_crop_assign(lhs, rhs, begin, end, step)\n\n\n\n\n_crop_assign is an alias of _slice_assign.\n\n\nAssign the rhs to a cropped subset of lhs.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as lhs.\n\n\nlhs and rhs are of the same data type, and on the same device.\n\n\n\n\nFrom:src/operator/tensor/matrix_op.cc:381\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: Source input\n\n\nrhs::NDArray-or-SymbolicNode\n: value to assign\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign_scalar\n \n \nMethod\n.\n\n\n_crop_assign_scalar(data, scalar, begin, end, step)\n\n\n\n\n_crop_assign_scalar is an alias of _slice_assign_scalar.\n\n\n(Assign the scalar to a cropped subset of the input.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as input\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:406\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nscalar::float, optional, default=0\n: The scalar value for assignment.\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvcopyMakeBorder\n \n \nMethod\n.\n\n\n_cvcopyMakeBorder(src, top, bot, left, right, type, value, values)\n\n\n\n\nPad image border with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\ntop::int, required\n: Top margin.\n\n\nbot::int, required\n: Bottom margin.\n\n\nleft::int, required\n: Left margin.\n\n\nright::int, required\n: Right margin.\n\n\ntype::int, optional, default='0'\n: Filling type (default=cv2.BORDER_CONSTANT).\n\n\nvalue::double, optional, default=0\n: (Deprecated! Use $values$ instead.) Fill with single value.\n\n\nvalues::tuple of \ndouble\n, optional, default=[]\n: Fill with value(RGB[A] or gray), up to 4 channels.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimdecode\n \n \nMethod\n.\n\n\n_cvimdecode(buf, flag, to_rgb)\n\n\n\n\nDecode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.\n\n\nArguments\n\n\n\n\nbuf::NDArray\n: Buffer containing binary encoded image\n\n\nflag::int, optional, default='1'\n: Convert decoded image to grayscale (0) or color (1).\n\n\nto_rgb::boolean, optional, default=1\n: Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimread\n \n \nMethod\n.\n\n\n_cvimread(filename, flag, to_rgb)\n\n\n\n\nRead and decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.\n\n\nArguments\n\n\n\n\nfilename::string, required\n: Name of the image file to be loaded.\n\n\nflag::int, optional, default='1'\n: Convert decoded image to grayscale (0) or color (1).\n\n\nto_rgb::boolean, optional, default=1\n: Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimresize\n \n \nMethod\n.\n\n\n_cvimresize(src, w, h, interp)\n\n\n\n\nResize image with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\nw::int, required\n: Width of resized image.\n\n\nh::int, required\n: Height of resized image.\n\n\ninterp::int, optional, default='1'\n: Interpolation method (default=cv2.INTER_LINEAR).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._div_scalar\n \n \nMethod\n.\n\n\n_div_scalar(data, scalar)\n\n\n\n\nDivide an array with a scalar.\n\n\n$_div_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._docsig\n \n \nMethod\n.\n\n\nGenerate docstring from function signature\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal\n \n \nMethod\n.\n\n\n_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal_scalar\n \n \nMethod\n.\n\n\n_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._get_ndarray_function_def\n \n \nMethod\n.\n\n\nThe libxmnet APIs are automatically imported from \nlibmxnet.so\n. The functions listed here operate on \nNDArray\n objects. The arguments to the functions are typically ordered as\n\n\n  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)\n\n\n\n\nunless \nNDARRAY_ARG_BEFORE_SCALAR\n is not set. In this case, the scalars are put before the input arguments:\n\n\n  func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)\n\n\n\n\nIf \nACCEPT_EMPTY_MUTATE_TARGET\n is set. An overloaded function without the output arguments will also be defined:\n\n\n  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)\n\n\n\n\nUpon calling, the output arguments will be automatically initialized with empty NDArrays.\n\n\nThose functions always return the output arguments. If there is only one output (the typical situation), that object (\nNDArray\n) is returned. Otherwise, a tuple containing all the outputs will be returned.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._grad_add\n \n \nMethod\n.\n\n\n_grad_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater\n \n \nMethod\n.\n\n\n_greater(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal\n \n \nMethod\n.\n\n\n_greater_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal_scalar\n \n \nMethod\n.\n\n\n_greater_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_scalar\n \n \nMethod\n.\n\n\n_greater_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot\n \n \nMethod\n.\n\n\n_hypot(lhs, rhs)\n\n\n\n\nGiven the \"legs\" of a right triangle, return its hypotenuse.\n\n\nDefined in src/operator/tensor/elemwise_binary_op_extended.cc:L79\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot_scalar\n \n \nMethod\n.\n\n\n_hypot_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._identity_with_attr_like_rhs\n \n \nMethod\n.\n\n\n_identity_with_attr_like_rhs(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input.\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._imdecode\n \n \nMethod\n.\n\n\n_imdecode(mean, index, x0, y0, x1, y1, c, size)\n\n\n\n\nDecode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer\n\n\nArguments\n\n\n\n\nmean::NDArray-or-SymbolicNode\n: image mean\n\n\nindex::int\n: buffer position for output\n\n\nx0::int\n: x0\n\n\ny0::int\n: y0\n\n\nx1::int\n: x1\n\n\ny1::int\n: y1\n\n\nc::int\n: channel\n\n\nsize::int\n: length of str_img\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser\n \n \nMethod\n.\n\n\n_lesser(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal\n \n \nMethod\n.\n\n\n_lesser_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal_scalar\n \n \nMethod\n.\n\n\n_lesser_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_scalar\n \n \nMethod\n.\n\n\n_lesser_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_gelqf\n \n \nMethod\n.\n\n\n_linalg_gelqf(A)\n\n\n\n\nLQ factorization for general matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, we compute the LQ factorization (LAPACK \ngelqf\n, followed by \norglq\n). \nA\n must have shape \n(x, y)\n with \nx \n= y\n, and must have full rank \n=x\n. The LQ factorization consists of \nL\n with shape \n(x, x)\n and \nQ\n with shape \n(x, y)\n, so that:\n\n\nA\n = \nL\n * \nQ\n\n\nHere, \nL\n is lower triangular (upper triangle equal to zero) with nonzero diagonal, and \nQ\n is row-orthonormal, meaning that\n\n\nQ\n * \nQ\n\\ :sup:\nT\n\n\nis equal to the identity matrix of shape \n(x, x)\n.\n\n\nIf \nn\n2\n, \ngelqf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]\n\n\n// Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L529\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be factorized\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_gemm\n \n \nMethod\n.\n\n\n_linalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)\n\n\n\n\nPerforms general matrix multiplication and accumulation. Input are tensors \nA\n, \nB\n, \nC\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n) + \nbeta\n * \nC\n\n\nHere, \nalpha\n and \nbeta\n are scalar parameters, and \nop()\n is either the identity or matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]\n\n\n// Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L69\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nC::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\nbeta::double, optional, default=1\n: Scalar factor multiplied with C.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_gemm2\n \n \nMethod\n.\n\n\n_linalg_gemm2(A, B, transpose_a, transpose_b, alpha)\n\n\n\n\nPerforms general matrix multiplication. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n)\n\n\nHere \nalpha\n is a scalar parameter and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]\n\n\n// Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L128\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_potrf\n \n \nMethod\n.\n\n\n_linalg_potrf(A)\n\n\n\n\nPerforms Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the Cholesky factor \nL\n of the symmetric, positive definite matrix \nA\n is computed. \nL\n is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:\n\n\nA\n = \nL\n * \nL\n\\ :sup:\nT\n\n\nIf \nn\n2\n, \npotrf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]\n\n\n// Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L178\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be decomposed\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_potri\n \n \nMethod\n.\n\n\n_linalg_potri(A)\n\n\n\n\nPerforms matrix inversion from a Cholesky factorization. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:\n\n\nout\n = \nA\n\\ :sup:\n-T\n * \nA\n\\ :sup:\n-1\n\n\nIn other words, if \nA\n is the Cholesky factor of a symmetric positive definite matrix \nB\n (obtained by \npotrf\n), then\n\n\nout\n = \nB\n\\ :sup:\n-1\n\n\nIf \nn\n2\n, \npotri\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\n.. note:: Use this operator only if you are certain you need the inverse of \nB\n, and           cannot use the Cholesky factor \nA\n (\npotrf\n), together with backsubstitution           (\ntrsm\n). The latter is numerically much safer, and also cheaper.\n\n\nExamples::\n\n\n// Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]\n\n\n// Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L236\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_sumlogdiag\n \n \nMethod\n.\n\n\n_linalg_sumlogdiag(A)\n\n\n\n\nComputes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).\n\n\nIf \nn\n2\n, \nsumlogdiag\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]\n\n\n// Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]\n\n\nDefined in src/operator/tensor/la_op.cc:L405\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of square matrices\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_syevd\n \n \nMethod\n.\n\n\n_linalg_syevd(A)\n\n\n\n\nEigendecomposition for symmetric matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n must be symmetric, of shape \n(x, x)\n. We compute the eigendecomposition, resulting in the orthonormal matrix \nU\n of eigenvectors, shape \n(x, x)\n, and the vector \nL\n of eigenvalues, shape \n(x,)\n, so that:\n\n\nU\n * \nA\n = \ndiag(L)\n * \nU\n\n\nHere:\n\n\nU\n * \nU\n\\ :sup:\nT\n = \nU\n\\ :sup:\nT\n * \nU\n = \nI\n\n\nwhere \nI\n is the identity matrix. Also, \nL(0) \n= L(1) \n= L(2) \n= ...\n (ascending order).\n\n\nIf \nn\n2\n, \nsyevd\n is performed separately on the trailing two dimensions of \nA\n (batch mode). In this case, \nU\n has \nn\n dimensions like \nA\n, and \nL\n has \nn-1\n dimensions.\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\n.. note:: Derivatives for this operator are defined only if \nA\n is such that all its           eigenvalues are distinct, and the eigengaps are not too small. If you need           gradients, do not apply this operator to matrices with multiple eigenvalues.\n\n\nExamples::\n\n\n// Single symmetric eigendecomposition    A = [[1., 2.], [2., 4.]]    U, L = syevd(A)    U = [[0.89442719, -0.4472136],         [0.4472136, 0.89442719]]    L = [0., 5.]\n\n\n// Batch symmetric eigendecomposition    A = [[[1., 2.], [2., 4.]],         [[1., 2.], [2., 5.]]]    U, L = syevd(A)    U = [[[0.89442719, -0.4472136],          [0.4472136, 0.89442719]],         [[0.92387953, -0.38268343],          [0.38268343, 0.92387953]]]    L = [[0., 5.],         [0.17157288, 5.82842712]]\n\n\nDefined in src/operator/tensor/la_op.cc:L598\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be factorized\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_syrk\n \n \nMethod\n.\n\n\n_linalg_syrk(A, transpose, alpha)\n\n\n\n\nMultiplication of matrix with its transpose. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the operator performs the BLAS3 function \nsyrk\n:\n\n\nout\n = \nalpha\n * \nA\n * \nA\n\\ :sup:\nT\n\n\nif \ntranspose=False\n, or\n\n\nout\n = \nalpha\n * \nA\n\\ :sup:\nT\n \\ * \nA\n\n\nif \ntranspose=True\n.\n\n\nIf \nn\n2\n, \nsyrk\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]\n\n\n// Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L461\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transpose of input matrix.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_trmm\n \n \nMethod\n.\n\n\n_linalg_trmm(A, B, transpose, rightside, alpha)\n\n\n\n\nPerforms multiplication with a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrmm\n:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n = \nalpha\n * \nB\n * \nop\n\\ (\nA\n)\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrmm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]\n\n\n// Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L293\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_trsm\n \n \nMethod\n.\n\n\n_linalg_trsm(A, B, transpose, rightside, alpha)\n\n\n\n\nSolves matrix equation involving a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrsm\n, solving for \nout\n in:\n\n\nop\n\\ (\nA\n) * \nout\n = \nalpha\n * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n * \nop\n\\ (\nA\n) = \nalpha\n * \nB\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrsm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n\n\n// Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L356\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum\n \n \nMethod\n.\n\n\n_maximum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum_scalar\n \n \nMethod\n.\n\n\n_maximum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum\n \n \nMethod\n.\n\n\n_minimum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum_scalar\n \n \nMethod\n.\n\n\n_minimum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus!\n \n \nMethod\n.\n\n\n_minus!(x::NDArray, y::NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus\n \n \nMethod\n.\n\n\n_minus(x::NDArray, y::NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus_scalar\n \n \nMethod\n.\n\n\n_minus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mod!\n \n \nMethod\n.\n\n\n_mod!(x::NDArray, y::NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mod\n \n \nMethod\n.\n\n\n_mod(x::NDArray, y::NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mod_scalar!\n \n \nMethod\n.\n\n\n_mod_scalar!(x::NDArray, y::Real)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mod_scalar\n \n \nMethod\n.\n\n\n_mod_scalar(x::NDArray, y::Real)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul\n \n \nMethod\n.\n\n\n_mul(lhs, rhs)\n\n\n\n\n_mul is an alias of elemwise_mul.\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul_scalar\n \n \nMethod\n.\n\n\n_mul_scalar(data, scalar)\n\n\n\n\nMultiply an array with a scalar.\n\n\n$_mul_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal\n \n \nMethod\n.\n\n\n_not_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal_scalar\n \n \nMethod\n.\n\n\n_not_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._onehot_encode\n \n \nMethod\n.\n\n\n_onehot_encode(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus!\n \n \nMethod\n.\n\n\n_plus!(x::NDArray, y::NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus\n \n \nMethod\n.\n\n\n_plus(x::NDArray, y::NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus_scalar\n \n \nMethod\n.\n\n\n_plus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power\n \n \nMethod\n.\n\n\n_power(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power_scalar\n \n \nMethod\n.\n\n\n_power_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_exponential\n \n \nMethod\n.\n\n\n_random_exponential(lam, shape, ctx, dtype)\n\n\n\n\nDraw random samples from an exponential distribution.\n\n\nSamples are distributed according to an exponential distribution parametrized by \nlambda\n (rate).\n\n\nExample::\n\n\nexponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]\n\n\nDefined in src/operator/random/sample_op.cc:L115\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the exponential distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_gamma\n \n \nMethod\n.\n\n\n_random_gamma(alpha, beta, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a gamma distribution.\n\n\nSamples are distributed according to a gamma distribution parametrized by \nalpha\n (shape) and \nbeta\n (scale).\n\n\nExample::\n\n\ngamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]\n\n\nDefined in src/operator/random/sample_op.cc:L100\n\n\nArguments\n\n\n\n\nalpha::float, optional, default=1\n: Alpha parameter (shape) of the gamma distribution.\n\n\nbeta::float, optional, default=1\n: Beta parameter (scale) of the gamma distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_generalized_negative_binomial\n \n \nMethod\n.\n\n\n_random_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a generalized negative binomial distribution.\n\n\nSamples are distributed according to a generalized negative binomial distribution parametrized by \nmu\n (mean) and \nalpha\n (dispersion). \nalpha\n is defined as \n1/k\n where \nk\n is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\ngeneralized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]\n\n\nDefined in src/operator/random/sample_op.cc:L168\n\n\nArguments\n\n\n\n\nmu::float, optional, default=1\n: Mean of the negative binomial distribution.\n\n\nalpha::float, optional, default=1\n: Alpha (dispersion) parameter of the negative binomial distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_negative_binomial\n \n \nMethod\n.\n\n\n_random_negative_binomial(k, p, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a negative binomial distribution.\n\n\nSamples are distributed according to a negative binomial distribution parametrized by \nk\n (limit of unsuccessful experiments) and \np\n (failure probability in each experiment). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\nnegative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]\n\n\nDefined in src/operator/random/sample_op.cc:L149\n\n\nArguments\n\n\n\n\nk::int, optional, default='1'\n: Limit of unsuccessful experiments.\n\n\np::float, optional, default=1\n: Failure probability in each experiment.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_normal\n \n \nMethod\n.\n\n\n_random_normal(loc, scale, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a normal (Gaussian) distribution.\n\n\n.. note:: The existing alias $normal$ is deprecated.\n\n\nSamples are distributed according to a normal distribution parametrized by \nloc\n (mean) and \nscale\n (standard deviation).\n\n\nExample::\n\n\nnormal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]\n\n\nDefined in src/operator/random/sample_op.cc:L85\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_poisson\n \n \nMethod\n.\n\n\n_random_poisson(lam, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a Poisson distribution.\n\n\nSamples are distributed according to a Poisson distribution parametrized by \nlambda\n (rate). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\npoisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]\n\n\nDefined in src/operator/random/sample_op.cc:L132\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the Poisson distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_uniform\n \n \nMethod\n.\n\n\n_random_uniform(low, high, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a uniform distribution.\n\n\n.. note:: The existing alias $uniform$ is deprecated.\n\n\nSamples are uniformly distributed over the half-open interval \n[low, high)\n (includes \nlow\n, but excludes \nhigh\n).\n\n\nExample::\n\n\nuniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]\n\n\nDefined in src/operator/random/sample_op.cc:L66\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: Lower bound of the distribution.\n\n\nhigh::float, optional, default=1\n: Upper bound of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rdiv_scalar\n \n \nMethod\n.\n\n\n_rdiv_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rminus_scalar\n \n \nMethod\n.\n\n\n_rminus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rmod_scalar!\n \n \nMethod\n.\n\n\n_rmod_scalar!(x::NDArray, y::Real)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rmod_scalar\n \n \nMethod\n.\n\n\n_rmod_scalar(x::NDArray, y::Real)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rpower_scalar\n \n \nMethod\n.\n\n\n_rpower_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_exponential\n \n \nMethod\n.\n\n\n_sample_exponential(lam, shape, dtype)\n\n\n\n\nConcurrent sampling from multiple exponential distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]\n\n\n// Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]\n\n\nDefined in src/operator/random/multisample_op.cc:L284\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_gamma\n \n \nMethod\n.\n\n\n_sample_gamma(alpha, shape, dtype, beta)\n\n\n\n\nConcurrent sampling from multiple gamma distributions with parameters \nalpha\n (shape) and \nbeta\n (scale).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nalpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]\n\n\n// Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]\n\n\n// Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]\n\n\nDefined in src/operator/random/multisample_op.cc:L282\n\n\nArguments\n\n\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (shape) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nbeta::NDArray-or-SymbolicNode\n: Beta (scale) parameters of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_generalized_negative_binomial\n \n \nMethod\n.\n\n\n_sample_generalized_negative_binomial(mu, shape, dtype, alpha)\n\n\n\n\nConcurrent sampling from multiple generalized negative binomial distributions with parameters \nmu\n (mean) and \nalpha\n (dispersion).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nmu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]\n\n\n// Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]\n\n\n// Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L293\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (dispersion) parameters of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_multinomial\n \n \nMethod\n.\n\n\n_sample_multinomial(data, shape, get_prob, dtype)\n\n\n\n\nConcurrent sampling from multiple multinomial distributions.\n\n\ndata\n is an \nn\n dimensional array whose last dimension has length \nk\n, where \nk\n is the number of possible outcomes of each multinomial distribution. This operator will draw \nshape\n samples from each distribution. If shape is empty one sample will be drawn from each distribution.\n\n\nIf \nget_prob\n is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.\n\n\nNote that the input distribution must be normalized, i.e. \ndata\n must sum to 1 along its last axis.\n\n\nExamples::\n\n\nprobs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]\n\n\n// Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]\n\n\n// Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]\n\n\n// requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Distribution probabilities. Must sum to one on the last axis.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\nget_prob::boolean, optional, default=0\n: Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.\n\n\ndtype::{'int32'},optional, default='int32'\n: DType of the output in case this can't be inferred. Only support int32 for now.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_negative_binomial\n \n \nMethod\n.\n\n\n_sample_negative_binomial(k, shape, dtype, p)\n\n\n\n\nConcurrent sampling from multiple negative binomial distributions with parameters \nk\n (failure limit) and \np\n (failure probability).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nk = [ 20, 49 ]    p = [ 0.4 , 0.77 ]\n\n\n// Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]\n\n\n// Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L289\n\n\nArguments\n\n\n\n\nk::NDArray-or-SymbolicNode\n: Limits of unsuccessful experiments.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\np::NDArray-or-SymbolicNode\n: Failure probabilities in each experiment.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_normal\n \n \nMethod\n.\n\n\n_sample_normal(mu, shape, dtype, sigma)\n\n\n\n\nConcurrent sampling from multiple normal distributions with parameters \nmu\n (mean) and \nsigma\n (standard deviation).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nmu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]\n\n\n// Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]\n\n\nDefined in src/operator/random/multisample_op.cc:L279\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nsigma::NDArray-or-SymbolicNode\n: Standard deviations of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_poisson\n \n \nMethod\n.\n\n\n_sample_poisson(lam, shape, dtype)\n\n\n\n\nConcurrent sampling from multiple Poisson distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]\n\n\n// Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L286\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_uniform\n \n \nMethod\n.\n\n\n_sample_uniform(low, shape, dtype, high)\n\n\n\n\nConcurrent sampling from multiple uniform distributions on the intervals given by \n[low,high)\n.\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nlow = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]\n\n\n// Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]\n\n\nDefined in src/operator/random/multisample_op.cc:L277\n\n\nArguments\n\n\n\n\nlow::NDArray-or-SymbolicNode\n: Lower bounds of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nhigh::NDArray-or-SymbolicNode\n: Upper bounds of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_elemwise_div\n \n \nMethod\n.\n\n\n_scatter_elemwise_div(lhs, rhs)\n\n\n\n\nDivides arguments element-wise.  If the left-hand-side input is 'row_sparse', then only the values which exist in the left-hand sparse array are computed.  The 'missing' values are ignored.\n\n\nThe storage type of $_scatter_elemwise_div$ output depends on storage types of inputs\n\n\n\n\n_scatter_elemwise_div(row_sparse, row_sparse) = row_sparse\n\n\n_scatter_elemwise_div(row_sparse, dense) = row_sparse\n\n\n_scatter_elemwise_div(row_sparse, csr) = row_sparse\n\n\notherwise, $_scatter_elemwise_div$ behaves exactly like elemwise_div and generates output\n\n\n\n\nwith default storage\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_minus_scalar\n \n \nMethod\n.\n\n\n_scatter_minus_scalar(data, scalar)\n\n\n\n\nSubtracts a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.\n\n\nThe storage type of $_scatter_minus_scalar$ output depends on storage types of inputs\n\n\n\n\n_scatter_minus_scalar(row_sparse, scalar) = row_sparse\n\n\n_scatter_minus_scalar(csr, scalar) = csr\n\n\notherwise, $_scatter_minus_scalar$ behaves exactly like _minus_scalar and generates output\n\n\n\n\nwith default storage\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_plus_scalar\n \n \nMethod\n.\n\n\n_scatter_plus_scalar(data, scalar)\n\n\n\n\nAdds a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.\n\n\nThe storage type of $_scatter_plus_scalar$ output depends on storage types of inputs\n\n\n\n\n_scatter_plus_scalar(row_sparse, scalar) = row_sparse\n\n\n_scatter_plus_scalar(csr, scalar) = csr\n\n\notherwise, $_scatter_plus_scalar$ behaves exactly like _plus_scalar and generates output\n\n\n\n\nwith default storage\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_set_nd\n \n \nMethod\n.\n\n\n_scatter_set_nd(data, indices, shape)\n\n\n\n\nThis operator has the same functionality as scatter_nd except that it does not reset the elements not indexed by the input index \nNDArray\n in the input data \nNDArray\n.\n\n\n.. note:: This operator is for internal use only.\n\n\nExamples::\n\n\ndata = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   out = [[1, 1], [1, 1]]   scatter_nd(data=data, indices=indices, out=out)   out = [[0, 1], [2, 3]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: data\n\n\nindices::NDArray-or-SymbolicNode\n: indices\n\n\nshape::Shape(tuple), required\n: Shape of output.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._set_value\n \n \nMethod\n.\n\n\n_set_value(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::real_t\n: Source input to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._slice_assign\n \n \nMethod\n.\n\n\n_slice_assign(lhs, rhs, begin, end, step)\n\n\n\n\nAssign the rhs to a cropped subset of lhs.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as lhs.\n\n\nlhs and rhs are of the same data type, and on the same device.\n\n\n\n\nFrom:src/operator/tensor/matrix_op.cc:381\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: Source input\n\n\nrhs::NDArray-or-SymbolicNode\n: value to assign\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._slice_assign_scalar\n \n \nMethod\n.\n\n\n_slice_assign_scalar(data, scalar, begin, end, step)\n\n\n\n\n(Assign the scalar to a cropped subset of the input.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as input\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:406\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nscalar::float, optional, default=0\n: The scalar value for assignment.\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_ElementWiseSum\n \n \nMethod\n.\n\n\n_sparse_ElementWiseSum(args)\n\n\n\n\n_sparse_ElementWiseSum is an alias of add_n.\n\n\nNote\n: _sparse_ElementWiseSum takes variable number of positional inputs. So instead of calling as _sparse_ElementWiseSum([x, y, z], num_args=3), one should call via _sparse_ElementWiseSum(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_abs\n \n \nMethod\n.\n\n\n_sparse_abs(data)\n\n\n\n\n_sparse_abs is an alias of abs.\n\n\nReturns element-wise absolute value of the input.\n\n\nExample::\n\n\nabs([-2, 0, 3]) = [2, 0, 3]\n\n\nThe storage type of $abs$ output depends upon the input storage type:\n\n\n\n\nabs(default) = default\n\n\nabs(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L386\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_adam_update\n \n \nMethod\n.\n\n\n_sparse_adam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_adam_update is an alias of adam_update.\n\n\nUpdate function for Adam optimizer. Adam is seen as a generalization of AdaGrad.\n\n\nAdam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).\n\n\n.. math::\n\n\ng_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }\n\n\nIt updates the weights using::\n\n\nm = beta1\nm + (1-beta1)\ngrad  v = beta2\nv + (1-beta2)\n(grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)\n\n\nIf w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::\n\n\nfor row in grad.indices:      m[row] = beta1\nm[row] + (1-beta1)\ngrad[row]      v[row] = beta2\nv[row] + (1-beta2)\n(grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)\n\n\nDefined in src/operator/optimizer_op.cc:L208\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmean::NDArray-or-SymbolicNode\n: Moving mean\n\n\nvar::NDArray-or-SymbolicNode\n: Moving variance\n\n\nlr::float, required\n: Learning rate\n\n\nbeta1::float, optional, default=0.9\n: The decay rate for the 1st moment estimates.\n\n\nbeta2::float, optional, default=0.999\n: The decay rate for the 2nd moment estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_add_n\n \n \nMethod\n.\n\n\n_sparse_add_n(args)\n\n\n\n\n_sparse_add_n is an alias of add_n.\n\n\nNote\n: _sparse_add_n takes variable number of positional inputs. So instead of calling as _sparse_add_n([x, y, z], num_args=3), one should call via _sparse_add_n(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arccos\n \n \nMethod\n.\n\n\n_sparse_arccos(data)\n\n\n\n\n_sparse_arccos is an alias of arccos.\n\n\nReturns element-wise inverse cosine of the input array.\n\n\nThe input should be in range \n[-1, 1]\n. The output is in the closed interval :math:\n[0, \\pi]\n\n\n.. math::    arccos([-1, -.707, 0, .707, 1]) = [\\pi, 3\\pi/4, \\pi/2, \\pi/4, 0]\n\n\nThe storage type of $arccos$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L123\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arccosh\n \n \nMethod\n.\n\n\n_sparse_arccosh(data)\n\n\n\n\n_sparse_arccosh is an alias of arccosh.\n\n\nReturns the element-wise inverse hyperbolic cosine of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arccosh$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L264\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arcsin\n \n \nMethod\n.\n\n\n_sparse_arcsin(data)\n\n\n\n\n_sparse_arcsin is an alias of arcsin.\n\n\nReturns element-wise inverse sine of the input array.\n\n\nThe input should be in the range \n[-1, 1]\n. The output is in the closed interval of [:math:\n-\\pi/2\n, :math:\n\\pi/2\n].\n\n\n.. math::    arcsin([-1, -.707, 0, .707, 1]) = [-\\pi/2, -\\pi/4, 0, \\pi/4, \\pi/2]\n\n\nThe storage type of $arcsin$ output depends upon the input storage type:\n\n\n\n\narcsin(default) = default\n\n\narcsin(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arcsinh\n \n \nMethod\n.\n\n\n_sparse_arcsinh(data)\n\n\n\n\n_sparse_arcsinh is an alias of arcsinh.\n\n\nReturns the element-wise inverse hyperbolic sine of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arcsinh$ output depends upon the input storage type:\n\n\n\n\narcsinh(default) = default\n\n\narcsinh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L250\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arctan\n \n \nMethod\n.\n\n\n_sparse_arctan(data)\n\n\n\n\n_sparse_arctan is an alias of arctan.\n\n\nReturns element-wise inverse tangent of the input array.\n\n\nThe output is in the closed interval :math:\n[-\\pi/2, \\pi/2]\n\n\n.. math::    arctan([-1, 0, 1]) = [-\\pi/4, 0, \\pi/4]\n\n\nThe storage type of $arctan$ output depends upon the input storage type:\n\n\n\n\narctan(default) = default\n\n\narctan(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L144\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arctanh\n \n \nMethod\n.\n\n\n_sparse_arctanh(data)\n\n\n\n\n_sparse_arctanh is an alias of arctanh.\n\n\nReturns the element-wise inverse hyperbolic tangent of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arctanh$ output depends upon the input storage type:\n\n\n\n\narctanh(default) = default\n\n\narctanh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L281\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_cast_storage\n \n \nMethod\n.\n\n\n_sparse_cast_storage(data, stype)\n\n\n\n\n_sparse_cast_storage is an alias of cast_storage.\n\n\nCasts tensor storage type to the new type.\n\n\nWhen an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:\n\n\n\n\nfor csr, zero values will not be retained\n\n\nfor row_sparse, row slices of all zeros will not be retained\n\n\n\n\nThe storage type of $cast_storage$ output depends on stype parameter:\n\n\n\n\ncast_storage(csr, 'default') = default\n\n\ncast_storage(row_sparse, 'default') = default\n\n\ncast_storage(default, 'csr') = csr\n\n\ncast_storage(default, 'row_sparse') = row_sparse\n\n\n\n\nExample::\n\n\ndense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]\n\n\n\n\nDefined in src/operator/tensor/cast_storage.cc:L69\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\nstype::{'csr', 'default', 'row_sparse'}, required\n: Output storage type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_ceil\n \n \nMethod\n.\n\n\n_sparse_ceil(data)\n\n\n\n\n_sparse_ceil is an alias of ceil.\n\n\nReturns element-wise ceiling of the input.\n\n\nThe ceil of the scalar x is the smallest integer i, such that i \n= x.\n\n\nExample::\n\n\nceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]\n\n\nThe storage type of $ceil$ output depends upon the input storage type:\n\n\n\n\nceil(default) = default\n\n\nceil(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L464\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_clip\n \n \nMethod\n.\n\n\n_sparse_clip(data, a_min, a_max)\n\n\n\n\n_sparse_clip is an alias of clip.\n\n\nClips (limits) the values in an array.\n\n\nGiven an interval, values outside the interval are clipped to the interval edges. Clipping $x$ between \na_min\n and \na_x\n would be::\n\n\nclip(x, a_min, a_max) = max(min(x, a_max), a_min))\n\n\nExample::\n\n\nx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nclip(x,1,8) = [ 1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  8.]\n\n\n\n\nThe storage type of $clip$ output depends on storage types of inputs and the a_min, a_max \nparameter values:\n\n\n\n\nclip(default) = default\n\n\nclip(row_sparse, a_min \n= 0, a_max \n= 0) = row_sparse\n\n\nclip(csr, a_min \n= 0, a_max \n= 0) = csr\n\n\nclip(row_sparse, a_min \n 0, a_max \n 0) = default\n\n\nclip(row_sparse, a_min \n 0, a_max \n 0) = default\n\n\nclip(csr, a_min \n 0, a_max \n 0) = csr\n\n\nclip(csr, a_min \n 0, a_max \n 0) = csr\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L486\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\na_min::float, required\n: Minimum value\n\n\na_max::float, required\n: Maximum value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_cos\n \n \nMethod\n.\n\n\n_sparse_cos(data)\n\n\n\n\n_sparse_cos is an alias of cos.\n\n\nComputes the element-wise cosine of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    cos([0, \\pi/4, \\pi/2]) = [1, 0.707, 0]\n\n\nThe storage type of $cos$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L63\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_cosh\n \n \nMethod\n.\n\n\n_sparse_cosh(data)\n\n\n\n\n_sparse_cosh is an alias of cosh.\n\n\nReturns the hyperbolic cosine  of the input array, computed element-wise.\n\n\n.. math::    cosh(x) = 0.5\\times(exp(x) + exp(-x))\n\n\nThe storage type of $cosh$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L216\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_degrees\n \n \nMethod\n.\n\n\n_sparse_degrees(data)\n\n\n\n\n_sparse_degrees is an alias of degrees.\n\n\nConverts each element of the input array from radians to degrees.\n\n\n.. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]\n\n\nThe storage type of $degrees$ output depends upon the input storage type:\n\n\n\n\ndegrees(default) = default\n\n\ndegrees(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L163\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_dot\n \n \nMethod\n.\n\n\n_sparse_dot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\n_sparse_dot is an alias of dot.\n\n\nDot product of two arrays.\n\n\n$dot$'s behavior depends on the input array dimensions:\n\n\n\n\n1-D arrays: inner product of vectors\n\n\n2-D arrays: matrix multiplication\n\n\n\n\nN-D arrays: a sum product over the last axis of the first input and the first axis of the second input\n\n\nFor example, given 3-D $x$ with shape \n(n,m,k)\n and $y$ with shape \n(k,r,s)\n, the result array will have shape \n(n,m,r,s)\n. It is computed by::\n\n\ndot(x,y)[i,j,a,b] = sum(x[i,j,:]*y[:,a,b])\n\n\nExample::\n\n\nx = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))   y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))   dot(x,y)[0,0,1,1] = 0   sum(x[0,0,:]*y[:,1,1]) = 0\n\n\n\n\n\n\nThe storage type of $dot$ output depends on storage types of inputs and transpose options:\n\n\n\n\ndot(csr, default) = default\n\n\ndot(csr.T, default) = row_sparse\n\n\ndot(csr, row_sparse) = default\n\n\ndot(default, csr) = csr\n\n\notherwise, $dot$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/dot.cc:L62\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: The first input\n\n\nrhs::NDArray-or-SymbolicNode\n: The second input\n\n\ntranspose_a::boolean, optional, default=0\n: If true then transpose the first input before dot.\n\n\ntranspose_b::boolean, optional, default=0\n: If true then transpose the second input before dot.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_add\n \n \nMethod\n.\n\n\n_sparse_elemwise_add(lhs, rhs)\n\n\n\n\n_sparse_elemwise_add is an alias of elemwise_add.\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_div\n \n \nMethod\n.\n\n\n_sparse_elemwise_div(lhs, rhs)\n\n\n\n\n_sparse_elemwise_div is an alias of elemwise_div.\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_mul\n \n \nMethod\n.\n\n\n_sparse_elemwise_mul(lhs, rhs)\n\n\n\n\n_sparse_elemwise_mul is an alias of elemwise_mul.\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_sub\n \n \nMethod\n.\n\n\n_sparse_elemwise_sub(lhs, rhs)\n\n\n\n\n_sparse_elemwise_sub is an alias of elemwise_sub.\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_exp\n \n \nMethod\n.\n\n\n_sparse_exp(data)\n\n\n\n\n_sparse_exp is an alias of exp.\n\n\nReturns element-wise exponential value of the input.\n\n\n.. math::    exp(x) = e^x \\approx 2.718^x\n\n\nExample::\n\n\nexp([0, 1, 2]) = [1., 2.71828175, 7.38905621]\n\n\nThe storage type of $exp$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L642\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_expm1\n \n \nMethod\n.\n\n\n_sparse_expm1(data)\n\n\n\n\n_sparse_expm1 is an alias of expm1.\n\n\nReturns $exp(x) - 1$ computed element-wise on the input.\n\n\nThis function provides greater precision than $exp(x) - 1$ for small values of $x$.\n\n\nThe storage type of $expm1$ output depends upon the input storage type:\n\n\n\n\nexpm1(default) = default\n\n\nexpm1(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L721\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_fix\n \n \nMethod\n.\n\n\n_sparse_fix(data)\n\n\n\n\n_sparse_fix is an alias of fix.\n\n\nReturns element-wise rounded value to the nearest \ninteger towards zero of the input.\n\n\nExample::\n\n\nfix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]\n\n\nThe storage type of $fix$ output depends upon the input storage type:\n\n\n\n\nfix(default) = default\n\n\nfix(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L521\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_floor\n \n \nMethod\n.\n\n\n_sparse_floor(data)\n\n\n\n\n_sparse_floor is an alias of floor.\n\n\nReturns element-wise floor of the input.\n\n\nThe floor of the scalar x is the largest integer i, such that i \n= x.\n\n\nExample::\n\n\nfloor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]\n\n\nThe storage type of $floor$ output depends upon the input storage type:\n\n\n\n\nfloor(default) = default\n\n\nfloor(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L483\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_ftrl_update\n \n \nMethod\n.\n\n\n_sparse_ftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_ftrl_update is an alias of ftrl_update.\n\n\nUpdate function for Ftrl optimizer. Referenced from \nAd Click Prediction: a View from the Trenches\n, available at http://dl.acm.org/citation.cfm?id=2488200.\n\n\nIt updates the weights using::\n\n\nrescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad\n2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad\n2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z) \n lamda1)\n\n\nIf w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::\n\n\nfor row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row]\n2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row]\n2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row]) \n lamda1)\n\n\nDefined in src/operator/optimizer_op.cc:L341\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nz::NDArray-or-SymbolicNode\n: z\n\n\nn::NDArray-or-SymbolicNode\n: Square of grad\n\n\nlr::float, required\n: Learning rate\n\n\nlamda1::float, optional, default=0.01\n: The L1 regularization coefficient.\n\n\nbeta::float, optional, default=1\n: Per-Coordinate Learning Rate beta.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_gamma\n \n \nMethod\n.\n\n\n_sparse_gamma(data)\n\n\n\n\n_sparse_gamma is an alias of gamma.\n\n\nReturns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.\n\n\nThe storage type of $gamma$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_gammaln\n \n \nMethod\n.\n\n\n_sparse_gammaln(data)\n\n\n\n\n_sparse_gammaln is an alias of gammaln.\n\n\nReturns element-wise log of the absolute value of the gamma function \nof the input.\n\n\nThe storage type of $gammaln$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log\n \n \nMethod\n.\n\n\n_sparse_log(data)\n\n\n\n\n_sparse_log is an alias of log.\n\n\nReturns element-wise Natural logarithmic value of the input.\n\n\nThe natural logarithm is logarithm in base \ne\n, so that $log(exp(x)) = x$\n\n\nThe storage type of $log$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L654\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log10\n \n \nMethod\n.\n\n\n_sparse_log10(data)\n\n\n\n\n_sparse_log10 is an alias of log10.\n\n\nReturns element-wise Base-10 logarithmic value of the input.\n\n\n$10**log10(x) = x$\n\n\nThe storage type of $log10$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L666\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log1p\n \n \nMethod\n.\n\n\n_sparse_log1p(data)\n\n\n\n\n_sparse_log1p is an alias of log1p.\n\n\nReturns element-wise $log(1 + x)$ value of the input.\n\n\nThis function is more accurate than $log(1 + x)$  for small $x$ so that :math:\n1+x\\approx 1\n\n\nThe storage type of $log1p$ output depends upon the input storage type:\n\n\n\n\nlog1p(default) = default\n\n\nlog1p(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L703\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log2\n \n \nMethod\n.\n\n\n_sparse_log2(data)\n\n\n\n\n_sparse_log2 is an alias of log2.\n\n\nReturns element-wise Base-2 logarithmic value of the input.\n\n\n$2**log2(x) = x$\n\n\nThe storage type of $log2$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L678\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_make_loss\n \n \nMethod\n.\n\n\n_sparse_make_loss(data)\n\n\n\n\n_sparse_make_loss is an alias of make_loss.\n\n\nMake your own loss function in network construction.\n\n\nThis operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.\n\n\nFor example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::\n\n\ncross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)\n\n\nWe will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.\n\n\nThe storage type of $make_loss$ output depends upon the input storage type:\n\n\n\n\nmake_loss(default) = default\n\n\nmake_loss(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L200\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_mean\n \n \nMethod\n.\n\n\n_sparse_mean(data, axis, keepdims, exclude)\n\n\n\n\n_sparse_mean is an alias of mean.\n\n\nComputes the mean of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L101\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_negative\n \n \nMethod\n.\n\n\n_sparse_negative(data)\n\n\n\n\n_sparse_negative is an alias of negative.\n\n\nNumerical negative of the argument, element-wise.\n\n\nThe storage type of $negative$ output depends upon the input storage type:\n\n\n\n\nnegative(default) = default\n\n\nnegative(row_sparse) = row_sparse\n\n\nnegative(csr) = csr\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_radians\n \n \nMethod\n.\n\n\n_sparse_radians(data)\n\n\n\n\n_sparse_radians is an alias of radians.\n\n\nConverts each element of the input array from degrees to radians.\n\n\n.. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]\n\n\nThe storage type of $radians$ output depends upon the input storage type:\n\n\n\n\nradians(default) = default\n\n\nradians(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L182\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_relu\n \n \nMethod\n.\n\n\n_sparse_relu(data)\n\n\n\n\n_sparse_relu is an alias of relu.\n\n\nComputes rectified linear.\n\n\n.. math::    max(features, 0)\n\n\nThe storage type of $relu$ output depends upon the input storage type:\n\n\n\n\nrelu(default) = default\n\n\nrelu(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L84\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_retain\n \n \nMethod\n.\n\n\n_sparse_retain(data, indices)\n\n\n\n\npick rows specified by user input index array from a row sparse matrix and save them in the output sparse matrix.\n\n\nExample::\n\n\ndata = [[1, 2], [3, 4], [5, 6]]   indices = [0, 1, 3]   shape = (4, 2)   rsp_in = row_sparse(data, indices)   to_retain = [0, 3]   rsp_out = retain(rsp_in, to_retain)   rsp_out.values = [[1, 2], [5, 6]]   rsp_out.indices = [0, 3]\n\n\nThe storage type of $retain$ output depends on storage types of inputs\n\n\n\n\nretain(row_sparse, default) = row_sparse\n\n\notherwise, $retain$ is not supported\n\n\n\n\nDefined in src/operator/tensor/sparse_retain.cc:L53\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array for sparse_retain operator.\n\n\nindices::NDArray-or-SymbolicNode\n: The index array of rows ids that will be retained.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_rint\n \n \nMethod\n.\n\n\n_sparse_rint(data)\n\n\n\n\n_sparse_rint is an alias of rint.\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\n.. note::\n\n\n\n\nFor input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.\n\n\nFor input $-n.5$ both $rint$ and $round$ returns $-n-1$.\n\n\n\n\nExample::\n\n\nrint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]\n\n\nThe storage type of $rint$ output depends upon the input storage type:\n\n\n\n\nrint(default) = default\n\n\nrint(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L445\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_round\n \n \nMethod\n.\n\n\n_sparse_round(data)\n\n\n\n\n_sparse_round is an alias of round.\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\nExample::\n\n\nround([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]\n\n\nThe storage type of $round$ output depends upon the input storage type:\n\n\n\n\nround(default) = default\n\n\nround(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L424\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_rsqrt\n \n \nMethod\n.\n\n\n_sparse_rsqrt(data)\n\n\n\n\n_sparse_rsqrt is an alias of rsqrt.\n\n\nReturns element-wise inverse square-root value of the input.\n\n\n.. math::    rsqrt(x) = 1/\\sqrt{x}\n\n\nExample::\n\n\nrsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]\n\n\nThe storage type of $rsqrt$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L585\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sgd_mom_update\n \n \nMethod\n.\n\n\n_sparse_sgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_sgd_mom_update is an alias of sgd_mom_update.\n\n\nMomentum update function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nMomentum update has better convergence rates on neural networks. Mathematically it looks like below:\n\n\n.. math::\n\n\nv_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t\n\n\nIt updates the weights using::\n\n\nv = momentum * v - learning_rate * gradient   weight += v\n\n\nWhere the parameter $momentum$ is the decay rate of momentum estimates at each epoch.\n\n\nIf weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::\n\n\nfor row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]\n\n\nDefined in src/operator/optimizer_op.cc:L94\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmom::NDArray-or-SymbolicNode\n: Momentum\n\n\nlr::float, required\n: Learning rate\n\n\nmomentum::float, optional, default=0\n: The decay rate of momentum estimates at each epoch.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sgd_update\n \n \nMethod\n.\n\n\n_sparse_sgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_sgd_update is an alias of sgd_update.\n\n\nUpdate function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nIt updates the weights using::\n\n\nweight = weight - learning_rate * gradient\n\n\nIf weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::\n\n\nfor row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]\n\n\nDefined in src/operator/optimizer_op.cc:L54\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nlr::float, required\n: Learning rate\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sigmoid\n \n \nMethod\n.\n\n\n_sparse_sigmoid(data)\n\n\n\n\n_sparse_sigmoid is an alias of sigmoid.\n\n\nComputes sigmoid of x element-wise.\n\n\n.. math::    y = 1 / (1 + exp(-x))\n\n\nThe storage type of $sigmoid$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L103\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sign\n \n \nMethod\n.\n\n\n_sparse_sign(data)\n\n\n\n\n_sparse_sign is an alias of sign.\n\n\nReturns element-wise sign of the input.\n\n\nExample::\n\n\nsign([-2, 0, 3]) = [-1, 0, 1]\n\n\nThe storage type of $sign$ output depends upon the input storage type:\n\n\n\n\nsign(default) = default\n\n\nsign(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L405\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sin\n \n \nMethod\n.\n\n\n_sparse_sin(data)\n\n\n\n\n_sparse_sin is an alias of sin.\n\n\nComputes the element-wise sine of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    sin([0, \\pi/4, \\pi/2]) = [0, 0.707, 1]\n\n\nThe storage type of $sin$ output depends upon the input storage type:\n\n\n\n\nsin(default) = default\n\n\nsin(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L46\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sinh\n \n \nMethod\n.\n\n\n_sparse_sinh(data)\n\n\n\n\n_sparse_sinh is an alias of sinh.\n\n\nReturns the hyperbolic sine of the input array, computed element-wise.\n\n\n.. math::    sinh(x) = 0.5\\times(exp(x) - exp(-x))\n\n\nThe storage type of $sinh$ output depends upon the input storage type:\n\n\n\n\nsinh(default) = default\n\n\nsinh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L201\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_slice\n \n \nMethod\n.\n\n\n_sparse_slice(data, begin, end, step)\n\n\n\n\n_sparse_slice is an alias of slice.\n\n\nSlices a region of the array.\n\n\n.. note:: $crop$ is deprecated. Use $slice$ instead.\n\n\nThis function returns a sliced array between the indices given by \nbegin\n and \nend\n with the corresponding \nstep\n.\n\n\nFor an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m \n= n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.\n\n\nThe resulting array's \nk\n-th dimension contains elements from the \nk\n-th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).\n\n\nIf the \nk\n-th elements are \nNone\n in the sequence of \nbegin\n, \nend\n, and \nstep\n, the following rule will be used to set default values. If \ns_k\n is \nNone\n, set \ns_k=1\n. If \ns_k \n 0\n, set \nb_k=0\n, \ne_k=d_k\n; else, set \nb_k=d_k-1\n, \ne_k=-1\n.\n\n\nThe storage type of $slice$ output depends on storage types of inputs\n\n\n\n\nslice(csr) = csr\n\n\notherwise, $slice$ generates output with default storage\n\n\n\n\n.. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.\n\n\nExample::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L355\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sqrt\n \n \nMethod\n.\n\n\n_sparse_sqrt(data)\n\n\n\n\n_sparse_sqrt is an alias of sqrt.\n\n\nReturns element-wise square-root value of the input.\n\n\n.. math::    \\textrm{sqrt}(x) = \\sqrt{x}\n\n\nExample::\n\n\nsqrt([4, 9, 16]) = [2, 3, 4]\n\n\nThe storage type of $sqrt$ output depends upon the input storage type:\n\n\n\n\nsqrt(default) = default\n\n\nsqrt(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L565\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_square\n \n \nMethod\n.\n\n\n_sparse_square(data)\n\n\n\n\n_sparse_square is an alias of square.\n\n\nReturns element-wise squared value of the input.\n\n\n.. math::    square(x) = x^2\n\n\nExample::\n\n\nsquare([2, 3, 4]) = [4, 9, 16]\n\n\nThe storage type of $square$ output depends upon the input storage type:\n\n\n\n\nsquare(default) = default\n\n\nsquare(row_sparse) = row_sparse\n\n\nsquare(csr) = csr\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L542\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_stop_gradient\n \n \nMethod\n.\n\n\n_sparse_stop_gradient(data)\n\n\n\n\n_sparse_stop_gradient is an alias of BlockGrad.\n\n\nStops gradient computation.\n\n\nStops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.\n\n\nExample::\n\n\nv1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)\n\n\nexecutor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]\n\n\nexecutor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L167\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sum\n \n \nMethod\n.\n\n\n_sparse_sum(data, axis, keepdims, exclude)\n\n\n\n\n_sparse_sum is an alias of sum.\n\n\nComputes the sum of array elements over given axes.\n\n\n.. Note::\n\n\nsum\n and \nsum_axis\n are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.\n\n\nExample::\n\n\ndata = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]\n\n\nsum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]\n\n\nsum(data, axis=[1,2])   [ 12.  19.  27.]\n\n\ndata = [[1,2,0],           [3,0,1],           [4,1,0]]\n\n\ncsr = cast_storage(data, 'csr')\n\n\nsum(csr, axis=0)   [ 8.  3.  1.]\n\n\nsum(csr, axis=1)   [ 3.  4.  5.]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L85\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_tan\n \n \nMethod\n.\n\n\n_sparse_tan(data)\n\n\n\n\n_sparse_tan is an alias of tan.\n\n\nComputes the element-wise tangent of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    tan([0, \\pi/4, \\pi/2]) = [0, 1, -inf]\n\n\nThe storage type of $tan$ output depends upon the input storage type:\n\n\n\n\ntan(default) = default\n\n\ntan(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L83\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_tanh\n \n \nMethod\n.\n\n\n_sparse_tanh(data)\n\n\n\n\n_sparse_tanh is an alias of tanh.\n\n\nReturns the hyperbolic tangent of the input array, computed element-wise.\n\n\n.. math::    tanh(x) = sinh(x) / cosh(x)\n\n\nThe storage type of $tanh$ output depends upon the input storage type:\n\n\n\n\ntanh(default) = default\n\n\ntanh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L234\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_trunc\n \n \nMethod\n.\n\n\n_sparse_trunc(data)\n\n\n\n\n_sparse_trunc is an alias of trunc.\n\n\nReturn the element-wise truncated value of the input.\n\n\nThe truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.\n\n\nExample::\n\n\ntrunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]\n\n\nThe storage type of $trunc$ output depends upon the input storage type:\n\n\n\n\ntrunc(default) = default\n\n\ntrunc(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L503\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_zeros_like\n \n \nMethod\n.\n\n\n_sparse_zeros_like(data)\n\n\n\n\n_sparse_zeros_like is an alias of zeros_like.\n\n\nReturn an array of zeros with the same shape and type as the input array.\n\n\nThe storage type of $zeros_like$ output depends on the storage type of the input\n\n\n\n\nzeros_like(row_sparse) = row_sparse\n\n\nzeros_like(csr) = csr\n\n\nzeros_like(default) = default\n\n\n\n\nExamples::\n\n\nx = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]\n\n\nzeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._square_sum\n \n \nMethod\n.\n\n\n_square_sum(data, axis, keepdims, exclude)\n\n\n\n\nComputes the square sum of array elements over a given axis for row-sparse matrix. This is a temporary solution for fusing ops square and sum together for row-sparse matrix to save memory for storing gradients. It will become deprecated once the functionality of fusing operators is finished in the future.\n\n\nExample::\n\n\ndns = mx.nd.array([[0, 0], [1, 2], [0, 0], [3, 4], [0, 0]])   rsp = dns.tostype('row_sparse')   sum = mx.nd._internal._square_sum(rsp, axis=1)   sum = [0, 5, 0, 25, 0]\n\n\nDefined in src/operator/tensor/square_sum.cc:L63\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.adam_update\n \n \nMethod\n.\n\n\nadam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdate function for Adam optimizer. Adam is seen as a generalization of AdaGrad.\n\n\nAdam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).\n\n\n.. math::\n\n\ng_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }\n\n\nIt updates the weights using::\n\n\nm = beta1\nm + (1-beta1)\ngrad  v = beta2\nv + (1-beta2)\n(grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)\n\n\nIf w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::\n\n\nfor row in grad.indices:      m[row] = beta1\nm[row] + (1-beta1)\ngrad[row]      v[row] = beta2\nv[row] + (1-beta2)\n(grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)\n\n\nDefined in src/operator/optimizer_op.cc:L208\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmean::NDArray-or-SymbolicNode\n: Moving mean\n\n\nvar::NDArray-or-SymbolicNode\n: Moving variance\n\n\nlr::float, required\n: Learning rate\n\n\nbeta1::float, optional, default=0.9\n: The decay rate for the 1st moment estimates.\n\n\nbeta2::float, optional, default=0.999\n: The decay rate for the 2nd moment estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.add_n\n \n \nMethod\n.\n\n\nadd_n(args)\n\n\n\n\nNote\n: add_n takes variable number of positional inputs. So instead of calling as add_n([x, y, z], num_args=3), one should call via add_n(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.add_to!\n \n \nMethod\n.\n\n\nadd_to!(dst::NDArray, args::NDArrayOrReal...)\n\n\n\n\nAdd a bunch of arguments into \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax\n \n \nMethod\n.\n\n\nargmax(data, axis, keepdims)\n\n\n\n\nReturns indices of the maximum values along an axis.\n\n\nIn the case of multiple occurrences of maximum values, the indices corresponding to the first occurrence are returned.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]\n\n\n// argmax along axis 0   argmax(x, axis=0) = [ 1.,  1.,  1.]\n\n\n// argmax along axis 1   argmax(x, axis=1) = [ 2.,  2.]\n\n\n// argmax along axis 1 keeping same dims as an input array   argmax(x, axis=1, keepdims=True) = [[ 2.],                                       [ 2.]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L52\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::int or None, optional, default='None'\n: The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$\n\n\nkeepdims::boolean, optional, default=0\n: If this is set to \nTrue\n, the reduced axis is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax_channel\n \n \nMethod\n.\n\n\nargmax_channel(data)\n\n\n\n\nReturns argmax indices of each channel from the input array.\n\n\nThe result will be an NDArray of shape (num_channel,).\n\n\nIn case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]\n\n\nargmax_channel(x) = [ 2.,  2.]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L97\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmin\n \n \nMethod\n.\n\n\nargmin(data, axis, keepdims)\n\n\n\n\nReturns indices of the minimum values along an axis.\n\n\nIn the case of multiple occurrences of minimum values, the indices corresponding to the first occurrence are returned.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]\n\n\n// argmin along axis 0   argmin(x, axis=0) = [ 0.,  0.,  0.]\n\n\n// argmin along axis 1   argmin(x, axis=1) = [ 0.,  0.]\n\n\n// argmin along axis 1 keeping same dims as an input array   argmin(x, axis=1, keepdims=True) = [[ 0.],                                       [ 0.]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L77\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::int or None, optional, default='None'\n: The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$\n\n\nkeepdims::boolean, optional, default=0\n: If this is set to \nTrue\n, the reduced axis is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argsort\n \n \nMethod\n.\n\n\nargsort(data, axis, is_ascend)\n\n\n\n\nReturns the indices that would sort an input array along the given axis.\n\n\nThis function performs sorting along the given axis and returns an array of indices having same shape as an input array that index data in sorted order.\n\n\nExamples::\n\n\nx = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]\n\n\n// sort along axis -1   argsort(x) = [[ 1.,  0.,  2.],                 [ 0.,  2.,  1.]]\n\n\n// sort along axis 0   argsort(x, axis=0) = [[ 1.,  0.,  1.]                         [ 0.,  1.,  0.]]\n\n\n// flatten and then sort   argsort(x) = [ 3.,  1.,  5.,  0.,  4.,  2.]\n\n\nDefined in src/operator/tensor/ordering_op.cc:L176\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=1\n: Whether to sort in ascending or descending order.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_dot\n \n \nMethod\n.\n\n\nbatch_dot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\nBatchwise dot product.\n\n\n$batch_dot$ is used to compute dot product of $x$ and $y$ when $x$ and $y$ are data in batch, namely 3D arrays in shape of \n(batch_size, :, :)\n.\n\n\nFor example, given $x$ with shape \n(batch_size, n, m)\n and $y$ with shape \n(batch_size, m, k)\n, the result array will have shape \n(batch_size, n, k)\n, which is computed by::\n\n\nbatch_dot(x,y)[i,:,:] = dot(x[i,:,:], y[i,:,:])\n\n\nDefined in src/operator/tensor/dot.cc:L110\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: The first input\n\n\nrhs::NDArray-or-SymbolicNode\n: The second input\n\n\ntranspose_a::boolean, optional, default=0\n: If true then transpose the first input before dot.\n\n\ntranspose_b::boolean, optional, default=0\n: If true then transpose the second input before dot.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_take\n \n \nMethod\n.\n\n\nbatch_take(a, indices)\n\n\n\n\nTakes elements from a data batch.\n\n\n.. note::   \nbatch_take\n is deprecated. Use \npick\n instead.\n\n\nGiven an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::\n\n\noutput[i] = input[i, indices[i]]\n\n\nExamples::\n\n\nx = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]\n\n\n// takes elements with specified indices   batch_take(x, [0,1,0]) = [ 1.  4.  5.]\n\n\nDefined in src/operator/tensor/indexing_op.cc:L382\n\n\nArguments\n\n\n\n\na::NDArray-or-SymbolicNode\n: The input array\n\n\nindices::NDArray-or-SymbolicNode\n: The index array\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_add\n \n \nMethod\n.\n\n\nbroadcast_add(lhs, rhs)\n\n\n\n\nReturns element-wise sum of the input arrays with broadcasting.\n\n\nbroadcast_plus\n is an alias to the function \nbroadcast_add\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]\n\n\nbroadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axes\n \n \nMethod\n.\n\n\nbroadcast_axes(data, axis, size)\n\n\n\n\nbroadcast_axes is an alias of broadcast_axis.\n\n\nBroadcasts the input array over particular axes.\n\n\nBroadcasting is allowed on axes with size 1, such as from \n(2,1,3,1)\n to \n(2,8,3,9)\n. Elements will be duplicated on the broadcasted axes.\n\n\nExample::\n\n\n// given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]\n\n\n// broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L207\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::Shape(tuple), optional, default=[]\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=[]\n: Target sizes of the broadcasting axes.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axis\n \n \nMethod\n.\n\n\nbroadcast_axis(data, axis, size)\n\n\n\n\nBroadcasts the input array over particular axes.\n\n\nBroadcasting is allowed on axes with size 1, such as from \n(2,1,3,1)\n to \n(2,8,3,9)\n. Elements will be duplicated on the broadcasted axes.\n\n\nExample::\n\n\n// given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]\n\n\n// broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L207\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::Shape(tuple), optional, default=[]\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=[]\n: Target sizes of the broadcasting axes.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_div\n \n \nMethod\n.\n\n\nbroadcast_div(lhs, rhs)\n\n\n\n\nReturns element-wise division of the input arrays with broadcasting.\n\n\nExample::\n\n\nx = [[ 6.,  6.,  6.],         [ 6.,  6.,  6.]]\n\n\ny = [[ 2.],         [ 3.]]\n\n\nbroadcast_div(x, y) = [[ 3.,  3.,  3.],                           [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L157\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_equal\n \n \nMethod\n.\n\n\nbroadcast_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nequal to\n (==) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_equal(x, y) = [[ 0.,  0.,  0.],                             [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L46\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater\n \n \nMethod\n.\n\n\nbroadcast_greater(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \ngreater than\n (\n) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_greater(x, y) = [[ 1.,  1.,  1.],                               [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L82\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater_equal\n \n \nMethod\n.\n\n\nbroadcast_greater_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \ngreater than or equal to\n (\n=) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_greater_equal(x, y) = [[ 1.,  1.,  1.],                                     [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L100\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_hypot\n \n \nMethod\n.\n\n\nbroadcast_hypot(lhs, rhs)\n\n\n\n\nReturns the hypotenuse of a right angled triangle, given its \"legs\" with broadcasting.\n\n\nIt is equivalent to doing :math:\nsqrt(x_1^2 + x_2^2)\n.\n\n\nExample::\n\n\nx = [[ 3.,  3.,  3.]]\n\n\ny = [[ 4.],         [ 4.]]\n\n\nbroadcast_hypot(x, y) = [[ 5.,  5.,  5.],                             [ 5.,  5.,  5.]]\n\n\nz = [[ 0.],         [ 4.]]\n\n\nbroadcast_hypot(x, z) = [[ 3.,  3.,  3.],                             [ 5.,  5.,  5.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L156\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser\n \n \nMethod\n.\n\n\nbroadcast_lesser(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nlesser than\n (\n) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_lesser(x, y) = [[ 0.,  0.,  0.],                              [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L118\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser_equal\n \n \nMethod\n.\n\n\nbroadcast_lesser_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nlesser than or equal to\n (\n=) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_lesser_equal(x, y) = [[ 0.,  0.,  0.],                                    [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L136\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_maximum\n \n \nMethod\n.\n\n\nbroadcast_maximum(lhs, rhs)\n\n\n\n\nReturns element-wise maximum of the input arrays with broadcasting.\n\n\nThis function compares two input arrays and returns a new array having the element-wise maxima.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_maximum(x, y) = [[ 1.,  1.,  1.],                               [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L80\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minimum\n \n \nMethod\n.\n\n\nbroadcast_minimum(lhs, rhs)\n\n\n\n\nReturns element-wise minimum of the input arrays with broadcasting.\n\n\nThis function compares two input arrays and returns a new array having the element-wise minima.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_maximum(x, y) = [[ 0.,  0.,  0.],                               [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L115\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minus\n \n \nMethod\n.\n\n\nbroadcast_minus(lhs, rhs)\n\n\n\n\nbroadcast_minus is an alias of broadcast_sub.\n\n\nReturns element-wise difference of the input arrays with broadcasting.\n\n\nbroadcast_minus\n is an alias to the function \nbroadcast_sub\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]\n\n\nbroadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mod\n \n \nMethod\n.\n\n\nbroadcast_mod(lhs, rhs)\n\n\n\n\nReturns element-wise modulo of the input arrays with broadcasting.\n\n\nExample::\n\n\nx = [[ 8.,  8.,  8.],         [ 8.,  8.,  8.]]\n\n\ny = [[ 2.],         [ 3.]]\n\n\nbroadcast_mod(x, y) = [[ 0.,  0.,  0.],                           [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L190\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mul\n \n \nMethod\n.\n\n\nbroadcast_mul(lhs, rhs)\n\n\n\n\nReturns element-wise product of the input arrays with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_mul(x, y) = [[ 0.,  0.,  0.],                           [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L123\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_not_equal\n \n \nMethod\n.\n\n\nbroadcast_not_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nnot equal to\n (!=) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_not_equal(x, y) = [[ 1.,  1.,  1.],                                 [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L64\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_plus\n \n \nMethod\n.\n\n\nbroadcast_plus(lhs, rhs)\n\n\n\n\nbroadcast_plus is an alias of broadcast_add.\n\n\nReturns element-wise sum of the input arrays with broadcasting.\n\n\nbroadcast_plus\n is an alias to the function \nbroadcast_add\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]\n\n\nbroadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_power\n \n \nMethod\n.\n\n\nbroadcast_power(lhs, rhs)\n\n\n\n\nReturns result of first array elements raised to powers from second array, element-wise with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_power(x, y) = [[ 2.,  2.,  2.],                             [ 4.,  4.,  4.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L45\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_sub\n \n \nMethod\n.\n\n\nbroadcast_sub(lhs, rhs)\n\n\n\n\nReturns element-wise difference of the input arrays with broadcasting.\n\n\nbroadcast_minus\n is an alias to the function \nbroadcast_sub\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]\n\n\nbroadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_to\n \n \nMethod\n.\n\n\nbroadcast_to(data, shape)\n\n\n\n\nBroadcasts the input array to a new shape.\n\n\nBroadcasting is a mechanism that allows NDArrays to perform arithmetic operations with arrays of different shapes efficiently without creating multiple copies of arrays. Also see, \nBroadcasting \nhttps://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n_ for more explanation.\n\n\nBroadcasting is allowed on axes with size 1, such as from \n(2,1,3,1)\n to \n(2,8,3,9)\n. Elements will be duplicated on the broadcasted axes.\n\n\nFor example::\n\n\nbroadcast_to([[1,2,3]], shape=(2,3)) = [[ 1.,  2.,  3.],                                            [ 1.,  2.,  3.]])\n\n\nThe dimension which you do not want to change can also be kept as \n0\n which means copy the original value. So with \nshape=(2,0)\n, we will obtain the same result as in the above example.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L231\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nshape::Shape(tuple), optional, default=[]\n: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g \nA = broadcast_to(B, shape=(10, 0, 0))\n has the same meaning as \nA = broadcast_axis(B, axis=0, size=10)\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cast\n \n \nMethod\n.\n\n\ncast(data, dtype)\n\n\n\n\ncast is an alias of Cast.\n\n\nCasts all elements of the input to a new type.\n\n\n.. note:: $Cast$ is deprecated. Use $cast$ instead.\n\n\nExample::\n\n\ncast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L311\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Output data type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cast_storage\n \n \nMethod\n.\n\n\ncast_storage(data, stype)\n\n\n\n\nCasts tensor storage type to the new type.\n\n\nWhen an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:\n\n\n\n\nfor csr, zero values will not be retained\n\n\nfor row_sparse, row slices of all zeros will not be retained\n\n\n\n\nThe storage type of $cast_storage$ output depends on stype parameter:\n\n\n\n\ncast_storage(csr, 'default') = default\n\n\ncast_storage(row_sparse, 'default') = default\n\n\ncast_storage(default, 'csr') = csr\n\n\ncast_storage(default, 'row_sparse') = row_sparse\n\n\n\n\nExample::\n\n\ndense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]\n\n\n\n\nDefined in src/operator/tensor/cast_storage.cc:L69\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\nstype::{'csr', 'default', 'row_sparse'}, required\n: Output storage type.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.choose_element_0index\n \n \nMethod\n.\n\n\nchoose_element_0index(lhs, rhs)\n\n\n\n\nChoose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.concat\n \n \nMethod\n.\n\n\nconcat(data, num_args, dim)\n\n\n\n\nconcat is an alias of Concat.\n\n\nNote\n: concat takes variable number of positional inputs. So instead of calling as concat([x, y, z], num_args=3), one should call via concat(x, y, z), and num_args will be determined automatically.\n\n\nJoins input arrays along a given axis.\n\n\n.. note:: \nConcat\n is deprecated. Use \nconcat\n instead.\n\n\nThe dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.\n\n\nExample::\n\n\nx = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]\n\n\nconcat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]\n\n\nNote that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.\n\n\nconcat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]\n\n\nDefined in src/operator/concat.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: List of arrays to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.crop\n \n \nMethod\n.\n\n\ncrop(data, begin, end, step)\n\n\n\n\ncrop is an alias of slice.\n\n\nSlices a region of the array.\n\n\n.. note:: $crop$ is deprecated. Use $slice$ instead.\n\n\nThis function returns a sliced array between the indices given by \nbegin\n and \nend\n with the corresponding \nstep\n.\n\n\nFor an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m \n= n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.\n\n\nThe resulting array's \nk\n-th dimension contains elements from the \nk\n-th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).\n\n\nIf the \nk\n-th elements are \nNone\n in the sequence of \nbegin\n, \nend\n, and \nstep\n, the following rule will be used to set default values. If \ns_k\n is \nNone\n, set \ns_k=1\n. If \ns_k \n 0\n, set \nb_k=0\n, \ne_k=d_k\n; else, set \nb_k=d_k-1\n, \ne_k=-1\n.\n\n\nThe storage type of $slice$ output depends on storage types of inputs\n\n\n\n\nslice(csr) = csr\n\n\notherwise, $slice$ generates output with default storage\n\n\n\n\n.. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.\n\n\nExample::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L355\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.degrees\n \n \nMethod\n.\n\n\ndegrees(data)\n\n\n\n\nConverts each element of the input array from radians to degrees.\n\n\n.. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]\n\n\nThe storage type of $degrees$ output depends upon the input storage type:\n\n\n\n\ndegrees(default) = default\n\n\ndegrees(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L163\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.div_from!\n \n \nMethod\n.\n\n\ndiv_from!(dst::NDArray, arg::NDArrayOrReal)\n\n\n\n\nElementwise divide a scalar or an \nNDArray\n of the same shape from \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_add\n \n \nMethod\n.\n\n\nelemwise_add(lhs, rhs)\n\n\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_div\n \n \nMethod\n.\n\n\nelemwise_div(lhs, rhs)\n\n\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_mul\n \n \nMethod\n.\n\n\nelemwise_mul(lhs, rhs)\n\n\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_sub\n \n \nMethod\n.\n\n\nelemwise_sub(lhs, rhs)\n\n\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fill\n \n \nMethod\n.\n\n\nfill(x, dims, ctx=cpu())\nfill(x, dims...)\n\n\n\n\nCreate an \nNDArray\n filled with the value \nx\n, like \nBase.fill\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fill_element_0index\n \n \nMethod\n.\n\n\nfill_element_0index(lhs, mhs, rhs)\n\n\n\n\nFill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nmhs::NDArray\n: Middle operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fix\n \n \nMethod\n.\n\n\nfix(data)\n\n\n\n\nReturns element-wise rounded value to the nearest \ninteger towards zero of the input.\n\n\nExample::\n\n\nfix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]\n\n\nThe storage type of $fix$ output depends upon the input storage type:\n\n\n\n\nfix(default) = default\n\n\nfix(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L521\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flatten\n \n \nMethod\n.\n\n\nflatten(data)\n\n\n\n\nflatten is an alias of Flatten.\n\n\nFlattens the input array into a 2-D array by collapsing the higher dimensions.\n\n\n.. note:: \nFlatten\n is deprecated. Use \nflatten\n instead.\n\n\nFor an input array with shape $(d1, d2, ..., dk)$, \nflatten\n operation reshapes the input array into an output array of shape $(d1, d2\n...\ndk)$.\n\n\nExample::\n\n\nx = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L208\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flip\n \n \nMethod\n.\n\n\nflip(data, axis)\n\n\n\n\nflip is an alias of reverse.\n\n\nReverses the order of elements along given axis while preserving array shape.\n\n\nNote: reverse and flip are equivalent. We use reverse in the following examples.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]\n\n\nreverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]\n\n\nreverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L662\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\naxis::Shape(tuple), required\n: The axis which to reverse elements.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ftml_update\n \n \nMethod\n.\n\n\nftml_update(weight, grad, d, v, z, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)\n\n\n\n\nThe FTML optimizer described in \nFTML - Follow the Moving Leader in Deep Learning\n, available at http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf.\n\n\n.. math::\n\n\ng_t = \\nabla J(W_{t-1})\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n d_t = \\frac{ (1 - \\beta_1^t) }{ \\eta_t } (\\sqrt{ \\frac{ v_t }{ 1 - \\beta_2^t } } + \\epsilon)  \\sigma_t = d_t - \\beta_1 d_{t-1}  z_t = \\beta_1 z_{ t-1 } + (1 - \\beta_1^t) g_t - \\sigma_t W_{t-1}  W_t = - \\frac{ z_t }{ d_t }\n\n\nDefined in src/operator/optimizer_op.cc:L161\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nd::NDArray-or-SymbolicNode\n: Internal state $d_t$\n\n\nv::NDArray-or-SymbolicNode\n: Internal state $v_t$\n\n\nz::NDArray-or-SymbolicNode\n: Internal state $z_t$\n\n\nlr::float, required\n: Learning rate\n\n\nbeta1::float, optional, default=0.9\n: The decay rate for the 1st moment estimates.\n\n\nbeta2::float, optional, default=0.999\n: The decay rate for the 2nd moment estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ftrl_update\n \n \nMethod\n.\n\n\nftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdate function for Ftrl optimizer. Referenced from \nAd Click Prediction: a View from the Trenches\n, available at http://dl.acm.org/citation.cfm?id=2488200.\n\n\nIt updates the weights using::\n\n\nrescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad\n2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad\n2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z) \n lamda1)\n\n\nIf w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::\n\n\nfor row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row]\n2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row]\n2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row]) \n lamda1)\n\n\nDefined in src/operator/optimizer_op.cc:L341\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nz::NDArray-or-SymbolicNode\n: z\n\n\nn::NDArray-or-SymbolicNode\n: Square of grad\n\n\nlr::float, required\n: Learning rate\n\n\nlamda1::float, optional, default=0.01\n: The L1 regularization coefficient.\n\n\nbeta::float, optional, default=1\n: Per-Coordinate Learning Rate beta.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gammaln\n \n \nMethod\n.\n\n\ngammaln(data)\n\n\n\n\nReturns element-wise log of the absolute value of the gamma function \nof the input.\n\n\nThe storage type of $gammaln$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gather_nd\n \n \nMethod\n.\n\n\ngather_nd(data, indices)\n\n\n\n\nGather elements or slices from \ndata\n and store to a tensor whose shape is defined by \nindices\n. \ngather_nd\n and \nscatter_nd\n are inverse functions to each other.\n\n\nGiven \ndata\n with shape \n(X_0, X_1, ..., X_{N-1})\n and indices with shape \n(M, Y_0, ..., Y_{K-1})\n, the output will have shape \n(Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1})\n, where \nM \n= N\n. If \nM == N\n, output shape will simply be \n(Y_0, ..., Y_{K-1})\n.\n\n\nThe elements in output is defined as follows::\n\n\noutput[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}] = data[indices[0, y_0, ..., y_{K-1}],                                                       ...,                                                       indices[M-1, y_0, ..., y_{K-1}],                                                       x_M, ..., x_{N-1}]\n\n\nExamples::\n\n\ndata = [[0, 1], [2, 3]]   indices = [[1, 1, 0], [0, 1, 0]]   gather_nd(data, indices) = [2, 3, 0]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: data\n\n\nindices::NDArray-or-SymbolicNode\n: indices\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.is_shared\n \n \nMethod\n.\n\n\nis_shared(j_arr, arr)\n\n\n\n\nTest whether \nj_arr\n is sharing data with \narr\n.\n\n\nArguments:\n\n\n\n\nj_arr::Array\n: the Julia Array.\n\n\narr::NDArray\n: the \nNDArray\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.khatri_rao\n \n \nMethod\n.\n\n\nkhatri_rao(args)\n\n\n\n\nNote\n: khatri_rao takes variable number of positional inputs. So instead of calling as khatri_rao([x, y, z], num_args=3), one should call via khatri_rao(x, y, z), and num_args will be determined automatically.\n\n\nComputes the Khatri-Rao product of the input matrices.\n\n\nGiven a collection of :math:\nn\n input matrices,\n\n\n.. math::    A_1 \\in \\mathbb{R}^{M_1 \\times M}, \\ldots, A_n \\in \\mathbb{R}^{M_n \\times N},\n\n\nthe (column-wise) Khatri-Rao product is defined as the matrix,\n\n\n.. math::    X = A_1 \\otimes \\cdots \\otimes A_n \\in \\mathbb{R}^{(M_1 \\cdots M_n) \\times N},\n\n\nwhere the :math:\nk\nth column is equal to the column-wise outer product :math:\n{A_1}_k \\otimes \\cdots \\otimes {A_n}_k\n where :math:\n{A_i}_k\n is the kth column of the ith matrix.\n\n\nExample::\n\n\n\n\n\n\n\n\nA = mx.nd.array([[1, -1],                  [2, -3]]) B = mx.nd.array([[1, 4],                  [2, 5],                  [3, 6]]) C = mx.nd.khatri_rao(A, B) print(C.asnumpy())\n\n\n\n\n\n\n\n\n[[  1.  -4.]    [  2.  -5.]    [  3.  -6.]    [  2. -12.]    [  4. -15.]    [  6. -18.]]\n\n\nDefined in src/operator/contrib/krprod.cc:L108\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input matrices\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_gelqf\n \n \nMethod\n.\n\n\nlinalg_gelqf(A)\n\n\n\n\nlinalg_gelqf is an alias of _linalg_gelqf.\n\n\nLQ factorization for general matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, we compute the LQ factorization (LAPACK \ngelqf\n, followed by \norglq\n). \nA\n must have shape \n(x, y)\n with \nx \n= y\n, and must have full rank \n=x\n. The LQ factorization consists of \nL\n with shape \n(x, x)\n and \nQ\n with shape \n(x, y)\n, so that:\n\n\nA\n = \nL\n * \nQ\n\n\nHere, \nL\n is lower triangular (upper triangle equal to zero) with nonzero diagonal, and \nQ\n is row-orthonormal, meaning that\n\n\nQ\n * \nQ\n\\ :sup:\nT\n\n\nis equal to the identity matrix of shape \n(x, x)\n.\n\n\nIf \nn\n2\n, \ngelqf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]\n\n\n// Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L529\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be factorized\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_gemm\n \n \nMethod\n.\n\n\nlinalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)\n\n\n\n\nlinalg_gemm is an alias of _linalg_gemm.\n\n\nPerforms general matrix multiplication and accumulation. Input are tensors \nA\n, \nB\n, \nC\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n) + \nbeta\n * \nC\n\n\nHere, \nalpha\n and \nbeta\n are scalar parameters, and \nop()\n is either the identity or matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]\n\n\n// Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L69\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nC::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\nbeta::double, optional, default=1\n: Scalar factor multiplied with C.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_gemm2\n \n \nMethod\n.\n\n\nlinalg_gemm2(A, B, transpose_a, transpose_b, alpha)\n\n\n\n\nlinalg_gemm2 is an alias of _linalg_gemm2.\n\n\nPerforms general matrix multiplication. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n)\n\n\nHere \nalpha\n is a scalar parameter and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]\n\n\n// Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L128\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_potrf\n \n \nMethod\n.\n\n\nlinalg_potrf(A)\n\n\n\n\nlinalg_potrf is an alias of _linalg_potrf.\n\n\nPerforms Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the Cholesky factor \nL\n of the symmetric, positive definite matrix \nA\n is computed. \nL\n is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:\n\n\nA\n = \nL\n * \nL\n\\ :sup:\nT\n\n\nIf \nn\n2\n, \npotrf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]\n\n\n// Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L178\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be decomposed\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_potri\n \n \nMethod\n.\n\n\nlinalg_potri(A)\n\n\n\n\nlinalg_potri is an alias of _linalg_potri.\n\n\nPerforms matrix inversion from a Cholesky factorization. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:\n\n\nout\n = \nA\n\\ :sup:\n-T\n * \nA\n\\ :sup:\n-1\n\n\nIn other words, if \nA\n is the Cholesky factor of a symmetric positive definite matrix \nB\n (obtained by \npotrf\n), then\n\n\nout\n = \nB\n\\ :sup:\n-1\n\n\nIf \nn\n2\n, \npotri\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\n.. note:: Use this operator only if you are certain you need the inverse of \nB\n, and           cannot use the Cholesky factor \nA\n (\npotrf\n), together with backsubstitution           (\ntrsm\n). The latter is numerically much safer, and also cheaper.\n\n\nExamples::\n\n\n// Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]\n\n\n// Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L236\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_sumlogdiag\n \n \nMethod\n.\n\n\nlinalg_sumlogdiag(A)\n\n\n\n\nlinalg_sumlogdiag is an alias of _linalg_sumlogdiag.\n\n\nComputes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).\n\n\nIf \nn\n2\n, \nsumlogdiag\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]\n\n\n// Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]\n\n\nDefined in src/operator/tensor/la_op.cc:L405\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of square matrices\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_syrk\n \n \nMethod\n.\n\n\nlinalg_syrk(A, transpose, alpha)\n\n\n\n\nlinalg_syrk is an alias of _linalg_syrk.\n\n\nMultiplication of matrix with its transpose. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the operator performs the BLAS3 function \nsyrk\n:\n\n\nout\n = \nalpha\n * \nA\n * \nA\n\\ :sup:\nT\n\n\nif \ntranspose=False\n, or\n\n\nout\n = \nalpha\n * \nA\n\\ :sup:\nT\n \\ * \nA\n\n\nif \ntranspose=True\n.\n\n\nIf \nn\n2\n, \nsyrk\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]\n\n\n// Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L461\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transpose of input matrix.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_trmm\n \n \nMethod\n.\n\n\nlinalg_trmm(A, B, transpose, rightside, alpha)\n\n\n\n\nlinalg_trmm is an alias of _linalg_trmm.\n\n\nPerforms multiplication with a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrmm\n:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n = \nalpha\n * \nB\n * \nop\n\\ (\nA\n)\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrmm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]\n\n\n// Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L293\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_trsm\n \n \nMethod\n.\n\n\nlinalg_trsm(A, B, transpose, rightside, alpha)\n\n\n\n\nlinalg_trsm is an alias of _linalg_trsm.\n\n\nSolves matrix equation involving a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrsm\n, solving for \nout\n in:\n\n\nop\n\\ (\nA\n) * \nout\n = \nalpha\n * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n * \nop\n\\ (\nA\n) = \nalpha\n * \nB\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrsm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n\n\n// Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L356\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename, ::Type{NDArray})\n\n\n\n\nLoad NDArrays from binary file.\n\n\nArguments:\n\n\n\n\nfilename::String\n: the path of the file to load. It could be S3 or HDFS address.\n\n\n\n\nReturns either \nDict{Symbol, NDArray}\n or \nVector{NDArray}\n.\n\n\nfilename\n can point to \ns3\n or \nhdfs\n resources if the \nlibmxnet\n is built with the corresponding components enabled. Examples:\n\n\n\n\ns3://my-bucket/path/my-s3-ndarray\n\n\nhdfs://my-bucket/path/my-hdfs-ndarray\n\n\n/path-to/my-local-ndarray\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.make_loss\n \n \nMethod\n.\n\n\nmake_loss(data)\n\n\n\n\nMake your own loss function in network construction.\n\n\nThis operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.\n\n\nFor example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::\n\n\ncross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)\n\n\nWe will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.\n\n\nThe storage type of $make_loss$ output depends upon the input storage type:\n\n\n\n\nmake_loss(default) = default\n\n\nmake_loss(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L200\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mod_from!\n \n \nMethod\n.\n\n\nmod_from!(x::NDArray, y::NDArray)\nmod_from!(x::NDArray, y::Real)\n\n\n\n\nElementwise modulo for \nNDArray\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mp_sgd_mom_update\n \n \nMethod\n.\n\n\nmp_sgd_mom_update(weight, grad, mom, weight32, lr, momentum, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdater function for multi-precision sgd optimizer\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmom::NDArray-or-SymbolicNode\n: Momentum\n\n\nweight32::NDArray-or-SymbolicNode\n: Weight32\n\n\nlr::float, required\n: Learning rate\n\n\nmomentum::float, optional, default=0\n: The decay rate of momentum estimates at each epoch.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mp_sgd_update\n \n \nMethod\n.\n\n\nmp_sgd_update(weight, grad, weight32, lr, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdater function for multi-precision sgd optimizer\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: gradient\n\n\nweight32::NDArray-or-SymbolicNode\n: Weight32\n\n\nlr::float, required\n: Learning rate\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mul_to!\n \n \nMethod\n.\n\n\nmul_to!(dst::NDArray, arg::NDArrayOrReal)\n\n\n\n\nElementwise multiplication into \ndst\n of either a scalar or an \nNDArray\n of the same shape. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nanprod\n \n \nMethod\n.\n\n\nnanprod(data, axis, keepdims, exclude)\n\n\n\n\nComputes the product of array elements over given axes treating Not a Numbers ($NaN$) as one.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L146\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nansum\n \n \nMethod\n.\n\n\nnansum(data, axis, keepdims, exclude)\n\n\n\n\nComputes the sum of array elements over given axes treating Not a Numbers ($NaN$) as zero.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L131\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.negative\n \n \nMethod\n.\n\n\nnegative(data)\n\n\n\n\nNumerical negative of the argument, element-wise.\n\n\nThe storage type of $negative$ output depends upon the input storage type:\n\n\n\n\nnegative(default) = default\n\n\nnegative(row_sparse) = row_sparse\n\n\nnegative(csr) = csr\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normal\n \n \nMethod\n.\n\n\nnormal(loc, scale, shape, ctx, dtype)\n\n\n\n\nnormal is an alias of _random_normal.\n\n\nDraw random samples from a normal (Gaussian) distribution.\n\n\n.. note:: The existing alias $normal$ is deprecated.\n\n\nSamples are distributed according to a normal distribution parametrized by \nloc\n (mean) and \nscale\n (standard deviation).\n\n\nExample::\n\n\nnormal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]\n\n\nDefined in src/operator/random/sample_op.cc:L85\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.one_hot\n \n \nMethod\n.\n\n\none_hot(indices, depth, on_value, off_value, dtype)\n\n\n\n\nReturns a one-hot array.\n\n\nThe locations represented by \nindices\n take value \non_value\n, while all other locations take value \noff_value\n.\n\n\none_hot\n operation with \nindices\n of shape $(i0, i1)$ and \ndepth\n  of $d$ would result in an output array of shape $(i0, i1, d)$ with::\n\n\noutput[i,j,:] = off_value   output[i,j,indices[i,j]] = on_value\n\n\nExamples::\n\n\none_hot([1,0,2,0], 3) = [[ 0.  1.  0.]                            [ 1.  0.  0.]                            [ 0.  0.  1.]                            [ 1.  0.  0.]]\n\n\none_hot([1,0,2,0], 3, on_value=8, off_value=1,           dtype='int32') = [[1 8 1]                             [8 1 1]                             [1 1 8]                             [8 1 1]]\n\n\none_hot([[1,0],[1,0],[2,0]], 3) = [[[ 0.  1.  0.]                                       [ 1.  0.  0.]]\n\n\n                                 [[ 0.  1.  0.]\n                                  [ 1.  0.  0.]]\n\n                                 [[ 0.  0.  1.]\n                                  [ 1.  0.  0.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L428\n\n\nArguments\n\n\n\n\nindices::NDArray-or-SymbolicNode\n: array of locations where to set on_value\n\n\ndepth::int, required\n: Depth of the one hot dimension.\n\n\non_value::double, optional, default=1\n: The value assigned to the locations represented by indices.\n\n\noff_value::double, optional, default=0\n: The value assigned to the locations not represented by indices.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: DType of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones\n \n \nMethod\n.\n\n\nones([DType], dims, [ctx::Context = cpu()])\nones([DType], dims...)\nones(x::NDArray)\n\n\n\n\nCreate an \nNDArray\n with specific shape \n type, and initialize with 1.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones_like\n \n \nMethod\n.\n\n\nones_like(data)\n\n\n\n\nReturn an array of ones with the same shape and type as the input array.\n\n\nExamples::\n\n\nx = [[ 0.,  0.,  0.],        [ 0.,  0.,  0.]]\n\n\nones_like(x) = [[ 1.,  1.,  1.],                   [ 1.,  1.,  1.]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.pad\n \n \nMethod\n.\n\n\npad(data, mode, pad_width, constant_value)\n\n\n\n\npad is an alias of Pad.\n\n\nPads an input array with a constant or edge values of the array.\n\n\n.. note:: \nPad\n is deprecated. Use \npad\n instead.\n\n\n.. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in \npad_width\n to be zero.\n\n\nThis operation pads an input array with either a \nconstant_value\n or edge values along each axis of the input array. The amount of padding is specified by \npad_width\n.\n\n\npad_width\n is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The \npad_width\n should be of length $2*N$ where $N$ is the number of dimensions of the array.\n\n\nFor dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.\n\n\nExample::\n\n\nx = [[[[  1.   2.   3.]           [  4.   5.   6.]]\n\n\n     [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]\n\n\n\n\npad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]\n\n\n\n\npad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]\n\n\n\n\nDefined in src/operator/pad.cc:L766\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: An n-dimensional input array.\n\n\nmode::{'constant', 'edge', 'reflect'}, required\n: Padding type to use. \"constant\" pads with \nconstant_value\n \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.\n\n\npad_width::Shape(tuple), required\n: Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.\n\n\nconstant_value::double, optional, default=0\n: The value used for padding when \nmode\n is \"constant\".\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.pick\n \n \nMethod\n.\n\n\npick(data, index, axis, keepdims)\n\n\n\n\nPicks elements from an input array according to the input indices along the given axis.\n\n\nGiven an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::\n\n\noutput[i] = input[i, indices[i]]\n\n\nBy default, if any index mentioned is too large, it is replaced by the index that addresses the last element along an axis (the \nclip\n mode).\n\n\nThis function supports n-dimensional input and (n-1)-dimensional indices arrays.\n\n\nExamples::\n\n\nx = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]\n\n\n// picks elements with specified indices along axis 0   pick(x, y=[0,1], 0) = [ 1.,  4.]\n\n\n// picks elements with specified indices along axis 1   pick(x, y=[0,1,0], 1) = [ 1.,  4.,  5.]\n\n\ny = [[ 1.],        [ 0.],        [ 2.]]\n\n\n// picks elements with specified indices along axis 1 and dims are maintained   pick(x,y, 1, keepdims=True) = [[ 2.],                                  [ 3.],                                  [ 6.]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L145\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\nindex::NDArray-or-SymbolicNode\n: The index array\n\n\naxis::int or None, optional, default='None'\n: The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$\n\n\nkeepdims::boolean, optional, default=0\n: If this is set to \nTrue\n, the reduced axis is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.radians\n \n \nMethod\n.\n\n\nradians(data)\n\n\n\n\nConverts each element of the input array from degrees to radians.\n\n\n.. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]\n\n\nThe storage type of $radians$ output depends upon the input storage type:\n\n\n\n\nradians(default) = default\n\n\nradians(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L182\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_exponential\n \n \nMethod\n.\n\n\nrandom_exponential(lam, shape, ctx, dtype)\n\n\n\n\nrandom_exponential is an alias of _random_exponential.\n\n\nDraw random samples from an exponential distribution.\n\n\nSamples are distributed according to an exponential distribution parametrized by \nlambda\n (rate).\n\n\nExample::\n\n\nexponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]\n\n\nDefined in src/operator/random/sample_op.cc:L115\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the exponential distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_gamma\n \n \nMethod\n.\n\n\nrandom_gamma(alpha, beta, shape, ctx, dtype)\n\n\n\n\nrandom_gamma is an alias of _random_gamma.\n\n\nDraw random samples from a gamma distribution.\n\n\nSamples are distributed according to a gamma distribution parametrized by \nalpha\n (shape) and \nbeta\n (scale).\n\n\nExample::\n\n\ngamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]\n\n\nDefined in src/operator/random/sample_op.cc:L100\n\n\nArguments\n\n\n\n\nalpha::float, optional, default=1\n: Alpha parameter (shape) of the gamma distribution.\n\n\nbeta::float, optional, default=1\n: Beta parameter (scale) of the gamma distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_generalized_negative_binomial\n \n \nMethod\n.\n\n\nrandom_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)\n\n\n\n\nrandom_generalized_negative_binomial is an alias of _random_generalized_negative_binomial.\n\n\nDraw random samples from a generalized negative binomial distribution.\n\n\nSamples are distributed according to a generalized negative binomial distribution parametrized by \nmu\n (mean) and \nalpha\n (dispersion). \nalpha\n is defined as \n1/k\n where \nk\n is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\ngeneralized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]\n\n\nDefined in src/operator/random/sample_op.cc:L168\n\n\nArguments\n\n\n\n\nmu::float, optional, default=1\n: Mean of the negative binomial distribution.\n\n\nalpha::float, optional, default=1\n: Alpha (dispersion) parameter of the negative binomial distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_negative_binomial\n \n \nMethod\n.\n\n\nrandom_negative_binomial(k, p, shape, ctx, dtype)\n\n\n\n\nrandom_negative_binomial is an alias of _random_negative_binomial.\n\n\nDraw random samples from a negative binomial distribution.\n\n\nSamples are distributed according to a negative binomial distribution parametrized by \nk\n (limit of unsuccessful experiments) and \np\n (failure probability in each experiment). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\nnegative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]\n\n\nDefined in src/operator/random/sample_op.cc:L149\n\n\nArguments\n\n\n\n\nk::int, optional, default='1'\n: Limit of unsuccessful experiments.\n\n\np::float, optional, default=1\n: Failure probability in each experiment.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_normal\n \n \nMethod\n.\n\n\nrandom_normal(loc, scale, shape, ctx, dtype)\n\n\n\n\nrandom_normal is an alias of _random_normal.\n\n\nDraw random samples from a normal (Gaussian) distribution.\n\n\n.. note:: The existing alias $normal$ is deprecated.\n\n\nSamples are distributed according to a normal distribution parametrized by \nloc\n (mean) and \nscale\n (standard deviation).\n\n\nExample::\n\n\nnormal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]\n\n\nDefined in src/operator/random/sample_op.cc:L85\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_poisson\n \n \nMethod\n.\n\n\nrandom_poisson(lam, shape, ctx, dtype)\n\n\n\n\nrandom_poisson is an alias of _random_poisson.\n\n\nDraw random samples from a Poisson distribution.\n\n\nSamples are distributed according to a Poisson distribution parametrized by \nlambda\n (rate). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\npoisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]\n\n\nDefined in src/operator/random/sample_op.cc:L132\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the Poisson distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_uniform\n \n \nMethod\n.\n\n\nrandom_uniform(low, high, shape, ctx, dtype)\n\n\n\n\nrandom_uniform is an alias of _random_uniform.\n\n\nDraw random samples from a uniform distribution.\n\n\n.. note:: The existing alias $uniform$ is deprecated.\n\n\nSamples are uniformly distributed over the half-open interval \n[low, high)\n (includes \nlow\n, but excludes \nhigh\n).\n\n\nExample::\n\n\nuniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]\n\n\nDefined in src/operator/random/sample_op.cc:L66\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: Lower bound of the distribution.\n\n\nhigh::float, optional, default=1\n: Upper bound of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rcbrt\n \n \nMethod\n.\n\n\nrcbrt(data)\n\n\n\n\nReturns element-wise inverse cube-root value of the input.\n\n\n.. math::    rcbrt(x) = 1/\\sqrt[3]{x}\n\n\nExample::\n\n\nrcbrt([1,8,-125]) = [1.0, 0.5, -0.2]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L619\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rdiv_from!\n \n \nMethod\n.\n\n\nrdiv_from!(x:: Real, y::NDArray)\n\n\n\n\nElementwise divide a scalar by an \nNDArray\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.reciprocal\n \n \nMethod\n.\n\n\nreciprocal(data)\n\n\n\n\nReturns the reciprocal of the argument, element-wise.\n\n\nCalculates 1/x.\n\n\nExample::\n\n\nreciprocal([-2, 1, 3, 1.6, 0.2]) = [-0.5, 1.0, 0.33333334, 0.625, 5.0]\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L364\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.reshape_like\n \n \nMethod\n.\n\n\nreshape_like(lhs, rhs)\n\n\n\n\nReshape lhs to have the same shape as rhs.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input.\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rint\n \n \nMethod\n.\n\n\nrint(data)\n\n\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\n.. note::\n\n\n\n\nFor input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.\n\n\nFor input $-n.5$ both $rint$ and $round$ returns $-n-1$.\n\n\n\n\nExample::\n\n\nrint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]\n\n\nThe storage type of $rint$ output depends upon the input storage type:\n\n\n\n\nrint(default) = default\n\n\nrint(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L445\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rmod_from!\n \n \nMethod\n.\n\n\nrmod_from!(y::Real, x::NDArray)\n\n\n\n\nElementwise modulo for \nNDArray\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rmsprop_update\n \n \nMethod\n.\n\n\nrmsprop_update(weight, grad, n, lr, gamma1, epsilon, wd, rescale_grad, clip_gradient, clip_weights)\n\n\n\n\nUpdate function for \nRMSProp\n optimizer.\n\n\nRMSprop\n is a variant of stochastic gradient descent where the gradients are divided by a cache which grows with the sum of squares of recent gradients?\n\n\nRMSProp\n is similar to \nAdaGrad\n, a popular variant of \nSGD\n which adaptively tunes the learning rate of each parameter. \nAdaGrad\n lowers the learning rate for each parameter monotonically over the course of training. While this is analytically motivated for convex optimizations, it may not be ideal for non-convex problems. \nRMSProp\n deals with this heuristically by allowing the learning rates to rebound as the denominator decays over time.\n\n\nDefine the Root Mean Square (RMS) error criterion of the gradient as :math:\nRMS[g]_t = \\sqrt{E[g^2]_t + \\epsilon}\n, where :math:\ng\n represents gradient and :math:\nE[g^2]_t\n is the decaying average over past squared gradient.\n\n\nThe :math:\nE[g^2]_t\n is given by:\n\n\n.. math::   E[g^2]\nt = \\gamma * E[g^2]\n + (1-\\gamma) * g_t^2\n\n\nThe update step is\n\n\n.. math::   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{RMS[g]_t} g_t\n\n\nThe RMSProp code follows the version in http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf Tieleman \n Hinton, 2012.\n\n\nHinton suggests the momentum term :math:\n\\gamma\n to be 0.9 and the learning rate :math:\n\\eta\n to be 0.001.\n\n\nDefined in src/operator/optimizer_op.cc:L262\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nn::NDArray-or-SymbolicNode\n: n\n\n\nlr::float, required\n: Learning rate\n\n\ngamma1::float, optional, default=0.95\n: The decay rate of momentum estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nclip_weights::float, optional, default=-1\n: Clip weights to the range of [-clip_weights, clip_weights] If clip_weights \n= 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rmspropalex_update\n \n \nMethod\n.\n\n\nrmspropalex_update(weight, grad, n, g, delta, lr, gamma1, gamma2, epsilon, wd, rescale_grad, clip_gradient, clip_weights)\n\n\n\n\nUpdate function for RMSPropAlex optimizer.\n\n\nRMSPropAlex\n is non-centered version of \nRMSProp\n.\n\n\nDefine :math:\nE[g^2]_t\n is the decaying average over past squared gradient and :math:\nE[g]_t\n is the decaying average over past gradient.\n\n\n.. math::   E[g^2]\nt = \\gamma_1 * E[g^2]\n + (1 - \\gamma_1) * g_t^2\\\n  E[g]\nt = \\gamma_1 * E[g]\n + (1 - \\gamma_1) * g_t\\\n  \\Delta_t = \\gamma_2 * \\Delta_{t-1} - \\frac{\\eta}{\\sqrt{E[g^2]_t - E[g]_t^2 + \\epsilon}} g_t\\\n The update step is\n\n\n.. math::   \\theta_{t+1} = \\theta_t + \\Delta_t\n\n\nThe RMSPropAlex code follows the version in http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.\n\n\nGraves suggests the momentum term :math:\n\\gamma_1\n to be 0.95, :math:\n\\gamma_2\n to be 0.9 and the learning rate :math:\n\\eta\n to be 0.0001.\n\n\nDefined in src/operator/optimizer_op.cc:L301\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nn::NDArray-or-SymbolicNode\n: n\n\n\ng::NDArray-or-SymbolicNode\n: g\n\n\ndelta::NDArray-or-SymbolicNode\n: delta\n\n\nlr::float, required\n: Learning rate\n\n\ngamma1::float, optional, default=0.95\n: Decay rate.\n\n\ngamma2::float, optional, default=0.9\n: Decay rate.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nclip_weights::float, optional, default=-1\n: Clip weights to the range of [-clip_weights, clip_weights] If clip_weights \n= 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rsqrt\n \n \nMethod\n.\n\n\nrsqrt(data)\n\n\n\n\nReturns element-wise inverse square-root value of the input.\n\n\n.. math::    rsqrt(x) = 1/\\sqrt{x}\n\n\nExample::\n\n\nrsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]\n\n\nThe storage type of $rsqrt$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L585\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_exponential\n \n \nMethod\n.\n\n\nsample_exponential(lam, shape, dtype)\n\n\n\n\nsample_exponential is an alias of _sample_exponential.\n\n\nConcurrent sampling from multiple exponential distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]\n\n\n// Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]\n\n\nDefined in src/operator/random/multisample_op.cc:L284\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_gamma\n \n \nMethod\n.\n\n\nsample_gamma(alpha, shape, dtype, beta)\n\n\n\n\nsample_gamma is an alias of _sample_gamma.\n\n\nConcurrent sampling from multiple gamma distributions with parameters \nalpha\n (shape) and \nbeta\n (scale).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nalpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]\n\n\n// Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]\n\n\n// Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]\n\n\nDefined in src/operator/random/multisample_op.cc:L282\n\n\nArguments\n\n\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (shape) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nbeta::NDArray-or-SymbolicNode\n: Beta (scale) parameters of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_generalized_negative_binomial\n \n \nMethod\n.\n\n\nsample_generalized_negative_binomial(mu, shape, dtype, alpha)\n\n\n\n\nsample_generalized_negative_binomial is an alias of _sample_generalized_negative_binomial.\n\n\nConcurrent sampling from multiple generalized negative binomial distributions with parameters \nmu\n (mean) and \nalpha\n (dispersion).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nmu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]\n\n\n// Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]\n\n\n// Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L293\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (dispersion) parameters of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_multinomial\n \n \nMethod\n.\n\n\nsample_multinomial(data, shape, get_prob, dtype)\n\n\n\n\nsample_multinomial is an alias of _sample_multinomial.\n\n\nConcurrent sampling from multiple multinomial distributions.\n\n\ndata\n is an \nn\n dimensional array whose last dimension has length \nk\n, where \nk\n is the number of possible outcomes of each multinomial distribution. This operator will draw \nshape\n samples from each distribution. If shape is empty one sample will be drawn from each distribution.\n\n\nIf \nget_prob\n is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.\n\n\nNote that the input distribution must be normalized, i.e. \ndata\n must sum to 1 along its last axis.\n\n\nExamples::\n\n\nprobs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]\n\n\n// Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]\n\n\n// Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]\n\n\n// requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Distribution probabilities. Must sum to one on the last axis.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\nget_prob::boolean, optional, default=0\n: Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.\n\n\ndtype::{'int32'},optional, default='int32'\n: DType of the output in case this can't be inferred. Only support int32 for now.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_negative_binomial\n \n \nMethod\n.\n\n\nsample_negative_binomial(k, shape, dtype, p)\n\n\n\n\nsample_negative_binomial is an alias of _sample_negative_binomial.\n\n\nConcurrent sampling from multiple negative binomial distributions with parameters \nk\n (failure limit) and \np\n (failure probability).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nk = [ 20, 49 ]    p = [ 0.4 , 0.77 ]\n\n\n// Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]\n\n\n// Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L289\n\n\nArguments\n\n\n\n\nk::NDArray-or-SymbolicNode\n: Limits of unsuccessful experiments.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\np::NDArray-or-SymbolicNode\n: Failure probabilities in each experiment.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_normal\n \n \nMethod\n.\n\n\nsample_normal(mu, shape, dtype, sigma)\n\n\n\n\nsample_normal is an alias of _sample_normal.\n\n\nConcurrent sampling from multiple normal distributions with parameters \nmu\n (mean) and \nsigma\n (standard deviation).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nmu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]\n\n\n// Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]\n\n\nDefined in src/operator/random/multisample_op.cc:L279\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nsigma::NDArray-or-SymbolicNode\n: Standard deviations of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_poisson\n \n \nMethod\n.\n\n\nsample_poisson(lam, shape, dtype)\n\n\n\n\nsample_poisson is an alias of _sample_poisson.\n\n\nConcurrent sampling from multiple Poisson distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]\n\n\n// Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L286\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_uniform\n \n \nMethod\n.\n\n\nsample_uniform(low, shape, dtype, high)\n\n\n\n\nsample_uniform is an alias of _sample_uniform.\n\n\nConcurrent sampling from multiple uniform distributions on the intervals given by \n[low,high)\n.\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nlow = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]\n\n\n// Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]\n\n\nDefined in src/operator/random/multisample_op.cc:L277\n\n\nArguments\n\n\n\n\nlow::NDArray-or-SymbolicNode\n: Lower bounds of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nhigh::NDArray-or-SymbolicNode\n: Upper bounds of the distributions.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename::AbstractString, data)\n\n\n\n\nSave NDarrays to binary file. Filename could be S3 or HDFS address, if \nlibmxnet\n is built with corresponding support (see \nload\n).\n\n\n\n\nfilename::String\n: path to the binary file to write to.\n\n\ndata\n: data to save to file. Data can be a\nNDArray\n, a \nVector\n of \nNDArray\n, or a \nDict{Symbol}\n contains \nNDArray\ns.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.scatter_nd\n \n \nMethod\n.\n\n\nscatter_nd(data, indices, shape)\n\n\n\n\nScatters data into a new tensor according to indices. \ngather_nd\n and \nscatter_nd\n are inverse functions to each other.\n\n\nGiven \ndata\n with shape \n(Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1})\n and indices with shape \n(M, Y_0, ..., Y_{K-1})\n, the output will have shape \n(X_0, X_1, ..., X_{N-1})\n, where \nM \n= N\n. If \nM == N\n, data shape should simply be \n(Y_0, ..., Y_{K-1})\n.\n\n\nThe elements in output is defined as follows::\n\n\noutput[indices[0, y_0, ..., y_{K-1}],          ...,          indices[M-1, y_0, ..., y_{K-1}],          x_M, ..., x_{N-1}] = data[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}]\n\n\nall other entries in output are 0.\n\n\nExamples::\n\n\ndata = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   shape = (2, 2)   scatter_nd(data, indices, shape) = [[0, 0], [2, 3]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: data\n\n\nindices::NDArray-or-SymbolicNode\n: indices\n\n\nshape::Shape(tuple), required\n: Shape of output.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_mom_update\n \n \nMethod\n.\n\n\nsgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)\n\n\n\n\nMomentum update function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nMomentum update has better convergence rates on neural networks. Mathematically it looks like below:\n\n\n.. math::\n\n\nv_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t\n\n\nIt updates the weights using::\n\n\nv = momentum * v - learning_rate * gradient   weight += v\n\n\nWhere the parameter $momentum$ is the decay rate of momentum estimates at each epoch.\n\n\nIf weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::\n\n\nfor row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]\n\n\nDefined in src/operator/optimizer_op.cc:L94\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmom::NDArray-or-SymbolicNode\n: Momentum\n\n\nlr::float, required\n: Learning rate\n\n\nmomentum::float, optional, default=0\n: The decay rate of momentum estimates at each epoch.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_update\n \n \nMethod\n.\n\n\nsgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdate function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nIt updates the weights using::\n\n\nweight = weight - learning_rate * gradient\n\n\nIf weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::\n\n\nfor row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]\n\n\nDefined in src/operator/optimizer_op.cc:L54\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nlr::float, required\n: Learning rate\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice\n \n \nMethod\n.\n\n\nslice(arr :: NDArray, start:stop)\n\n\n\n\nCreate a view into a sub-slice of an \nNDArray\n. Note only slicing at the slowest changing dimension is supported. In Julia's column-major perspective, this is the last dimension. For example, given an \nNDArray\n of shape (2,3,4), \nslice(array, 2:3)\n will create a \nNDArray\n of shape (2,3,2), sharing the data with the original array. This operation is used in data parallelization to split mini-batch into sub-batches for different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice\n \n \nMethod\n.\n\n\nslice(data, begin, end, step)\n\n\n\n\nSlices a region of the array.\n\n\n.. note:: $crop$ is deprecated. Use $slice$ instead.\n\n\nThis function returns a sliced array between the indices given by \nbegin\n and \nend\n with the corresponding \nstep\n.\n\n\nFor an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m \n= n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.\n\n\nThe resulting array's \nk\n-th dimension contains elements from the \nk\n-th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).\n\n\nIf the \nk\n-th elements are \nNone\n in the sequence of \nbegin\n, \nend\n, and \nstep\n, the following rule will be used to set default values. If \ns_k\n is \nNone\n, set \ns_k=1\n. If \ns_k \n 0\n, set \nb_k=0\n, \ne_k=d_k\n; else, set \nb_k=d_k-1\n, \ne_k=-1\n.\n\n\nThe storage type of $slice$ output depends on storage types of inputs\n\n\n\n\nslice(csr) = csr\n\n\notherwise, $slice$ generates output with default storage\n\n\n\n\n.. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.\n\n\nExample::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L355\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice_axis\n \n \nMethod\n.\n\n\nslice_axis(data, axis, begin, end)\n\n\n\n\nSlices along a given axis.\n\n\nReturns an array slice along a given \naxis\n starting from the \nbegin\n index to the \nend\n index.\n\n\nExamples::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice_axis(x, axis=0, begin=1, end=3) = [[  5.,   6.,   7.,   8.],                                            [  9.,  10.,  11.,  12.]]\n\n\nslice_axis(x, axis=1, begin=0, end=2) = [[  1.,   2.],                                            [  5.,   6.],                                            [  9.,  10.]]\n\n\nslice_axis(x, axis=1, begin=-3, end=-1) = [[  2.,   3.],                                              [  6.,   7.],                                              [ 10.,  11.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L442\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\naxis::int, required\n: Axis along which to be sliced, supports negative indexes.\n\n\nbegin::int, required\n: The beginning index along the axis to be sliced,  supports negative indexes.\n\n\nend::int or None, required\n: The ending index along the axis to be sliced,  supports negative indexes.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.smooth_l1\n \n \nMethod\n.\n\n\nsmooth_l1(data, scalar)\n\n\n\n\nCalculate Smooth L1 Loss(lhs, scalar) by summing\n\n\n.. math::\n\n\nf(x) =\n\\begin{cases}\n(\\sigma x)^2/2,\n \\text{if }x \n 1/\\sigma^2\\\\\n|x|-0.5/\\sigma^2,\n \\text{otherwise}\n\\end{cases}\n\n\n\n\nwhere :math:\nx\n is an element of the tensor \nlhs\n and :math:\n\\sigma\n is the scalar.\n\n\nExample::\n\n\nsmooth_l1([1, 2, 3, 4], sigma=1) = [0.5, 1.5, 2.5, 3.5]\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_extended.cc:L103\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax_cross_entropy\n \n \nMethod\n.\n\n\nsoftmax_cross_entropy(data, label)\n\n\n\n\nCalculate cross entropy of softmax output and one-hot label.\n\n\n\n\n\n\nThis operator computes the cross entropy in two steps:\n\n\n\n\nApplies softmax function on the input array.\n\n\nComputes and returns the cross entropy loss between the softmax output and the labels.\n\n\n\n\nThe softmax function and cross entropy loss is given by:\n\n\n\n\n\n\nSoftmax Function:\n\n\n\n\n\n\n.. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n\n\n\n\nCross Entropy Function:\n\n\n\n\n.. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)\n\n\n\n\n\n\nExample::\n\n\nx = [[1, 2, 3],        [11, 7, 5]]\n\n\nlabel = [2, 0]\n\n\nsoftmax(x) = [[0.09003057, 0.24472848, 0.66524094],                 [0.97962922, 0.01794253, 0.00242826]]\n\n\nsoftmax_cross_entropy(data, label) = - log(0.66524084) - log(0.97962922) = 0.4281871\n\n\nDefined in src/operator/loss_binary_op.cc:L59\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.square\n \n \nMethod\n.\n\n\nsquare(data)\n\n\n\n\nReturns element-wise squared value of the input.\n\n\n.. math::    square(x) = x^2\n\n\nExample::\n\n\nsquare([2, 3, 4]) = [4, 9, 16]\n\n\nThe storage type of $square$ output depends upon the input storage type:\n\n\n\n\nsquare(default) = default\n\n\nsquare(row_sparse) = row_sparse\n\n\nsquare(csr) = csr\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L542\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.stack\n \n \nMethod\n.\n\n\nstack(data, axis, num_args)\n\n\n\n\nNote\n: stack takes variable number of positional inputs. So instead of calling as stack([x, y, z], num_args=3), one should call via stack(x, y, z), and num_args will be determined automatically.\n\n\nJoin a sequence of arrays along a new axis.\n\n\nThe axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.\n\n\nExamples::\n\n\nx = [1, 2]   y = [3, 4]\n\n\nstack(x, y) = [[1, 2],                  [3, 4]]   stack(x, y, axis=1) = [[1, 3],                          [2, 4]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: List of arrays to stack\n\n\naxis::int, optional, default='0'\n: The axis in the result array along which the input arrays are stacked.\n\n\nnum_args::int, required\n: Number of inputs to be stacked.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.stop_gradient\n \n \nMethod\n.\n\n\nstop_gradient(data)\n\n\n\n\nstop_gradient is an alias of BlockGrad.\n\n\nStops gradient computation.\n\n\nStops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.\n\n\nExample::\n\n\nv1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)\n\n\nexecutor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]\n\n\nexecutor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L167\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sub_from!\n \n \nMethod\n.\n\n\nsub_from!(dst::NDArray, args::NDArrayOrReal...)\n\n\n\n\nSubtract a bunch of arguments from \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum_axis\n \n \nMethod\n.\n\n\nsum_axis(data, axis, keepdims, exclude)\n\n\n\n\nsum_axis is an alias of sum.\n\n\nComputes the sum of array elements over given axes.\n\n\n.. Note::\n\n\nsum\n and \nsum_axis\n are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.\n\n\nExample::\n\n\ndata = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]\n\n\nsum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]\n\n\nsum(data, axis=[1,2])   [ 12.  19.  27.]\n\n\ndata = [[1,2,0],           [3,0,1],           [4,1,0]]\n\n\ncsr = cast_storage(data, 'csr')\n\n\nsum(csr, axis=0)   [ 8.  3.  1.]\n\n\nsum(csr, axis=1)   [ 3.  4.  5.]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L85\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.swapaxes\n \n \nMethod\n.\n\n\nswapaxes(data, dim1, dim2)\n\n\n\n\nswapaxes is an alias of SwapAxis.\n\n\nInterchanges two axes of an array.\n\n\nExamples::\n\n\nx = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]\n\n\nx = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array\n\n\nswapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]\n\n\nDefined in src/operator/swapaxis.cc:L70\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.take\n \n \nMethod\n.\n\n\ntake(a, indices, axis, mode)\n\n\n\n\nTakes elements from an input array along the given axis.\n\n\nThis function slices the input array along a particular axis with the provided indices.\n\n\nGiven an input array with shape $(d0, d1, d2)$ and indices with shape $(i0, i1)$, the output will have shape $(i0, i1, d1, d2)$, computed by::\n\n\noutput[i,j,:,:] = input[indices[i,j],:,:]\n\n\n.. note::\n\n\n\n\naxis\n- Only slicing along axis 0 is supported for now.\n\n\nmode\n- Only \nclip\n mode is supported for now.\n\n\n\n\nExamples::\n\n\nx = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]\n\n\n// takes elements with specified indices along axis 0   take(x, [[0,1],[1,2]]) = [[[ 1.,  2.],                              [ 3.,  4.]],\n\n\n                        [[ 3.,  4.],\n                         [ 5.,  6.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L327\n\n\nArguments\n\n\n\n\na::NDArray-or-SymbolicNode\n: The input array.\n\n\nindices::NDArray-or-SymbolicNode\n: The indices of the values to be extracted.\n\n\naxis::int, optional, default='0'\n: The axis of input array to be taken.\n\n\nmode::{'clip', 'raise', 'wrap'},optional, default='clip'\n: Specify how out-of-bound indices bahave. \"clip\" means clip to the range. So, if all indices mentioned are too large, they are replaced by the index that addresses the last element along an axis.  \"wrap\" means to wrap around.  \"raise\" means to raise an error.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.tile\n \n \nMethod\n.\n\n\ntile(data, reps)\n\n\n\n\nRepeats the whole array multiple times.\n\n\nIf $reps$ has length \nd\n, and input array has dimension of \nn\n. There are three cases:\n\n\n\n\n\n\nn=d\n. Repeat \ni\n-th dimension of the input by $reps[i]$ times::\n\n\nx = [[1, 2],        [3, 4]]\n\n\ntile(x, reps=(2,3)) = [[ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.],                          [ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.]]\n  * \nn\nd\n. $reps$ is promoted to length \nn\n by pre-pending 1's to it. Thus for an input shape $(2,3)$, $repos=(2,)$ is treated as $(1,2)$::\n\n\n\n\n\n\ntile(x, reps=(2,)) = [[ 1.,  2.,  1.,  2.],\n                      [ 3.,  4.,  3.,  4.]]\n\n\n\n\n\n\n\n\nn\nd\n. The input is promoted to be d-dimensional by prepending new axes. So a shape $(2,2)$ array is promoted to $(1,2,2)$ for 3-D replication::\n\n\ntile(x, reps=(2,2,3)) = [[[ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.],                             [ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.]],\n\n\n[[ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.],\n                        [ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.]]]\n\n\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L621\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\nreps::Shape(tuple), required\n: The number of times for repeating the tensor a. If reps has length d, the result will have dimension of max(d, a.ndim); If a.ndim \n d, a is promoted to be d-dimensional by prepending new axes. If a.ndim \n d, reps is promoted to a.ndim by pre-pending 1's to it.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.topk\n \n \nMethod\n.\n\n\ntopk(data, axis, k, ret_typ, is_ascend)\n\n\n\n\nReturns the top \nk\n elements in an input array along the given axis.\n\n\nExamples::\n\n\nx = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]\n\n\n// returns an index of the largest element on last axis   topk(x) = [[ 2.],              [ 1.]]\n\n\n// returns the value of top-2 largest elements on last axis   topk(x, ret_typ='value', k=2) = [[ 0.4,  0.3],                                    [ 0.3,  0.2]]\n\n\n// returns the value of top-2 smallest elements on last axis   topk(x, ret_typ='value', k=2, is_ascend=1) = [[ 0.2 ,  0.3],                                                [ 0.1 ,  0.2]]\n\n\n// returns the value of top-2 largest elements on axis 0   topk(x, axis=0, ret_typ='value', k=2) = [[ 0.3,  0.3,  0.4],                                            [ 0.1,  0.2,  0.2]]\n\n\n// flattens and then returns list of both values and indices   topk(x, ret_typ='both', k=2) = [[[ 0.4,  0.3], [ 0.3,  0.2]] ,  [[ 2.,  0.], [ 1.,  2.]]]\n\n\nDefined in src/operator/tensor/ordering_op.cc:L63\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.\n\n\nk::int, optional, default='1'\n: Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k \n 1.\n\n\nret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices'\n: The return type.\n\n\n\n\n\"value\" means to return the top k values, \"indices\" means to return the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return a list of both values and indices of top k elements.\n\n\n\n\nis_ascend::boolean, optional, default=0\n: Whether to choose k largest or k smallest elements. Top K largest elements will be chosen if set to false.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.try_get_shared\n \n \nMethod\n.\n\n\ntry_get_shared(arr; sync=:nop)\n\n\n\n\nTry to create a Julia array by sharing the data with the underlying \nNDArray\n.\n\n\nArguments:\n\n\n\n\narr::NDArray\n: the array to be shared.\n\n\n\n\n\n\nNote\n\n\nThe returned array does not guarantee to share data with the underlying \nNDArray\n. In particular, data sharing is possible only when the \nNDArray\n lives on CPU.\n\n\n\n\n\n\nsync::Symbol\n: \n:nop\n,\n:write\n, \n:read\n On CPU, invoke \n_wait_to_read\n if \n:read\n; invoke \n_wait_to_write\n if \n:write\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.uniform\n \n \nMethod\n.\n\n\nuniform(low, high, shape, ctx, dtype)\n\n\n\n\nuniform is an alias of _random_uniform.\n\n\nDraw random samples from a uniform distribution.\n\n\n.. note:: The existing alias $uniform$ is deprecated.\n\n\nSamples are uniformly distributed over the half-open interval \n[low, high)\n (includes \nlow\n, but excludes \nhigh\n).\n\n\nExample::\n\n\nuniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]\n\n\nDefined in src/operator/random/sample_op.cc:L66\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: Lower bound of the distribution.\n\n\nhigh::float, optional, default=1\n: Upper bound of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.where\n \n \nMethod\n.\n\n\nwhere(condition, x, y)\n\n\n\n\nGiven three ndarrays, condition, x, and y, return an ndarray with the elements from x or y, depending on the elements from condition are true or false. x and y must have the same shape. If condition has the same shape as x, each element in the output array is from x if the corresponding element in the condition is true, and from y if false. If condition does not have the same shape as x, it must be a 1D array whose size is the same as x's first dimension size. Each row of the output array is from x's row if the corresponding element from condition is true, and from y's row if false.\n\n\nFrom:src/operator/tensor/control_flow_op.cc:40\n\n\nArguments\n\n\n\n\ncondition::NDArray-or-SymbolicNode\n: condition array\n\n\nx::NDArray-or-SymbolicNode\n:\n\n\ny::NDArray-or-SymbolicNode\n:\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros\n \n \nMethod\n.\n\n\nzeros([DType], dims, [ctx::Context = cpu()])\nzeros([DType], dims...)\nzeros(x::NDArray)\n\n\n\n\nCreate zero-ed \nNDArray\n with specific shape and type.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros_like\n \n \nMethod\n.\n\n\nzeros_like(data)\n\n\n\n\nReturn an array of zeros with the same shape and type as the input array.\n\n\nThe storage type of $zeros_like$ output depends on the storage type of the input\n\n\n\n\nzeros_like(row_sparse) = row_sparse\n\n\nzeros_like(csr) = csr\n\n\nzeros_like(default) = default\n\n\n\n\nExamples::\n\n\nx = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]\n\n\nzeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nTakingBroadcastSeriously.broadcast_\n \n \nMethod\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@nd_as_jl\n \n \nMacro\n.\n\n\nManipulating as Julia Arrays\n\n\n@nd_as_jl(captures..., statement)\n\n\n\n\nA convenient macro that allows to operate \nNDArray\n as Julia Arrays. For example,\n\n\n  x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end\n\n\n\n\nUnder the hood, the macro convert all the declared captures from \nNDArray\n into Julia Arrays, by using \ntry_get_shared\n. And automatically commit the modifications back into the \nNDArray\n that is declared as \nrw\n. This is useful for fast prototyping and when implement non-critical computations, such as \nAbstractEvalMetric\n.\n\n\n\n\nNote\n\n\n\n\n\n\nMultiple \nrw\n and / or \nro\n capture declaration could be made.\n\n\nThe macro does \nnot\n check to make sure that \nro\n captures are not modified. If the original \nNDArray\n lives in CPU memory, then it is very likely the corresponding Julia Array shares data with the \nNDArray\n, so modifying the Julia Array will also modify the underlying \nNDArray\n.\n\n\nMore importantly, since the \nNDArray\n is asynchronized, we will wait for \nwriting\n for \nrw\n variables but wait only for \nreading\n in \nro\n variables. If we write into those \nro\n variables, \nand\n if the memory is shared, racing condition might happen, and the behavior is undefined.\n\n\nWhen an \nNDArray\n is declared to be captured as \nrw\n, its contents is always sync back in the end.\n\n\nThe execution results of the expanded macro is always \nnothing\n.\n\n\nThe statements are wrapped in a \nlet\n, thus locally introduced new variables will not be available after the statements. So you will need to declare the variables before calling the macro if needed.\n\n\n\n\nsource", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/ndarray/#ndarray-api", 
            "text": "", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/ndarray/#arithmetic-operations", 
            "text": "In the following example  y  can be a  Real  value or another  NDArray     API  Example       +  x .+ y  Elementwise summation    -  x .- y  Elementwise minus    *  x .* y  Elementwise multiplication    /  x ./ y  Elementwise division    ^  x .^ y  Elementwise power    %  x .% y  Elementwise modulo", 
            "title": "Arithmetic Operations"
        }, 
        {
            "location": "/api/ndarray/#trigonometric-functions", 
            "text": "API  Example       sin  sin.(x)  Elementwise sine    cos  cos.(x)  Elementwise cosine    tan  tan.(x)  Elementwise tangent    asin  asin.(x)  Elementwise inverse sine    acos  acos.(x)  Elementwise inverse cosine    atan  atan.(x)  Elementwise inverse tangent", 
            "title": "Trigonometric Functions"
        }, 
        {
            "location": "/api/ndarray/#hyperbolic-functions", 
            "text": "API  Example       sinh  sinh.(x)  Elementwise hyperbolic sine    cosh  cosh.(x)  Elementwise hyperbolic cosine    tanh  tanh.(x)  Elementwise hyperbolic tangent    asinh  asinh.(x)  Elementwise inverse hyperbolic sine    acosh  acosh.(x)  Elementwise inverse hyperbolic cosine    atanh  atanh.(x)  Elementwise inverse hyperbolic tangent", 
            "title": "Hyperbolic Functions"
        }, 
        {
            "location": "/api/ndarray/#activation-functions", 
            "text": "API  Example       \u03c3  \u03c3.(x)  Sigmoid function    sigmoid  sigmoid.(x)  Sigmoid function    relu  relu.(x)  ReLU function    softmax  softmax.(x)  Softmax function    log_softmax  log_softmax.(x)  Softmax followed by log", 
            "title": "Activation Functions"
        }, 
        {
            "location": "/api/ndarray/#reference", 
            "text": "#  MXNet.mx.NDArray     Type .  NDArray  Wrapper of the  NDArray  type in  libmxnet . This is the basic building block of tensor-based computation.   Note  since C/C++ use row-major ordering for arrays while Julia follows a   column-major ordering. To keep things consistent, we keep the underlying data   in their original layout, but use  language-native  convention when we talk   about shapes. For example, a mini-batch of 100 MNIST images is a tensor of   C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory   have shape (28,28,1,100).   source  #  Base.cos     Function .  cos.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L63  source  #  Base.cosh     Function .  cosh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L216  source  #  Base.reshape     Method .  reshape(arr::NDArray, dim; reverse=false)  Defined in src/operator/tensor/matrix_op.cc:L164  source  #  Base.reshape     Method .  reshape(arr::NDArray, dim...; reverse=false)  Defined in src/operator/tensor/matrix_op.cc:L164  source  #  Base.sin     Function .  sin.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L46  source  #  Base.sinh     Function .  sinh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L201  source  #  Base.tan     Function .  tan.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L83  source  #  Base.tanh     Function .  tanh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L234  source  #  MXNet.mx.clip!     Method .  clip(x::NDArray, min, max)\nclip!(x::NDArray, min, max)  Clips (limits) the values in  NDArray . Given an interval, values outside the interval are clipped to the interval edges. Clipping  x  between  min  and  x  would be:  clip(x, min_, max_) = max(min(x, max_), min_))  julia  x = NDArray(1:9);\n\njulia  mx.clip(x, 2, 8)'\n1\u00d79 mx.NDArray{Int64,2} @ CPU0:\n 2  2  3  4  5  6  7  8  8  The storage type of clip output depends on storage types of inputs and the  min ,  max  parameter values:   clip(default) = default  clip(row_sparse, min  = 0, max  = 0) = row_sparse  clip(csr, min  = 0, max  = 0) = csr  clip(row_sparse, min   0, max   0) = default  clip(row_sparse, min   0, max   0) = default  clip(csr, min   0, max   0) = csr  clip(csr, min   0, max   0) = csr   Defined in src/operator/tensor/matrix_op.cc:L486  source  #  MXNet.mx.clip     Method .  clip(x::NDArray, min, max)\nclip!(x::NDArray, min, max)  Clips (limits) the values in  NDArray . Given an interval, values outside the interval are clipped to the interval edges. Clipping  x  between  min  and  x  would be:  clip(x, min_, max_) = max(min(x, max_), min_))  julia  x = NDArray(1:9);\n\njulia  mx.clip(x, 2, 8)'\n1\u00d79 mx.NDArray{Int64,2} @ CPU0:\n 2  2  3  4  5  6  7  8  8  The storage type of clip output depends on storage types of inputs and the  min ,  max  parameter values:   clip(default) = default  clip(row_sparse, min  = 0, max  = 0) = row_sparse  clip(csr, min  = 0, max  = 0) = csr  clip(row_sparse, min   0, max   0) = default  clip(row_sparse, min   0, max   0) = default  clip(csr, min   0, max   0) = csr  clip(csr, min   0, max   0) = csr   Defined in src/operator/tensor/matrix_op.cc:L486  source  #  MXNet.mx.context     Method .  context(arr::NDArray)  Get the context that this  NDArray  lives on.  source  #  MXNet.mx.empty     Method .  empty(dims::Tuple[, ctx::Context = cpu()])\nempty(dim1, dim2, ...)  Allocate memory for an uninitialized  NDArray  with specific shape of type Float32.  source  #  MXNet.mx.empty     Method .  empty(DType, dims[, ctx::Context = cpu()])\nempty(DType, dims)\nempty(DType, dim1, dim2, ...)  Allocate memory for an uninitialized  NDArray  with a specified type.  source  #  MXNet.mx.expand_dims     Method .  expand_dims(x::NDArray, dim)  Insert a new axis into  dim .  julia  x\n4 mx.NDArray{Float64,1} @ CPU0:\n 1.0\n 2.0\n 3.0\n 4.0\n\njulia  mx.expand_dims(x, 1)\n1\u00d74 mx.NDArray{Float64,2} @ CPU0:\n 1.0  2.0  3.0  4.0\n\njulia  mx.expand_dims(x, 2)\n4\u00d71 mx.NDArray{Float64,2} @ CPU0:\n 1.0\n 2.0\n 3.0\n 4.0  Defined in src/operator/tensor/matrix_op.cc:L289  source  #  MXNet.mx.log_softmax     Function .  log_softmax.(x::NDArray, [dim = ndims(x)])  Computes the log softmax of the input. This is equivalent to computing softmax followed by log.  julia  x 2\u00d73 mx.NDArray{Float64,2} @ CPU0:  1.0  2.0  0.1  0.1  2.0  1.0  julia  mx.log_softmax.(x) 2\u00d73 mx.NDArray{Float64,2} @ CPU0:  -1.41703  -0.41703  -2.31703  -2.31703  -0.41703  -1.41703  source  #  MXNet.mx.relu     Function .  relu.(x::NDArray)  Computes rectified linear.   \n\\max(x, 0)   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L84  source  #  MXNet.mx.sigmoid     Function .  \u03c3.(x::NDArray)\nsigmoid.(x::NDArray)  Computes sigmoid of x element-wise.   \n\u03c3(x) = \\frac{1}{(1 + exp(-x))}   The storage type of  sigmoid  output is always dense.  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L103  source  #  MXNet.mx.softmax     Function .  softmax.(x::NDArray, [dim = ndims(x)])  Applies the softmax function.  The resulting array contains elements in the range  (0, 1)  and the elements along the given axis sum up to 1.   \nsoftmax(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}   Defined in src/operator/nn/softmax.cc:L54  source  #  MXNet.mx.\u03c3     Function .  \u03c3.(x::NDArray)\nsigmoid.(x::NDArray)  Computes sigmoid of x element-wise.   \n\u03c3(x) = \\frac{1}{(1 + exp(-x))}   The storage type of  sigmoid  output is always dense.  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L103  source  #  MXNet.mx.@inplace     Macro .  @inplace  Julia does not support re-definiton of  +=  operator (like  __iadd__  in python), When one write  a += b , it gets translated to  a = a+b .  a+b  will allocate new memory for the results, and the newly allocated  NDArray  object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.  This macro is a simple utility to implement this behavior. Write    @mx.inplace a += b  will translate into    mx.add_to!(a, b)  which will do inplace adding of the contents of  b  into  a .  source  #  Base.Iterators.Flatten     Method .  Flatten(data)  Flattens the input array into a 2-D array by collapsing the higher dimensions.  .. note::  Flatten  is deprecated. Use  flatten  instead.  For an input array with shape $(d1, d2, ..., dk)$,  flatten  operation reshapes the input array into an output array of shape $(d1, d2 ... dk)$.  Example::  x = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]  Defined in src/operator/tensor/matrix_op.cc:L208  Arguments   data::NDArray-or-SymbolicNode : Input array.   source  #  Base.:%     Method .  .%(x::NDArray, y::NDArray)\n.%(x::NDArray, y::Real)\n.%(x::Real, y::NDArray)  Elementwise modulo for  NDArray .  source  #  Base.:*     Method .  .*(x, y)  Elementwise multiplication for  NDArray .  source  #  Base.:*     Method .  *(A::NDArray, B::NDArray)  Matrix/tensor multiplication.  source  #  Base.:+     Method .  +(args...)\n.+(args...)  Summation. Multiple arguments of either scalar or  NDArray  could be added together. Note at least the first or second argument needs to be an  NDArray  to avoid ambiguity of built-in summation.  source  #  Base.:-     Method .  -(x::NDArray)\n-(x, y)\n.-(x, y)  Subtraction  x - y , of scalar types or  NDArray . Or create the negative of  x .  source  #  Base.:/     Method .  ./(x::NDArray, y::NDArray)\n./(x::NDArray, y::Real)\n./(x::Real, y::NDArray)   Elementwise dividing an  NDArray  by a scalar or another  NDArray   of the same shape.   Elementwise divide a scalar by an  NDArray .  Matrix division (solving linear systems) is not implemented yet.   source  #  Base.LinAlg.dot     Method .  dot(x::NDArray, y::NDArray)  Defined in src/operator/tensor/dot.cc:L62  source  #  Base.LinAlg.norm     Method .  norm(data)  Flattens the input array and then computes the l2 norm.  Examples::  x = [[1, 2],        [3, 4]]  norm(x) = [5.47722578]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L257  Arguments   data::NDArray-or-SymbolicNode : Source input   source  #  Base.Math.cbrt     Method .  cbrt(data)  Returns element-wise cube-root value of the input.  .. math::    cbrt(x) = \\sqrt[3]{x}  Example::  cbrt([1, 8, -125]) = [1, 2, -5]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L602  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.Math.gamma     Method .  gamma(data)  Returns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.  The storage type of $gamma$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base._div     Method .  _div(lhs, rhs)  _div is an alias of elemwise_div.  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  Base._sub     Method .  _sub(lhs, rhs)  _sub is an alias of elemwise_sub.  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  Base.abs     Method .  abs(data)  Returns element-wise absolute value of the input.  Example::  abs([-2, 0, 3]) = [2, 0, 3]  The storage type of $abs$ output depends upon the input storage type:   abs(default) = default  abs(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L386  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.acos     Function .  acos.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L123  source  #  Base.acosh     Function .  acosh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L264  source  #  Base.asin     Function .  asin.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L104  source  #  Base.asinh     Function .  asinh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L250  source  #  Base.atan     Function .  atan.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L144  source  #  Base.atanh     Function .  atanh.(x::NDArray)Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L281  source  #  Base.cat     Method .  cat(dim, xs::NDArray...)  Concate the  NDArray s which have the same element type along the  dim . Building a diagonal matrix is not supported yet.  source  #  Base.ceil     Method .  ceil(data)  Returns element-wise ceiling of the input.  The ceil of the scalar x is the smallest integer i, such that i  = x.  Example::  ceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]  The storage type of $ceil$ output depends upon the input storage type:   ceil(default) = default  ceil(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L464  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.convert     Method .  convert(::Type{Array{ :Real}}, x::NDArray)  Convert an  NDArray  into a Julia  Array  of specific type. Data will be copied.  source  #  Base.copy!     Method .  copy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})  Copy contents of  src  into  dst .  source  #  Base.copy     Method .  copy(arr :: NDArray)\ncopy(arr :: NDArray, ctx :: Context)\ncopy(arr :: Array, ctx :: Context)  Create a copy of an array. When no  Context  is given, create a Julia  Array . Otherwise, create an  NDArray  on the specified context.  source  #  Base.deepcopy     Method .  deepcopy(arr::NDArray)  Get a deep copy of the data blob in the form of an NDArray of default storage type. This function blocks. Do not use it in performance critical code.  source  #  Base.eltype     Method .  eltype(x::NDArray)  Get the element type of an  NDArray .  source  #  Base.exp     Method .  exp(data)  Returns element-wise exponential value of the input.  .. math::    exp(x) = e^x \\approx 2.718^x  Example::  exp([0, 1, 2]) = [1., 2.71828175, 7.38905621]  The storage type of $exp$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L642  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.expm1     Method .  expm1(data)  Returns $exp(x) - 1$ computed element-wise on the input.  This function provides greater precision than $exp(x) - 1$ for small values of $x$.  The storage type of $expm1$ output depends upon the input storage type:   expm1(default) = default  expm1(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L721  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.fill!     Method .  fill!(arr::NDArray, x)  Create an  NDArray  filled with the value  x , like  Base.fill! .  source  #  Base.floor     Method .  floor(data)  Returns element-wise floor of the input.  The floor of the scalar x is the largest integer i, such that i  = x.  Example::  floor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]  The storage type of $floor$ output depends upon the input storage type:   floor(default) = default  floor(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L483  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.getindex     Method .  getindex(arr::NDArray, idx)  Shortcut for  slice . A typical use is to write    arr[:] += 5  which translates into    arr[:] = arr[:] + 5  which furthur translates into    setindex!(getindex(arr, Colon()), 5, Colon())   Note  The behavior is quite different from indexing into Julia's  Array . For example,  arr[2:5]  create a  copy  of the sub-array for Julia  Array , while for  NDArray , this is a  slice  that shares the memory.   source  #  Base.getindex     Method .  Shortcut for  slice .  NOTE  the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call  slice , which shares the underlying memory.  source  #  Base.hcat     Method .  hcat(x::NDArray...)  source  #  Base.identity     Method .  identity(data)  identity is an alias of _copy.  Returns a copy of the input.  From:src/operator/tensor/elemwise_unary_op_basic.cc:112  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.length     Method .  length(x::NDArray)  Get the number of elements in an  NDArray .  source  #  Base.log     Method .  log(data)  Returns element-wise Natural logarithmic value of the input.  The natural logarithm is logarithm in base  e , so that $log(exp(x)) = x$  The storage type of $log$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L654  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.log10     Method .  log10(data)  Returns element-wise Base-10 logarithmic value of the input.  $10**log10(x) = x$  The storage type of $log10$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L666  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.log1p     Method .  log1p(data)  Returns element-wise $log(1 + x)$ value of the input.  This function is more accurate than $log(1 + x)$  for small $x$ so that :math: 1+x\\approx 1  The storage type of $log1p$ output depends upon the input storage type:   log1p(default) = default  log1p(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L703  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.log2     Method .  log2(data)  Returns element-wise Base-2 logarithmic value of the input.  $2**log2(x) = x$  The storage type of $log2$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L678  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.maximum     Method .  maximum(arr::NDArray, dims)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L160  source  #  Base.maximum     Method .  maximum(arr::NDArray)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L160  source  #  Base.mean     Method .  mean(arr::NDArray, region)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L101  source  #  Base.mean     Method .  mean(arr::NDArray)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L101  source  #  Base.minimum     Method .  minimum(arr::NDArray, dims)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L174  source  #  Base.minimum     Method .  minimum(arr::NDArray)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L174  source  #  Base.ndims     Method .  ndims(x::NDArray)  Get the number of dimensions of an  NDArray . Is equivalent to  length(size(arr)) .  source  #  Base.permutedims     Method .  permutedims(arr::NDArray, axes)  Defined in src/operator/tensor/matrix_op.cc:L253  source  #  Base.prod     Method .  prod(arr::NDArray, dims)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L116  source  #  Base.prod     Method .  prod(arr::NDArray)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L116  source  #  Base.repeat     Method .  repeat(data, repeats, axis)  Repeats elements of an array.  By default, $repeat$ flattens the input array into 1-D and then repeats the elements::  x = [[ 1, 2],        [ 3, 4]]  repeat(x, repeats=2) = [ 1.,  1.,  2.,  2.,  3.,  3.,  4.,  4.]  The parameter $axis$ specifies the axis along which to perform repeat::  repeat(x, repeats=2, axis=1) = [[ 1.,  1.,  2.,  2.],                                   [ 3.,  3.,  4.,  4.]]  repeat(x, repeats=2, axis=0) = [[ 1.,  2.],                                   [ 1.,  2.],                                   [ 3.,  4.],                                   [ 3.,  4.]]  repeat(x, repeats=2, axis=-1) = [[ 1.,  1.,  2.,  2.],                                    [ 3.,  3.,  4.,  4.]]  Defined in src/operator/tensor/matrix_op.cc:L560  Arguments   data::NDArray-or-SymbolicNode : Input data array  repeats::int, required : The number of repetitions for each element.  axis::int or None, optional, default='None' : The axis along which to repeat values. The negative numbers are interpreted counting from the backward. By default, use the flattened input array, and return a flat output array.   source  #  Base.reverse     Method .  reverse(data, axis)  Reverses the order of elements along given axis while preserving array shape.  Note: reverse and flip are equivalent. We use reverse in the following examples.  Examples::  x = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]  reverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]  reverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]  Defined in src/operator/tensor/matrix_op.cc:L662  Arguments   data::NDArray-or-SymbolicNode : Input data array  axis::Shape(tuple), required : The axis which to reverse elements.   source  #  Base.round     Method .  round(data)  Returns element-wise rounded value to the nearest integer of the input.  Example::  round([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]  The storage type of $round$ output depends upon the input storage type:   round(default) = default  round(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L424  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.setindex!     Method .  setindex!(arr::NDArray, val, idx)  Assign values to an  NDArray . The following scenarios are supported   single value assignment via linear indexing:  arr[42] = 24  arr[:] = val : whole array assignment,  val  could be a scalar or an array (Julia  Array  or  NDArray ) of the same shape.  arr[start:stop] = val : assignment to a  slice ,  val  could be a scalar or an array of the same shape to the slice. See also  slice .   source  #  Base.sign     Method .  sign(data)  Returns element-wise sign of the input.  Example::  sign([-2, 0, 3]) = [-1, 0, 1]  The storage type of $sign$ output depends upon the input storage type:   sign(default) = default  sign(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L405  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.similar     Method .  similar(x::NDArray)  Create an  NDArray  with similar shape, data type, and context with the given one. Note that the returned  NDArray  is uninitialized.  source  #  Base.size     Method .  size(x::NDArray)\nsize(x::NDArray, dims...)  Get the shape of an  NDArray . The shape is in Julia's column-major convention. See also the notes on NDArray shapes  NDArray .  source  #  Base.sort     Method .  sort(data, axis, is_ascend)  Returns a sorted copy of an input array along the given axis.  Examples::  x = [[ 1, 4],        [ 3, 1]]  // sorts along the last axis   sort(x) = [[ 1.,  4.],              [ 1.,  3.]]  // flattens and then sorts   sort(x) = [ 1.,  1.,  3.,  4.]  // sorts along the first axis   sort(x, axis=0) = [[ 1.,  1.],                      [ 3.,  4.]]  // in a descend order   sort(x, is_ascend=0) = [[ 4.,  1.],                           [ 3.,  1.]]  Defined in src/operator/tensor/ordering_op.cc:L126  Arguments   data::NDArray-or-SymbolicNode : The input array  axis::int or None, optional, default='-1' : Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=1 : Whether to sort in ascending or descending order.   source  #  Base.split     Method .  split(data, num_outputs, axis, squeeze_axis)  split is an alias of SliceChannel.  Splits an array along a particular axis into multiple sub-arrays.  .. note:: $SliceChannel$ is deprecated. Use $split$ instead.  Note  that  num_outputs  should evenly divide the length of the axis along which to split the array.  Example::  x  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)  y = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]     [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]  y[0].shape = (3, 1, 1)  z = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]     [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]  z[0].shape = (1, 2, 1)  squeeze_axis=1  removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $1$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to true only if $input.shape[axis] == num_outputs$.  Example::  z = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]     [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]  z[0].shape = (2 ,1 )  Defined in src/operator/slice_channel.cc:L107  Arguments   data::NDArray-or-SymbolicNode : The input  num_outputs::int, required : Number of splits. Note that this should evenly divide the length of the  axis .  axis::int, optional, default='1' : Axis along which to split.  squeeze_axis::boolean, optional, default=0 : If true, Removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $true$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to $true$ only if $input.shape[axis] == num_outputs$.   source  #  Base.sqrt     Method .  sqrt(data)  Returns element-wise square-root value of the input.  .. math::    \\textrm{sqrt}(x) = \\sqrt{x}  Example::  sqrt([4, 9, 16]) = [2, 3, 4]  The storage type of $sqrt$ output depends upon the input storage type:   sqrt(default) = default  sqrt(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L565  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.sum     Method .  sum(arr::NDArray, dims)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L85  source  #  Base.sum     Method .  sum(arr::NDArray)  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L85  source  #  Base.transpose     Method .  transpose(arr::NDArray{T, 1}) where T  Defined in src/operator/tensor/matrix_op.cc:L164  source  #  Base.transpose     Method .  transpose(arr::NDArray{T, 2}) where T  Defined in src/operator/tensor/matrix_op.cc:L253  source  #  Base.trunc     Method .  trunc(data)  Return the element-wise truncated value of the input.  The truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.  Example::  trunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]  The storage type of $trunc$ output depends upon the input storage type:   trunc(default) = default  trunc(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L503  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  Base.vcat     Method .  vcat(x::NDArray...)  source  #  MXNet.mx.Activation     Method .  Activation(data, act_type)  Applies an activation function element-wise to the input.  The following activation functions are supported:   relu : Rectified Linear Unit, :math: y = max(x, 0)  sigmoid : :math: y = \\frac{1}{1 + exp(-x)}  tanh : Hyperbolic tangent, :math: y = \\frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}  softrelu : Soft ReLU, or SoftPlus, :math: y = log(1 + exp(x))   Defined in src/operator/nn/activation.cc:L92  Arguments   data::NDArray-or-SymbolicNode : Input array to activation function.  act_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required : Activation function to be applied.   source  #  MXNet.mx.BatchNorm     Method .  BatchNorm(data, gamma, beta, moving_mean, moving_var, eps, momentum, fix_gamma, use_global_stats, output_mean_var, axis, cudnn_off)  Batch normalization.  Normalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.  Assume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:  .. math::  data_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])  Then compute the normalized output, which has the same shape as input, as following:  .. math::  out[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]  Both  mean  and  var  returns a scalar by treating the input as a vector.  Assume the input has size  k  on axis 1, then both $gamma$ and $beta$ have shape  (k,) . If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.  Besides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are  k -length vectors. They are global statistics for the whole dataset, which are updated by::  moving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)  If $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.  The parameter $axis$ specifies which axis of the input shape denotes the 'channel' (separately normalized groups).  The default is 1.  Specifying -1 sets the channel axis to be the last item in the input shape.  Both $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.  Defined in src/operator/nn/batch_norm.cc:L400  Arguments   data::NDArray-or-SymbolicNode : Input data to batch normalization  gamma::NDArray-or-SymbolicNode : gamma array  beta::NDArray-or-SymbolicNode : beta array  moving_mean::NDArray-or-SymbolicNode : running mean of input  moving_var::NDArray-or-SymbolicNode : running variance of input  eps::double, optional, default=0.001 : Epsilon to prevent div 0. Must be no less than CUDNN_BN_MIN_EPSILON defined in cudnn.h when using cudnn (usually 1e-5)  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=1 : Fix gamma while training  use_global_stats::boolean, optional, default=0 : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.  output_mean_var::boolean, optional, default=0 : Output All,normal mean and var  axis::int, optional, default='1' : Specify which shape axis the channel is specified  cudnn_off::boolean, optional, default=0 : Do not select CUDNN operator, if available   source  #  MXNet.mx.BatchNorm_v1     Method .  BatchNorm_v1(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)  Batch normalization.  Normalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.  Assume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:  .. math::  data_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])  Then compute the normalized output, which has the same shape as input, as following:  .. math::  out[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]  Both  mean  and  var  returns a scalar by treating the input as a vector.  Assume the input has size  k  on axis 1, then both $gamma$ and $beta$ have shape  (k,) . If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.  Besides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are  k -length vectors. They are global statistics for the whole dataset, which are updated by::  moving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)  If $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.  Both $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.  Defined in src/operator/batch_norm_v1.cc:L90  Arguments   data::NDArray-or-SymbolicNode : Input data to batch normalization  gamma::NDArray-or-SymbolicNode : gamma array  beta::NDArray-or-SymbolicNode : beta array  eps::float, optional, default=0.001 : Epsilon to prevent div 0  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=1 : Fix gamma while training  use_global_stats::boolean, optional, default=0 : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.  output_mean_var::boolean, optional, default=0 : Output All,normal mean and var   source  #  MXNet.mx.BilinearSampler     Method .  BilinearSampler(data, grid)  Applies bilinear sampling to input feature map.  Bilinear Sampling is the key of  [NIPS2015] \\\"Spatial Transformer Networks\\\". The usage of the operator is very similar to remap function in OpenCV, except that the operator has the backward pass.  Given :math: data  and :math: grid , then the output is computed by  .. math::   x_{src} = grid[batch, 0, y_{dst}, x_{dst}] \\\n  y_{src} = grid[batch, 1, y_{dst}, x_{dst}] \\\n  output[batch, channel, y_{dst}, x_{dst}] = G(data[batch, channel, y_{src}, x_{src})  :math: x_{dst} , :math: y_{dst}  enumerate all spatial locations in :math: output , and :math: G()  denotes the bilinear interpolation kernel. The out-boundary points will be padded with zeros.The shape of the output will be (data.shape[0], data.shape[1], grid.shape[2], grid.shape[3]).  The operator assumes that :math: data  has 'NCHW' layout and :math: grid  has been normalized to [-1, 1].  BilinearSampler often cooperates with GridGenerator which generates sampling grids for BilinearSampler. GridGenerator supports two kinds of transformation: $affine$ and $warp$. If users want to design a CustomOp to manipulate :math: grid , please firstly refer to the code of GridGenerator.  Example 1::  Zoom out data two times  data = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])  affine_matrix = array([[2, 0, 0],                          [0, 2, 0]])  affine_matrix = reshape(affine_matrix, shape=(1, 6))  grid = GridGenerator(data=affine_matrix, transform_type='affine', target_shape=(4, 4))  out = BilinearSampler(data, grid)  out   [[[[ 0,   0,     0,   0],      [ 0,   3.5,   6.5, 0],      [ 0,   1.25,  2.5, 0],      [ 0,   0,     0,   0]]]  Example 2::  shift data horizontally by -1 pixel  data = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])  warp_maxtrix = array([[[[1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1]],                          [[0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0]]]])  grid = GridGenerator(data=warp_matrix, transform_type='warp')   out = BilinearSampler(data, grid)  out   [[[[ 4,  3,  6,  0],      [ 8,  8,  9,  0],      [ 4,  1,  5,  0],      [ 0,  1,  3,  0]]]  Defined in src/operator/bilinear_sampler.cc:L245  Arguments   data::NDArray-or-SymbolicNode : Input data to the BilinearsamplerOp.  grid::NDArray-or-SymbolicNode : Input grid to the BilinearsamplerOp.grid has two channels: x_src, y_src   source  #  MXNet.mx.BlockGrad     Method .  BlockGrad(data)  Stops gradient computation.  Stops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.  Example::  v1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)  executor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]  executor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L167  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.Cast     Method .  Cast(data, dtype)  Casts all elements of the input to a new type.  .. note:: $Cast$ is deprecated. Use $cast$ instead.  Example::  cast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L311  Arguments   data::NDArray-or-SymbolicNode : The input.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Output data type.   source  #  MXNet.mx.Concat     Method .  Concat(data, num_args, dim)  Note : Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.  Joins input arrays along a given axis.  .. note::  Concat  is deprecated. Use  concat  instead.  The dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.  Example::  x = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]  concat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]  Note that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.  concat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]  Defined in src/operator/concat.cc:L104  Arguments   data::NDArray-or-SymbolicNode[] : List of arrays to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.   source  #  MXNet.mx.Convolution     Method .  Convolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  Compute  N -D convolution on  (N+2) -D input.  In the 2-D convolution, given input data with shape  (batch_size, channel, height, width) , the output is computed by  .. math::  out[n,i,:,:] = bias[i] + \\sum_{j=0}^{channel} data[n,j,:,:] \\star    weight[i,j,:,:]  where :math: \\star  is the 2-D cross-correlation operator.  For general 2-D convolution, the shapes are   data :  (batch_size, channel, height, width)  weight :  (num_filter, channel, kernel[0], kernel[1])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_height, out_width) .   Define::  f(x,k,p,s,d) = floor((x+2 p-d (k-1)-1)/s)+1  then we have::  out_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])  If $no_bias$ is set to be true, then the $bias$ term is ignored.  The default data $layout$ is  NCHW , namely  (batch_size, channel, height, width) . We can choose other layouts such as  NHWC .  If $num_group$ is larger than 1, denoted by  g , then split the input $data$ evenly into  g  parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the  i -th part of the data with the  i -th weight part. The output is obtained by concatenating all the  g  results.  1-D convolution does not have  height  dimension but only  width  in space.   data :  (batch_size, channel, width)  weight :  (num_filter, channel, kernel[0])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_width) .   3-D convolution adds an additional  depth  dimension besides  height  and  width . The shapes are   data :  (batch_size, channel, depth, height, width)  weight :  (num_filter, channel, kernel[0], kernel[1], kernel[2])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_depth, out_height, out_width) .   Both $weight$ and $bias$ are learnable parameters.  There are other options to tune the performance.    cudnn_tune : enable this option leads to higher startup time but may give faster speed. Options are   off : no tuning  limited_workspace :run test and pick the fastest algorithm that doesn't exceed workspace limit.  fastest : pick the fastest algorithm and ignore workspace limit.  None  (default): the behavior is determined by environment variable $MXNET_CUDNN_AUTOTUNE_DEFAULT$. 0 for off, 1 for limited workspace (default), 2 for fastest.  workspace : A large number leads to more (GPU) memory usage but may improve the performance.     Defined in src/operator/nn/convolution.cc:L170  Arguments   data::NDArray-or-SymbolicNode : Input data to the ConvolutionOp.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : Convolution kernel size: (w,), (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=[] : Convolution stride: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  dilate::Shape(tuple), optional, default=[] : Convolution dilate: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : Zero pad for convolution: (w,), (h, w) or (d, h, w). Defaults to no padding.  num_filter::int (non-negative), required : Convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions.  workspace::long (non-negative), optional, default=1024 : Maximum temporary workspace allowed for convolution (MB).  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algo by running performance test.  cudnn_off::boolean, optional, default=0 : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.   source  #  MXNet.mx.Convolution_v1     Method .  Convolution_v1(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  This operator is DEPRECATED. Apply convolution to input then add a bias.  Arguments   data::NDArray-or-SymbolicNode : Input data to the ConvolutionV1Op.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : convolution kernel size: (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=[] : convolution stride: (h, w) or (d, h, w)  dilate::Shape(tuple), optional, default=[] : convolution dilate: (h, w) or (d, h, w)  pad::Shape(tuple), optional, default=[] : pad for convolution: (h, w) or (d, h, w)  num_filter::int (non-negative), required : convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results  workspace::long (non-negative), optional, default=1024 : Maximum tmp workspace allowed for convolution (MB).  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.  cudnn_off::boolean, optional, default=0 : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.   source  #  MXNet.mx.Correlation     Method .  Correlation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)  Applies correlation to inputs.  The correlation layer performs multiplicative patch comparisons between two feature maps.  Given two multi-channel feature maps :math: f_{1}, f_{2} , with :math: w , :math: h , and :math: c  being their width, height, and number of channels, the correlation layer lets the network compare each patch from :math: f_{1}  with each patch from :math: f_{2} .  For now we consider only a single comparison of two patches. The 'correlation' of two patches centered at :math: x_{1}  in the first map and :math: x_{2}  in the second map is then defined as:  .. math::    c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k] \\times [-k,k]}   for a square patch of size :math: K:=2k+1 .  Note that the equation above is identical to one step of a convolution in neural networks, but instead of convolving data with a filter, it convolves data with other data. For this reason, it has no training weights.  Computing :math: c(x_{1}, x_{2})  involves :math: c * K^{2}  multiplications. Comparing all patch combinations involves :math: w^{2}*h^{2}  such computations.  Given a maximum displacement :math: d , for each location :math: x_{1}  it computes correlations :math: c(x_{1}, x_{2})  only in a neighborhood of size :math: D:=2d+1 , by limiting the range of :math: x_{2} . We use strides :math: s_{1}, s_{2} , to quantize :math: x_{1}  globally and to quantize :math: x_{2}  within the neighborhood centered around :math: x_{1} .  The final output is defined by the following expression:  .. math::   out[n, q, i, j] = c(x_{i, j}, x_{q})  where :math: i  and :math: j  enumerate spatial locations in :math: f_{1} , and :math: q  denotes the :math: q^{th}  neighborhood of :math: x_{i,j} .  Defined in src/operator/correlation.cc:L192  Arguments   data1::NDArray-or-SymbolicNode : Input data1 to the correlation.  data2::NDArray-or-SymbolicNode : Input data2 to the correlation.  kernel_size::int (non-negative), optional, default=1 : kernel size for Correlation must be an odd number  max_displacement::int (non-negative), optional, default=1 : Max displacement of Correlation  stride1::int (non-negative), optional, default=1 : stride1 quantize data1 globally  stride2::int (non-negative), optional, default=1 : stride2 quantize data2 within the neighborhood centered around data1  pad_size::int (non-negative), optional, default=0 : pad for Correlation  is_multiply::boolean, optional, default=1 : operation type is either multiplication or subduction   source  #  MXNet.mx.Crop     Method .  Crop(data, num_args, offset, h_w, center_crop)  Note : Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.  .. note::  Crop  is deprecated. Use  slice  instead.  Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used  Defined in src/operator/crop.cc:L50  Arguments   data::SymbolicNode or SymbolicNode[] : Tensor or List of Tensors, the second input will be used as crop_like shape reference  num_args::int, required : Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here  offset::Shape(tuple), optional, default=[0,0] : crop offset coordinate: (y, x)  h_w::Shape(tuple), optional, default=[0,0] : crop height and width: (h, w)  center_crop::boolean, optional, default=0 : If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like   source  #  MXNet.mx.Custom     Method .  Custom(data, op_type)  Apply a custom operator implemented in a frontend language (like Python).  Custom operators should override required methods like  forward  and  backward . The custom operator must be registered before it can be used. Please check the tutorial here: http://mxnet.io/how_to/new_op.html.  Defined in src/operator/custom/custom.cc:L378  Arguments   data::NDArray-or-SymbolicNode[] : Input data for the custom operator.  op_type::string : Name of the custom operator. This is the name that is passed to  mx.operator.register  to register the operator.   source  #  MXNet.mx.Deconvolution     Method .  Deconvolution(data, weight, bias, kernel, stride, dilate, pad, adj, target_shape, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  Computes 1D or 2D transposed convolution (aka fractionally strided convolution) of the input tensor. This operation can be seen as the gradient of Convolution operation with respect to its input. Convolution usually reduces the size of the input. Transposed convolution works the other way, going from a smaller input to a larger output while preserving the connectivity pattern.  Arguments   data::NDArray-or-SymbolicNode : Input tensor to the deconvolution operation.  weight::NDArray-or-SymbolicNode : Weights representing the kernel.  bias::NDArray-or-SymbolicNode : Bias added to the result after the deconvolution operation.  kernel::Shape(tuple), required : Deconvolution kernel size: (w,), (h, w) or (d, h, w). This is same as the kernel size used for the corresponding convolution  stride::Shape(tuple), optional, default=[] : The stride used for the corresponding convolution: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  dilate::Shape(tuple), optional, default=[] : Dilation factor for each dimension of the input: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : The amount of implicit zero padding added during convolution for each dimension of the input: (w,), (h, w) or (d, h, w). $(kernel-1)/2$ is usually a good choice. If  target_shape  is set,  pad  will be ignored and a padding that will generate the target shape will be used. Defaults to no padding.  adj::Shape(tuple), optional, default=[] : Adjustment for output shape: (w,), (h, w) or (d, h, w). If  target_shape  is set,  adj  will be ignored and computed accordingly.  target_shape::Shape(tuple), optional, default=[] : Shape of the output tensor: (w,), (h, w) or (d, h, w).  num_filter::int (non-negative), required : Number of output filters.  num_group::int (non-negative), optional, default=1 : Number of groups partition.  workspace::long (non-negative), optional, default=512 : Maximum temporal workspace allowed for deconvolution (MB).  no_bias::boolean, optional, default=1 : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algorithm by running performance test.  cudnn_off::boolean, optional, default=0 : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for default layout, NCW for 1d, NCHW for 2d and NCDHW for 3d.   source  #  MXNet.mx.Dropout     Method .  Dropout(data, p, mode)  Applies dropout operation to input array.   During training, each element of the input is set to zero with probability p. The whole array is rescaled by :math: 1/(1-p)  to keep the expected sum of the input unchanged.  During testing, this operator does not change the input if mode is 'training'. If mode is 'always', the same computaion as during training will be applied.   Example::  random.seed(998)   input_array = array([[3., 0.5,  -0.5,  2., 7.],                       [2., -0.4,   7.,  3., 0.2]])   a = symbol.Variable('a')   dropout = symbol.Dropout(a, p = 0.2)   executor = dropout.simple_bind(a = input_array.shape)  If training  executor.forward(is_train = True, a = input_array)   executor.outputs   [[ 3.75   0.625 -0.     2.5    8.75 ]    [ 2.5   -0.5    8.75   3.75   0.   ]]  If testing  executor.forward(is_train = False, a = input_array)   executor.outputs   [[ 3.     0.5   -0.5    2.     7.   ]    [ 2.    -0.4    7.     3.     0.2  ]]  Defined in src/operator/nn/dropout.cc:L78  Arguments   data::NDArray-or-SymbolicNode : Input array to which dropout will be applied.  p::float, optional, default=0.5 : Fraction of the input that gets dropped out during training time.  mode::{'always', 'training'},optional, default='training' : Whether to only turn on dropout during training or to also turn on for inference.   source  #  MXNet.mx.ElementWiseSum     Method .  ElementWiseSum(args)  ElementWiseSum is an alias of add_n.  Note : ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments   source  #  MXNet.mx.Embedding     Method .  Embedding(data, weight, input_dim, output_dim, dtype)  Maps integer indices to vector representations (embeddings).  This operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.  For an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).  If the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).  By default, if any index mentioned is too large, it is replaced by the index that addresses the last vector in an embedding matrix.  Examples::  input_dim = 4   output_dim = 5  // Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]  // Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]  // Mapped input x to its vector representation y.   Embedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                             [ 15.,  16.,  17.,  18.,  19.]],                         [[  0.,   1.,   2.,   3.,   4.],\n                        [ 10.,  11.,  12.,  13.,  14.]]]  Defined in src/operator/tensor/indexing_op.cc:L185  Arguments   data::NDArray-or-SymbolicNode : The input array to the embedding operator.  weight::NDArray-or-SymbolicNode : The embedding weight matrix.  input_dim::int, required : Vocabulary size of the input indices.  output_dim::int, required : Dimension of the embedding vectors.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Data type of weight.   source  #  MXNet.mx.FullyConnected     Method .  FullyConnected(data, weight, bias, num_hidden, no_bias, flatten)  Applies a linear transformation: :math: Y = XW^T + b .  If $flatten$ is set to be true, then the shapes are:   data :  (batch_size, x1, x2, ..., xn)  weight :  (num_hidden, x1 * x2 * ... * xn)  bias :  (num_hidden,)  out :  (batch_size, num_hidden)   If $flatten$ is set to be false, then the shapes are:   data :  (x1, x2, ..., xn, input_dim)  weight :  (num_hidden, input_dim)  bias :  (num_hidden,)  out :  (x1, x2, ..., xn, num_hidden)   The learnable parameters include both $weight$ and $bias$.  If $no_bias$ is set to be true, then the $bias$ term is ignored.  Defined in src/operator/nn/fully_connected.cc:L98  Arguments   data::NDArray-or-SymbolicNode : Input data.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  num_hidden::int, required : Number of hidden nodes of the output.  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  flatten::boolean, optional, default=1 : Whether to collapse all but the first axis of the input data tensor.   source  #  MXNet.mx.GridGenerator     Method .  GridGenerator(data, transform_type, target_shape)  Generates 2D sampling grid for bilinear sampling.  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  transform_type::{'affine', 'warp'}, required : The type of transformation. For  affine , input data should be an affine matrix of size (batch, 6). For  warp , input data should be an optical flow of size (batch, 2, h, w).  target_shape::Shape(tuple), optional, default=[0,0] : Specifies the output shape (H, W). This is required if transformation type is  affine . If transformation type is  warp , this parameter is ignored.   source  #  MXNet.mx.IdentityAttachKLSparseReg     Method .  IdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)  Apply a sparse regularization to the output a sigmoid activation function.  Arguments   data::NDArray-or-SymbolicNode : Input data.  sparseness_target::float, optional, default=0.1 : The sparseness target  penalty::float, optional, default=0.001 : The tradeoff parameter for the sparseness penalty  momentum::float, optional, default=0.9 : The momentum for running average   source  #  MXNet.mx.InstanceNorm     Method .  InstanceNorm(data, gamma, beta, eps)  Applies instance normalization to the n-dimensional input array.  This operator takes an n-dimensional input array where (n 2) and normalizes the input using the following formula:  .. math::  out = \\frac{x - mean[data]}{ \\sqrt{Var[data]} + \\epsilon} * gamma + beta  This layer is similar to batch normalization layer ( BatchNorm ) with two differences: first, the normalization is carried out per example (instance), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as  contrast normalization .  If the input data is of shape [batch, channel, spacial_dim1, spacial_dim2, ...],  gamma  and  beta  parameters must be vectors of shape [channel].  This implementation is based on paper:  .. [1] Instance Normalization: The Missing Ingredient for Fast Stylization,    D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2).  Examples::  // Input of shape (2,1,2)   x = [[[ 1.1,  2.2]],        [[ 3.3,  4.4]]]  // gamma parameter of length 1   gamma = [1.5]  // beta parameter of length 1   beta = [0.5]  // Instance normalization is calculated with the above formula   InstanceNorm(x,gamma,beta) = [[[-0.997527  ,  1.99752665]],                                 [[-0.99752653,  1.99752724]]]  Defined in src/operator/instance_norm.cc:L95  Arguments   data::NDArray-or-SymbolicNode : An n-dimensional input array (n   2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].  gamma::NDArray-or-SymbolicNode : A vector of length 'channel', which multiplies the normalized input.  beta::NDArray-or-SymbolicNode : A vector of length 'channel', which is added to the product of the normalized input and the weight.  eps::float, optional, default=0.001 : An  epsilon  parameter to prevent division by 0.   source  #  MXNet.mx.L2Normalization     Method .  L2Normalization(data, eps, mode)  Normalize the input array using the L2 norm.  For 1-D NDArray, it computes::  out = data / sqrt(sum(data ** 2) + eps)  For N-D NDArray, if the input array has shape (N, N, ..., N),  with $mode$ = $instance$, it normalizes each instance in the multidimensional array by its L2 norm.::  for i in 0...N     out[i,:,:,...,:] = data[i,:,:,...,:] / sqrt(sum(data[i,:,:,...,:] ** 2) + eps)  with $mode$ = $channel$, it normalizes each channel in the array by its L2 norm.::  for i in 0...N     out[:,i,:,...,:] = data[:,i,:,...,:] / sqrt(sum(data[:,i,:,...,:] ** 2) + eps)  with $mode$ = $spatial$, it normalizes the cross channel norm for each position in the array by its L2 norm.::  for dim in 2...N     for i in 0...N       out[.....,i,...] = take(out, indices=i, axis=dim) / sqrt(sum(take(out, indices=i, axis=dim) ** 2) + eps)           -dim-  Example::  x = [[[1,2],         [3,4]],        [[2,2],         [5,6]]]  L2Normalization(x, mode='instance')   =[[[ 0.18257418  0.36514837]      [ 0.54772252  0.73029673]]     [[ 0.24077171  0.24077171]      [ 0.60192931  0.72231513]]]  L2Normalization(x, mode='channel')   =[[[ 0.31622776  0.44721359]      [ 0.94868326  0.89442718]]     [[ 0.37139067  0.31622776]      [ 0.92847669  0.94868326]]]  L2Normalization(x, mode='spatial')   =[[[ 0.44721359  0.89442718]      [ 0.60000002  0.80000001]]     [[ 0.70710677  0.70710677]      [ 0.6401844   0.76822126]]]  Defined in src/operator/l2_normalization.cc:L93  Arguments   data::NDArray-or-SymbolicNode : Input array to normalize.  eps::float, optional, default=1e-10 : A small constant for numerical stability.  mode::{'channel', 'instance', 'spatial'},optional, default='instance' : Specify the dimension along which to compute L2 norm.   source  #  MXNet.mx.LRN     Method .  LRN(data, alpha, beta, knorm, nsize)  Applies local response normalization to the input.  The local response normalization layer performs \"lateral inhibition\" by normalizing over local input regions.  If :math: a_{x,y}^{i}  is the activity of a neuron computed by applying kernel :math: i  at position :math: (x, y)  and then applying the ReLU nonlinearity, the response-normalized activity :math: b_{x,y}^{i}  is given by the expression:  .. math::    b_{x,y}^{i} = \\frac{a_{x,y}^{i}}{\\Bigg({k + \\alpha \\sum_{j=max(0, i-\\frac{n}{2})}^{min(N-1, i+\\frac{n}{2})} (a_{x,y}^{j})^{2}}\\Bigg)^{\\beta}}  where the sum runs over :math: n  \"adjacent\" kernel maps at the same spatial position, and :math: N  is the total number of kernels in the layer.  Defined in src/operator/lrn.cc:L73  Arguments   data::NDArray-or-SymbolicNode : Input data.  alpha::float, optional, default=0.0001 : The variance scaling parameter :math: \u0007lpha  in the LRN expression.  beta::float, optional, default=0.75 : The power parameter :math: \beta  in the LRN expression.  knorm::float, optional, default=2 : The parameter :math: k  in the LRN expression.  nsize::int (non-negative), required : normalization window width in elements.   source  #  MXNet.mx.LeakyReLU     Method .  LeakyReLU(data, act_type, slope, lower_bound, upper_bound)  Applies Leaky rectified linear unit activation element-wise to the input.  Leaky ReLUs attempt to fix the \"dying ReLU\" problem by allowing a small  slope  when the input is negative and has a slope of one when input is positive.  The following modified ReLU Activation functions are supported:   elu : Exponential Linear Unit.  y = x   0 ? x : slope * (exp(x)-1)  leaky : Leaky ReLU.  y = x   0 ? x : slope * x  prelu : Parametric ReLU. This is same as  leaky  except that  slope  is learnt during training.  rrelu : Randomized ReLU. same as  leaky  but the  slope  is uniformly and randomly chosen from  [lower_bound, upper_bound)  for training, while fixed to be  (lower_bound+upper_bound)/2  for inference.   Defined in src/operator/leaky_relu.cc:L58  Arguments   data::NDArray-or-SymbolicNode : Input data to activation function.  act_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky' : Activation function to be applied.  slope::float, optional, default=0.25 : Init slope for the activation. (For leaky and elu only)  lower_bound::float, optional, default=0.125 : Lower bound of random slope. (For rrelu only)  upper_bound::float, optional, default=0.334 : Upper bound of random slope. (For rrelu only)   source  #  MXNet.mx.LinearRegressionOutput     Method .  LinearRegressionOutput(data, label, grad_scale)  Computes and optimizes for squared loss during backward propagation. Just outputs $data$ during forward propagation.  If :math: \\hat{y}_i  is the predicted value of the i-th sample, and :math: y_i  is the corresponding target value, then the squared loss estimated over :math: n  samples is defined as  :math: \\text{SquaredLoss}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left( y_i - \\hat{y}_i \\right)^2  .. note::    Use the LinearRegressionOutput as the final output layer of a net.  By default, gradients of this loss function are scaled by factor  1/n , where n is the number of training examples. The parameter  grad_scale  can be used to change this scale to  grad_scale/n .  Defined in src/operator/regression_output.cc:L70  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  label::NDArray-or-SymbolicNode : Input label to the function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   source  #  MXNet.mx.LogisticRegressionOutput     Method .  LogisticRegressionOutput(data, label, grad_scale)  Applies a logistic function to the input.  The logistic function, also known as the sigmoid function, is computed as :math: \\frac{1}{1+exp(-x)} .  Commonly, the sigmoid is used to squash the real-valued output of a linear model :math:wTx+b into the [0,1] range so that it can be interpreted as a probability. It is suitable for binary classification or probability prediction tasks.  .. note::    Use the LogisticRegressionOutput as the final output layer of a net.  By default, gradients of this loss function are scaled by factor  1/n , where n is the number of training examples. The parameter  grad_scale  can be used to change this scale to  grad_scale/n .  Defined in src/operator/regression_output.cc:L112  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  label::NDArray-or-SymbolicNode : Input label to the function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   source  #  MXNet.mx.MAERegressionOutput     Method .  MAERegressionOutput(data, label, grad_scale)  Computes mean absolute error of the input.  MAE is a risk metric corresponding to the expected value of the absolute error.  If :math: \\hat{y}_i  is the predicted value of the i-th sample, and :math: y_i  is the corresponding target value, then the mean absolute error (MAE) estimated over :math: n  samples is defined as  :math: \\text{MAE}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left| y_i - \\hat{y}_i \\right|  .. note::    Use the MAERegressionOutput as the final output layer of a net.  By default, gradients of this loss function are scaled by factor  1/n , where n is the number of training examples. The parameter  grad_scale  can be used to change this scale to  grad_scale/n .  Defined in src/operator/regression_output.cc:L91  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  label::NDArray-or-SymbolicNode : Input label to the function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   source  #  MXNet.mx.MakeLoss     Method .  MakeLoss(data, grad_scale, valid_thresh, normalization)  Make your own loss function in network construction.  This operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.  For example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::  cross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = MakeLoss(cross_entropy)  We will need to use $MakeLoss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.  In addition, we can give a scale to the loss by setting $grad_scale$, so that the gradient of the loss will be rescaled in the backpropagation.  .. note:: This operator should be used as a Symbol instead of NDArray.  Defined in src/operator/make_loss.cc:L71  Arguments   data::NDArray-or-SymbolicNode : Input array.  grad_scale::float, optional, default=1 : Gradient scale as a supplement to unary and binary operators  valid_thresh::float, optional, default=0 : clip each element in the array to 0 when it is less than $valid_thresh$. This is used when $normalization$ is set to $'valid'$.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If this is set to null, the output gradient will not be normalized. If this is set to batch, the output gradient will be divided by the batch size. If this is set to valid, the output gradient will be divided by the number of valid input elements.   source  #  MXNet.mx.Pad     Method .  Pad(data, mode, pad_width, constant_value)  Pads an input array with a constant or edge values of the array.  .. note::  Pad  is deprecated. Use  pad  instead.  .. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in  pad_width  to be zero.  This operation pads an input array with either a  constant_value  or edge values along each axis of the input array. The amount of padding is specified by  pad_width .  pad_width  is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The  pad_width  should be of length $2*N$ where $N$ is the number of dimensions of the array.  For dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.  Example::  x = [[[[  1.   2.   3.]           [  4.   5.   6.]]       [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]  pad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]  pad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]  Defined in src/operator/pad.cc:L766  Arguments   data::NDArray-or-SymbolicNode : An n-dimensional input array.  mode::{'constant', 'edge', 'reflect'}, required : Padding type to use. \"constant\" pads with  constant_value  \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.  pad_width::Shape(tuple), required : Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.  constant_value::double, optional, default=0 : The value used for padding when  mode  is \"constant\".   source  #  MXNet.mx.Pooling     Method .  Pooling(data, global_pool, cudnn_off, kernel, pool_type, pooling_convention, stride, pad)  Performs pooling on the input.  The shapes for 1-D pooling are   data :  (batch_size, channel, width) ,  out :  (batch_size, num_filter, out_width) .   The shapes for 2-D pooling are   data :  (batch_size, channel, height, width)   out :  (batch_size, num_filter, out_height, out_width) , with::  out_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])    The definition of  f  depends on $pooling_convention$, which has two options:    valid  (default)::  f(x, k, p, s) = floor((x+2*p-k)/s)+1\n  *  full , which is compatible with Caffe::  f(x, k, p, s) = ceil((x+2*p-k)/s)+1    But $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.  Three pooling options are supported by $pool_type$:   avg : average pooling  max : max pooling  sum : sum pooling   For 3-D pooling, an additional  depth  dimension is added before  height . Namely the input data will have shape  (batch_size, channel, depth, height, width) .  Defined in src/operator/nn/pooling.cc:L133  Arguments   data::NDArray-or-SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=0 : Ignore kernel size, do global pooling based on current input feature map.  cudnn_off::boolean, optional, default=0 : Turn off cudnn pooling and use MXNet pooling operator.  kernel::Shape(tuple), required : Pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  pooling_convention::{'full', 'valid'},optional, default='valid' : Pooling convention to be applied.  stride::Shape(tuple), optional, default=[] : Stride: for pooling (y, x) or (d, y, x). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : Pad for pooling: (y, x) or (d, y, x). Defaults to no padding.   source  #  MXNet.mx.Pooling_v1     Method .  Pooling_v1(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)  This operator is DEPRECATED. Perform pooling on the input.  The shapes for 2-D pooling is   data :  (batch_size, channel, height, width)   out :  (batch_size, num_filter, out_height, out_width) , with::  out_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])    The definition of  f  depends on $pooling_convention$, which has two options:    valid  (default)::  f(x, k, p, s) = floor((x+2*p-k)/s)+1\n  *  full , which is compatible with Caffe::  f(x, k, p, s) = ceil((x+2*p-k)/s)+1    But $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.  Three pooling options are supported by $pool_type$:   avg : average pooling  max : max pooling  sum : sum pooling   1-D pooling is special case of 2-D pooling with  weight=1  and  kernel[1]=1 .  For 3-D pooling, an additional  depth  dimension is added before  height . Namely the input data will have shape  (batch_size, channel, depth, height, width) .  Defined in src/operator/pooling_v1.cc:L104  Arguments   data::NDArray-or-SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=0 : Ignore kernel size, do global pooling based on current input feature map.  kernel::Shape(tuple), required : pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  pooling_convention::{'full', 'valid'},optional, default='valid' : Pooling convention to be applied.  stride::Shape(tuple), optional, default=[] : stride: for pooling (y, x) or (d, y, x)  pad::Shape(tuple), optional, default=[] : pad for pooling: (y, x) or (d, y, x)   source  #  MXNet.mx.RNN     Method .  RNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)  Applies a recurrent layer to input.  Arguments   data::NDArray-or-SymbolicNode : Input data to RNN  parameters::NDArray-or-SymbolicNode : Vector of all RNN trainable parameters concatenated  state::NDArray-or-SymbolicNode : initial hidden state of the RNN  state_cell::NDArray-or-SymbolicNode : initial cell state for LSTM networks (only for LSTM)  state_size::int (non-negative), required : size of the state for each layer  num_layers::int (non-negative), required : number of stacked layers  bidirectional::boolean, optional, default=0 : whether to use bidirectional recurrent layers  mode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required : the type of RNN to compute  p::float, optional, default=0 : Dropout probability, fraction of the input that gets dropped out at training time  state_outputs::boolean, optional, default=0 : Whether to have the states as symbol outputs.   source  #  MXNet.mx.ROIPooling     Method .  ROIPooling(data, rois, pooled_size, spatial_scale)  Performs region of interest(ROI) pooling on the input array.  ROI pooling is a variant of a max pooling layer, in which the output size is fixed and region of interest is a parameter. Its purpose is to perform max pooling on the inputs of non-uniform sizes to obtain fixed-size feature maps. ROI pooling is a neural-net layer mostly used in training a  Fast R-CNN  network for object detection.  This operator takes a 4D feature map as an input array and region proposals as  rois , then it pools over sub-regions of input and produces a fixed-sized output array regardless of the ROI size.  To crop the feature map accordingly, you can resize the bounding box coordinates by changing the parameters  rois  and  spatial_scale .  The cropped feature maps are pooled by standard max pooling operation to a fixed size output indicated by a  pooled_size  parameter. batch_size will change to the number of region bounding boxes after  ROIPooling .  The size of each region of interest doesn't have to be perfectly divisible by the number of pooling sections( pooled_size ).  Example::  x = [[[[  0.,   1.,   2.,   3.,   4.,   5.],          [  6.,   7.,   8.,   9.,  10.,  11.],          [ 12.,  13.,  14.,  15.,  16.,  17.],          [ 18.,  19.,  20.,  21.,  22.,  23.],          [ 24.,  25.,  26.,  27.,  28.,  29.],          [ 30.,  31.,  32.,  33.,  34.,  35.],          [ 36.,  37.,  38.,  39.,  40.,  41.],          [ 42.,  43.,  44.,  45.,  46.,  47.]]]]  // region of interest i.e. bounding box coordinates.   y = [[0,0,0,4,4]]  // returns array of shape (2,2) according to the given roi with max pooling.   ROIPooling(x, y, (2,2), 1.0) = [[[[ 14.,  16.],                                     [ 26.,  28.]]]]  // region of interest is changed due to the change in  spacial_scale  parameter.   ROIPooling(x, y, (2,2), 0.7) = [[[[  7.,   9.],                                     [ 19.,  21.]]]]  Defined in src/operator/roi_pooling.cc:L287  Arguments   data::NDArray-or-SymbolicNode : The input array to the pooling operator,  a 4D Feature maps  rois::NDArray-or-SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]], where (x1, y1) and (x2, y2) are top left and bottom right corners of designated region of interest.  batch_index  indicates the index of corresponding image in the input array  pooled_size::Shape(tuple), required : ROI pooling output shape (h,w)  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers   source  #  MXNet.mx.SVMOutput     Method .  SVMOutput(data, label, margin, regularization_coefficient, use_linear)  Computes support vector machine based transformation of the input.  This tutorial demonstrates using SVM as output layer for classification instead of softmax: https://github.com/dmlc/mxnet/tree/master/example/svm_mnist.  Arguments   data::NDArray-or-SymbolicNode : Input data for SVM transformation.  label::NDArray-or-SymbolicNode : Class label for the input data.  margin::float, optional, default=1 : The loss function penalizes outputs that lie outside this margin. Default margin is 1.  regularization_coefficient::float, optional, default=1 : Regularization parameter for the SVM. This balances the tradeoff between coefficient size and error.  use_linear::boolean, optional, default=0 : Whether to use L1-SVM objective. L2-SVM objective is used by default.   source  #  MXNet.mx.SequenceLast     Method .  SequenceLast(data, sequence_length, use_sequence_length)  Takes the last element of a sequence.  This function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns a (n-1)-dimensional array of the form [batch_size, other_feature_dims].  Parameter  sequence_length  is used to handle variable-length sequences.  sequence_length  should be an input array of positive ints of dimension [batch_size]. To use this parameter, set  use_sequence_length  to  True , otherwise each example in the batch is assumed to have the max sequence length.  .. note:: Alternatively, you can also use  take  operator.  Example::  x = [[[  1.,   2.,   3.],          [  4.,   5.,   6.],          [  7.,   8.,   9.]],      [[ 10.,   11.,   12.],\n     [ 13.,   14.,   15.],\n     [ 16.,   17.,   18.]],\n\n    [[  19.,   20.,   21.],\n     [  22.,   23.,   24.],\n     [  25.,   26.,   27.]]]  // returns last sequence when sequence_length parameter is not used    SequenceLast(x) = [[  19.,   20.,   21.],                       [  22.,   23.,   24.],                       [  25.,   26.,   27.]]  // sequence_length y is used    SequenceLast(x, y=[1,1,1], use_sequence_length=True) =             [[  1.,   2.,   3.],              [  4.,   5.,   6.],              [  7.,   8.,   9.]]  // sequence_length y is used    SequenceLast(x, y=[1,2,3], use_sequence_length=True) =             [[  1.,    2.,   3.],              [  13.,  14.,  15.],              [  25.,  26.,  27.]]  Defined in src/operator/sequence_last.cc:L92  Arguments   data::NDArray-or-SymbolicNode : n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n 2  sequence_length::NDArray-or-SymbolicNode : vector of sequence lengths of the form [batch_size]  use_sequence_length::boolean, optional, default=0 : If set to true, this layer takes in an extra input parameter  sequence_length  to specify variable length sequence   source  #  MXNet.mx.SequenceMask     Method .  SequenceMask(data, sequence_length, use_sequence_length, value)  Sets all elements outside the sequence to a constant value.  This function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.  Parameter  sequence_length  is used to handle variable-length sequences.  sequence_length  should be an input array of positive ints of dimension [batch_size]. To use this parameter, set  use_sequence_length  to  True , otherwise each example in the batch is assumed to have the max sequence length and this operator works as the  identity  operator.  Example::  x = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],      [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]  // Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]  // Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]  // works as identity operator when sequence_length parameter is not used    SequenceMask(x) = [[[  1.,   2.,   3.],                        [  4.,   5.,   6.]],                    [[  7.,   8.,   9.],\n                   [ 10.,  11.,  12.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]  // sequence_length [1,1] means 1 of each batch will be kept    // and other rows are masked with default mask value = 0    SequenceMask(x, y=[1,1], use_sequence_length=True) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],               [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]],\n\n             [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]]]  // sequence_length [2,3] means 2 of batch B1 and 3 of batch B2 will be kept    // and other rows are masked with value = 1    SequenceMask(x, y=[2,3], use_sequence_length=True, value=1) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],               [[  7.,   8.,   9.],\n              [  10.,  11.,  12.]],\n\n             [[   1.,   1.,   1.],\n              [  16.,  17.,  18.]]]  Defined in src/operator/sequence_mask.cc:L114  Arguments   data::NDArray-or-SymbolicNode : n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n 2  sequence_length::NDArray-or-SymbolicNode : vector of sequence lengths of the form [batch_size]  use_sequence_length::boolean, optional, default=0 : If set to true, this layer takes in an extra input parameter  sequence_length  to specify variable length sequence  value::float, optional, default=0 : The value to be used as a mask.   source  #  MXNet.mx.SequenceReverse     Method .  SequenceReverse(data, sequence_length, use_sequence_length)  Reverses the elements of each sequence.  This function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.  Parameter  sequence_length  is used to handle variable-length sequences.  sequence_length  should be an input array of positive ints of dimension [batch_size]. To use this parameter, set  use_sequence_length  to  True , otherwise each example in the batch is assumed to have the max sequence length.  Example::  x = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],      [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]  // Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]  // Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]  // returns reverse sequence when sequence_length parameter is not used    SequenceReverse(x) = [[[ 13.,  14.,   15.],                           [ 16.,  17.,   18.]],                       [[  7.,   8.,   9.],\n                      [ 10.,  11.,  12.]],\n\n                     [[  1.,   2.,   3.],\n                      [  4.,   5.,   6.]]]  // sequence_length [2,2] means 2 rows of    // both batch B1 and B2 will be reversed.    SequenceReverse(x, y=[2,2], use_sequence_length=True) =                      [[[  7.,   8.,   9.],                        [ 10.,  11.,  12.]],                    [[  1.,   2.,   3.],\n                   [  4.,   5.,   6.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]  // sequence_length [2,3] means 2 of batch B2 and 3 of batch B3    // will be reversed.    SequenceReverse(x, y=[2,3], use_sequence_length=True) =                     [[[  7.,   8.,   9.],                       [ 16.,  17.,  18.]],                   [[  1.,   2.,   3.],\n                  [ 10.,  11.,  12.]],\n\n                 [[ 13.,  14,   15.],\n                  [  4.,   5.,   6.]]]  Defined in src/operator/sequence_reverse.cc:L113  Arguments   data::NDArray-or-SymbolicNode : n-dimensional input array of the form [max_sequence_length, batch_size, other dims] where n 2  sequence_length::NDArray-or-SymbolicNode : vector of sequence lengths of the form [batch_size]  use_sequence_length::boolean, optional, default=0 : If set to true, this layer takes in an extra input parameter  sequence_length  to specify variable length sequence   source  #  MXNet.mx.SliceChannel     Method .  SliceChannel(data, num_outputs, axis, squeeze_axis)  Splits an array along a particular axis into multiple sub-arrays.  .. note:: $SliceChannel$ is deprecated. Use $split$ instead.  Note  that  num_outputs  should evenly divide the length of the axis along which to split the array.  Example::  x  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)  y = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]     [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]  y[0].shape = (3, 1, 1)  z = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]     [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]  z[0].shape = (1, 2, 1)  squeeze_axis=1  removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $1$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to true only if $input.shape[axis] == num_outputs$.  Example::  z = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]     [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]  z[0].shape = (2 ,1 )  Defined in src/operator/slice_channel.cc:L107  Arguments   data::NDArray-or-SymbolicNode : The input  num_outputs::int, required : Number of splits. Note that this should evenly divide the length of the  axis .  axis::int, optional, default='1' : Axis along which to split.  squeeze_axis::boolean, optional, default=0 : If true, Removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $true$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to $true$ only if $input.shape[axis] == num_outputs$.   source  #  MXNet.mx.SoftmaxActivation     Method .  SoftmaxActivation(data, mode)  Applies softmax activation to input. This is intended for internal layers.  .. note::  This operator has been deprecated, please use  softmax .  If  mode  = $instance$, this operator will compute a softmax for each instance in the batch. This is the default mode.  If  mode  = $channel$, this operator will compute a k-class softmax at each position of each instance, where  k  = $num_channel$. This mode can only be used when the input array has at least 3 dimensions. This can be used for  fully convolutional network ,  image segmentation , etc.  Example::     input_array = mx.nd.array([[3., 0.5, -0.5, 2., 7.],                            [2., -.4, 7.,   3., 0.2]]) softmax_act = mx.nd.SoftmaxActivation(input_array) print softmax_act.asnumpy()     [[  1.78322066e-02   1.46375655e-03   5.38485940e-04   6.56010211e-03   9.73605454e-01]    [  6.56221947e-03   5.95310994e-04   9.73919690e-01   1.78379621e-02   1.08472735e-03]]  Defined in src/operator/nn/softmax_activation.cc:L67  Arguments   data::NDArray-or-SymbolicNode : Input array to activation function.  mode::{'channel', 'instance'},optional, default='instance' : Specifies how to compute the softmax. If set to $instance$, it computes softmax for each instance. If set to $channel$, It computes cross channel softmax for each position of each instance.   source  #  MXNet.mx.SoftmaxOutput     Method .  SoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad, smooth_alpha)  Computes the gradient of cross entropy loss with respect to softmax output.    This operator computes the gradient in two steps. The cross entropy loss does not actually need to be computed.   Applies softmax function on the input array.  Computes and returns the gradient of cross entropy loss w.r.t. the softmax output.   The softmax function, cross entropy loss and gradient is given by:    Softmax Function:  .. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n      * Cross Entropy Function:  .. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)\n      * The gradient of cross entropy loss w.r.t softmax output:  .. math:: \\text{gradient} = \\text{output} - \\text{label}\n  * During forward propagation, the softmax function is computed for each instance in the input array.    For general  N -D input arrays with shape :math: (d_1, d_2, ..., d_n) . The size is :math: s=d_1 \\cdot d_2 \\cdot \\cdot \\cdot d_n . We can use the parameters  preserve_shape  and  multi_output  to specify the way to compute softmax:   By default,  preserve_shape  is $false$. This operator will reshape the input array into a 2-D array with shape :math: (d_1, \\frac{s}{d_1})  and then compute the softmax function for each row in the reshaped array, and afterwards reshape it back to the original shape :math: (d_1, d_2, ..., d_n) .  If  preserve_shape  is $true$, the softmax function will be computed along the last axis ( axis  = $-1$).  If  multi_output  is $true$, the softmax function will be computed along the second axis ( axis  = $1$).   During backward propagation, the gradient of cross-entropy loss w.r.t softmax output array is computed. The provided label can be a one-hot label array or a probability label array.    If the parameter  use_ignore  is $true$,  ignore_label  can specify input instances with a particular label to be ignored during backward propagation.  This has no effect when softmax  output  has same shape as  label .  Example::  data = [[1,2,3,4],[2,2,2,2],[3,3,3,3],[4,4,4,4]]   label = [1,0,2,3]   ignore_label = 1   SoftmaxOutput(data=data, label = label,\n                multi_output=true, use_ignore=true,\n                ignore_label=ignore_label)", 
            "title": "Reference"
        }, 
        {
            "location": "/api/ndarray/#forward-softmax-output", 
            "text": "[[ 0.0320586   0.08714432  0.23688284  0.64391428]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]]", 
            "title": "forward softmax output"
        }, 
        {
            "location": "/api/ndarray/#backward-gradient-output", 
            "text": "[[ 0.    0.    0.    0.  ]    [-0.75  0.25  0.25  0.25]    [ 0.25  0.25 -0.75  0.25]    [ 0.25  0.25  0.25 -0.75]]", 
            "title": "backward gradient output"
        }, 
        {
            "location": "/api/ndarray/#notice-that-the-first-row-is-all-0-because-label0-is-1-which-is-equal-to-ignore_label", 
            "text": "* The parameter `grad_scale` can be used to rescale the gradient, which is often used to give each loss function different weights.\n  * This operator also supports various ways to normalize the gradient by `normalization`, The `normalization` is applied if softmax output has different shape than the labels. The `normalization` mode can be set to the followings:   $'null'$: do nothing.  $'batch'$: divide the gradient by the batch size.  $'valid'$: divide the gradient by the number of instances which are not ignored.       Defined in src/operator/softmax_output.cc:L123  Arguments   data::NDArray-or-SymbolicNode : Input array.  label::NDArray-or-SymbolicNode : Ground truth label.  grad_scale::float, optional, default=1 : Scales the gradient by a float factor.  ignore_label::float, optional, default=-1 : The instances whose  labels  ==  ignore_label  will be ignored during backward, if  use_ignore  is set to $true$).  multi_output::boolean, optional, default=0 : If set to $true$, the softmax function will be computed along axis $1$. This is applied when the shape of input array differs from the shape of label array.  use_ignore::boolean, optional, default=0 : If set to $true$, the  ignore_label  value will not contribute to the backward gradient.  preserve_shape::boolean, optional, default=0 : If set to $true$, the softmax function will be computed along the last axis ($-1$).  normalization::{'batch', 'null', 'valid'},optional, default='null' : Normalizes the gradient.  out_grad::boolean, optional, default=0 : Multiplies gradient with output gradient element-wise.  smooth_alpha::float, optional, default=0 : Constant for computing a label smoothed version of cross-entropyfor the backwards pass.  This constant gets subtracted from theone-hot encoding of the gold label and distributed uniformly toall other labels.   source  #  MXNet.mx.SpatialTransformer     Method .  SpatialTransformer(data, loc, target_shape, transform_type, sampler_type)  Applies a spatial transformer to input feature map.  Arguments   data::NDArray-or-SymbolicNode : Input data to the SpatialTransformerOp.  loc::NDArray-or-SymbolicNode : localisation net, the output dim should be 6 when transform_type is affine. You shold initialize the weight and bias with identity tranform.  target_shape::Shape(tuple), optional, default=[0,0] : output shape(h, w) of spatial transformer: (y, x)  transform_type::{'affine'}, required : transformation type  sampler_type::{'bilinear'}, required : sampling type   source  #  MXNet.mx.SwapAxis     Method .  SwapAxis(data, dim1, dim2)  Interchanges two axes of an array.  Examples::  x = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]  x = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array  swapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]  Defined in src/operator/swapaxis.cc:L70  Arguments   data::NDArray-or-SymbolicNode : Input array.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.   source  #  MXNet.mx.UpSampling     Method .  UpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)  Note : UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.  Performs nearest neighbor/bilinear up sampling to inputs.  Arguments   data::NDArray-or-SymbolicNode[] : Array of tensors to upsample  scale::int (non-negative), required : Up sampling scale  num_filter::int (non-negative), optional, default=0 : Input filter. Only used by bilinear sample_type.  sample_type::{'bilinear', 'nearest'}, required : upsampling method  multi_input_mode::{'concat', 'sum'},optional, default='concat' : How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.  num_args::int, required : Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale h_0,scale w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)   source  #  MXNet.mx._CachedOp     Method .  _CachedOp()  Arguments  source  #  MXNet.mx._CrossDeviceCopy     Method .  _CrossDeviceCopy()  Special op to copy data cross device  Arguments  source  #  MXNet.mx._CustomFunction     Method .  _CustomFunction()  Arguments  source  #  MXNet.mx._Div     Method .  _Div(lhs, rhs)  _Div is an alias of elemwise_div.  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._DivScalar     Method .  _DivScalar(data, scalar)  _DivScalar is an alias of _div_scalar.  Divide an array with a scalar.  $_div_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Equal     Method .  _Equal(lhs, rhs)  _Equal is an alias of _equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._EqualScalar     Method .  _EqualScalar(data, scalar)  _EqualScalar is an alias of _equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Greater     Method .  _Greater(lhs, rhs)  _Greater is an alias of _greater.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._GreaterEqualScalar     Method .  _GreaterEqualScalar(data, scalar)  _GreaterEqualScalar is an alias of _greater_equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._GreaterScalar     Method .  _GreaterScalar(data, scalar)  _GreaterScalar is an alias of _greater_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Greater_Equal     Method .  _Greater_Equal(lhs, rhs)  _Greater_Equal is an alias of _greater_equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._Hypot     Method .  _Hypot(lhs, rhs)  _Hypot is an alias of _hypot.  Given the \"legs\" of a right triangle, return its hypotenuse.  Defined in src/operator/tensor/elemwise_binary_op_extended.cc:L79  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._HypotScalar     Method .  _HypotScalar(data, scalar)  _HypotScalar is an alias of _hypot_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Lesser     Method .  _Lesser(lhs, rhs)  _Lesser is an alias of _lesser.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._LesserEqualScalar     Method .  _LesserEqualScalar(data, scalar)  _LesserEqualScalar is an alias of _lesser_equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._LesserScalar     Method .  _LesserScalar(data, scalar)  _LesserScalar is an alias of _lesser_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Lesser_Equal     Method .  _Lesser_Equal(lhs, rhs)  _Lesser_Equal is an alias of _lesser_equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._Maximum     Method .  _Maximum(lhs, rhs)  _Maximum is an alias of _maximum.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._MaximumScalar     Method .  _MaximumScalar(data, scalar)  _MaximumScalar is an alias of _maximum_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Minimum     Method .  _Minimum(lhs, rhs)  _Minimum is an alias of _minimum.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._MinimumScalar     Method .  _MinimumScalar(data, scalar)  _MinimumScalar is an alias of _minimum_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._MinusScalar     Method .  _MinusScalar(data, scalar)  _MinusScalar is an alias of _minus_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._ModScalar     Method .  _ModScalar(data, scalar)  _ModScalar is an alias of _mod_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Mul     Method .  _Mul(lhs, rhs)  _Mul is an alias of elemwise_mul.  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._MulScalar     Method .  _MulScalar(data, scalar)  _MulScalar is an alias of _mul_scalar.  Multiply an array with a scalar.  $_mul_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._NDArray     Method .  _NDArray(data, info)  Stub for implementing an operator implemented in native frontend language with ndarray.  Arguments   data::NDArray-or-SymbolicNode[] : Input data for the custom operator.  info::ptr, required :   source  #  MXNet.mx._Native     Method .  _Native(data, info, need_top_grad)  Stub for implementing an operator implemented in native frontend language.  Arguments   data::NDArray-or-SymbolicNode[] : Input data for the custom operator.  info::ptr, required :  need_top_grad::boolean, optional, default=1 : Whether this layer needs out grad for backward. Should be false for loss layers.   source  #  MXNet.mx._NoGradient     Method .  _NoGradient()  Place holder for variable who cannot perform gradient  Arguments  source  #  MXNet.mx._NotEqualScalar     Method .  _NotEqualScalar(data, scalar)  _NotEqualScalar is an alias of _not_equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Not_Equal     Method .  _Not_Equal(lhs, rhs)  _Not_Equal is an alias of _not_equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._PlusScalar     Method .  _PlusScalar(data, scalar)  _PlusScalar is an alias of _plus_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._Power     Method .  _Power(lhs, rhs)  _Power is an alias of _power.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._PowerScalar     Method .  _PowerScalar(data, scalar)  _PowerScalar is an alias of _power_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._RDivScalar     Method .  _RDivScalar(data, scalar)  _RDivScalar is an alias of _rdiv_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._RMinusScalar     Method .  _RMinusScalar(data, scalar)  _RMinusScalar is an alias of _rminus_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._RModScalar     Method .  _RModScalar(data, scalar)  _RModScalar is an alias of _rmod_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._RPowerScalar     Method .  _RPowerScalar(data, scalar)  _RPowerScalar is an alias of _rpower_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._add     Method .  _add(lhs, rhs)  _add is an alias of elemwise_add.  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._arange     Method .  _arange(start, stop, step, repeat, ctx, dtype)  Return evenly spaced values within a given interval. Similar to Numpy  Arguments   start::double, required : Start of interval. The interval includes this value. The default start value is 0.  stop::double or None, optional, default=None : End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.  step::double, optional, default=1 : Spacing between values.  repeat::int, optional, default='1' : The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013  a, a, a.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'int64', 'uint8'},optional, default='float32' : Target data type.   source  #  MXNet.mx._backward_Activation     Method .  _backward_Activation()  Arguments  source  #  MXNet.mx._backward_BatchNorm     Method .  _backward_BatchNorm()  Arguments  source  #  MXNet.mx._backward_BatchNorm_v1     Method .  _backward_BatchNorm_v1()  Arguments  source  #  MXNet.mx._backward_BilinearSampler     Method .  _backward_BilinearSampler()  Arguments  source  #  MXNet.mx._backward_CachedOp     Method .  _backward_CachedOp()  Arguments  source  #  MXNet.mx._backward_Concat     Method .  _backward_Concat()  Arguments  source  #  MXNet.mx._backward_Convolution     Method .  _backward_Convolution()  Arguments  source  #  MXNet.mx._backward_Convolution_v1     Method .  _backward_Convolution_v1()  Arguments  source  #  MXNet.mx._backward_Correlation     Method .  _backward_Correlation()  Arguments  source  #  MXNet.mx._backward_Crop     Method .  _backward_Crop()  Arguments  source  #  MXNet.mx._backward_Custom     Method .  _backward_Custom()  Arguments  source  #  MXNet.mx._backward_CustomFunction     Method .  _backward_CustomFunction()  Arguments  source  #  MXNet.mx._backward_Deconvolution     Method .  _backward_Deconvolution()  Arguments  source  #  MXNet.mx._backward_Dropout     Method .  _backward_Dropout()  Arguments  source  #  MXNet.mx._backward_Embedding     Method .  _backward_Embedding()  Arguments  source  #  MXNet.mx._backward_FullyConnected     Method .  _backward_FullyConnected()  Arguments  source  #  MXNet.mx._backward_GridGenerator     Method .  _backward_GridGenerator()  Arguments  source  #  MXNet.mx._backward_IdentityAttachKLSparseReg     Method .  _backward_IdentityAttachKLSparseReg()  Arguments  source  #  MXNet.mx._backward_InstanceNorm     Method .  _backward_InstanceNorm()  Arguments  source  #  MXNet.mx._backward_L2Normalization     Method .  _backward_L2Normalization()  Arguments  source  #  MXNet.mx._backward_LRN     Method .  _backward_LRN()  Arguments  source  #  MXNet.mx._backward_LeakyReLU     Method .  _backward_LeakyReLU()  Arguments  source  #  MXNet.mx._backward_LinearRegressionOutput     Method .  _backward_LinearRegressionOutput()  Arguments  source  #  MXNet.mx._backward_LogisticRegressionOutput     Method .  _backward_LogisticRegressionOutput()  Arguments  source  #  MXNet.mx._backward_MAERegressionOutput     Method .  _backward_MAERegressionOutput()  Arguments  source  #  MXNet.mx._backward_MakeLoss     Method .  _backward_MakeLoss()  Arguments  source  #  MXNet.mx._backward_Pad     Method .  _backward_Pad()  Arguments  source  #  MXNet.mx._backward_Pooling     Method .  _backward_Pooling()  Arguments  source  #  MXNet.mx._backward_Pooling_v1     Method .  _backward_Pooling_v1()  Arguments  source  #  MXNet.mx._backward_RNN     Method .  _backward_RNN()  Arguments  source  #  MXNet.mx._backward_ROIPooling     Method .  _backward_ROIPooling()  Arguments  source  #  MXNet.mx._backward_SVMOutput     Method .  _backward_SVMOutput()  Arguments  source  #  MXNet.mx._backward_SequenceLast     Method .  _backward_SequenceLast()  Arguments  source  #  MXNet.mx._backward_SequenceMask     Method .  _backward_SequenceMask()  Arguments  source  #  MXNet.mx._backward_SequenceReverse     Method .  _backward_SequenceReverse()  Arguments  source  #  MXNet.mx._backward_SliceChannel     Method .  _backward_SliceChannel()  Arguments  source  #  MXNet.mx._backward_Softmax     Method .  _backward_Softmax()  Arguments  source  #  MXNet.mx._backward_SoftmaxActivation     Method .  _backward_SoftmaxActivation()  Arguments  source  #  MXNet.mx._backward_SoftmaxOutput     Method .  _backward_SoftmaxOutput()  Arguments  source  #  MXNet.mx._backward_SparseEmbedding     Method .  _backward_SparseEmbedding()  Arguments  source  #  MXNet.mx._backward_SpatialTransformer     Method .  _backward_SpatialTransformer()  Arguments  source  #  MXNet.mx._backward_SwapAxis     Method .  _backward_SwapAxis()  Arguments  source  #  MXNet.mx._backward_UpSampling     Method .  _backward_UpSampling()  Arguments  source  #  MXNet.mx._backward__CrossDeviceCopy     Method .  _backward__CrossDeviceCopy()  Arguments  source  #  MXNet.mx._backward__NDArray     Method .  _backward__NDArray()  Arguments  source  #  MXNet.mx._backward__Native     Method .  _backward__Native()  Arguments  source  #  MXNet.mx._backward__contrib_CTCLoss     Method .  _backward__contrib_CTCLoss()  Arguments  source  #  MXNet.mx._backward__contrib_DeformableConvolution     Method .  _backward__contrib_DeformableConvolution()  Arguments  source  #  MXNet.mx._backward__contrib_DeformablePSROIPooling     Method .  _backward__contrib_DeformablePSROIPooling()  Arguments  source  #  MXNet.mx._backward__contrib_MultiBoxDetection     Method .  _backward__contrib_MultiBoxDetection()  Arguments  source  #  MXNet.mx._backward__contrib_MultiBoxPrior     Method .  _backward__contrib_MultiBoxPrior()  Arguments  source  #  MXNet.mx._backward__contrib_MultiBoxTarget     Method .  _backward__contrib_MultiBoxTarget()  Arguments  source  #  MXNet.mx._backward__contrib_MultiProposal     Method .  _backward__contrib_MultiProposal()  Arguments  source  #  MXNet.mx._backward__contrib_PSROIPooling     Method .  _backward__contrib_PSROIPooling()  Arguments  source  #  MXNet.mx._backward__contrib_Proposal     Method .  _backward__contrib_Proposal()  Arguments  source  #  MXNet.mx._backward__contrib_count_sketch     Method .  _backward__contrib_count_sketch()  Arguments  source  #  MXNet.mx._backward__contrib_fft     Method .  _backward__contrib_fft()  Arguments  source  #  MXNet.mx._backward__contrib_ifft     Method .  _backward__contrib_ifft()  Arguments  source  #  MXNet.mx._backward_abs     Method .  _backward_abs(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_add     Method .  _backward_add()  Arguments  source  #  MXNet.mx._backward_arccos     Method .  _backward_arccos(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_arccosh     Method .  _backward_arccosh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_arcsin     Method .  _backward_arcsin(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_arcsinh     Method .  _backward_arcsinh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_arctan     Method .  _backward_arctan(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_arctanh     Method .  _backward_arctanh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_batch_dot     Method .  _backward_batch_dot()  Arguments  source  #  MXNet.mx._backward_broadcast_add     Method .  _backward_broadcast_add()  Arguments  source  #  MXNet.mx._backward_broadcast_div     Method .  _backward_broadcast_div()  Arguments  source  #  MXNet.mx._backward_broadcast_hypot     Method .  _backward_broadcast_hypot()  Arguments  source  #  MXNet.mx._backward_broadcast_maximum     Method .  _backward_broadcast_maximum()  Arguments  source  #  MXNet.mx._backward_broadcast_minimum     Method .  _backward_broadcast_minimum()  Arguments  source  #  MXNet.mx._backward_broadcast_mod     Method .  _backward_broadcast_mod()  Arguments  source  #  MXNet.mx._backward_broadcast_mul     Method .  _backward_broadcast_mul()  Arguments  source  #  MXNet.mx._backward_broadcast_power     Method .  _backward_broadcast_power()  Arguments  source  #  MXNet.mx._backward_broadcast_sub     Method .  _backward_broadcast_sub()  Arguments  source  #  MXNet.mx._backward_cast     Method .  _backward_cast()  Arguments  source  #  MXNet.mx._backward_cbrt     Method .  _backward_cbrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_clip     Method .  _backward_clip()  Arguments  source  #  MXNet.mx._backward_contrib_bipartite_matching     Method .  _backward_contrib_bipartite_matching(is_ascend, threshold, topk)  Arguments   is_ascend::boolean, optional, default=0 : Use ascend order for scores instead of descending. Please set threshold accordingly.  threshold::float, required : Ignore matching when score   thresh, if is_ascend=false, or ignore score   thresh, if is_ascend=true.  topk::int, optional, default='-1' : Limit the number of matches to topk, set -1 for no limit   source  #  MXNet.mx._backward_contrib_box_iou     Method .  _backward_contrib_box_iou(format)  Arguments   format::{'center', 'corner'},optional, default='corner' : The box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].  source  #  MXNet.mx._backward_contrib_box_nms     Method .  _backward_contrib_box_nms(overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)  Arguments   overlap_thresh::float, optional, default=0.5 : Overlapping(IoU) threshold to suppress object with smaller score.  topk::int, optional, default='-1' : Apply nms to topk boxes with descending scores, -1 to no restriction.  coord_start::int, optional, default='2' : Start index of the consecutive 4 coordinates.  score_index::int, optional, default='1' : Index of the scores/confidence of boxes.  id_index::int, optional, default='-1' : Optional, index of the class categories, -1 to disable.  force_suppress::boolean, optional, default=0 : Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category  in_format::{'center', 'corner'},optional, default='corner' : The input box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   out_format::{'center', 'corner'},optional, default='corner' : The output box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].  source  #  MXNet.mx._backward_copy     Method .  _backward_copy()  Arguments  source  #  MXNet.mx._backward_cos     Method .  _backward_cos(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_cosh     Method .  _backward_cosh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_degrees     Method .  _backward_degrees(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_div     Method .  _backward_div()  Arguments  source  #  MXNet.mx._backward_div_scalar     Method .  _backward_div_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._backward_dot     Method .  _backward_dot(transpose_a, transpose_b)  Arguments   transpose_a::boolean, optional, default=0 : If true then transpose the first input before dot.  transpose_b::boolean, optional, default=0 : If true then transpose the second input before dot.   source  #  MXNet.mx._backward_expm1     Method .  _backward_expm1(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_gamma     Method .  _backward_gamma(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_gammaln     Method .  _backward_gammaln(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_hypot     Method .  _backward_hypot()  Arguments  source  #  MXNet.mx._backward_hypot_scalar     Method .  _backward_hypot_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_linalg_gelqf     Method .  _backward_linalg_gelqf()  Arguments  source  #  MXNet.mx._backward_linalg_gemm     Method .  _backward_linalg_gemm()  Arguments  source  #  MXNet.mx._backward_linalg_gemm2     Method .  _backward_linalg_gemm2()  Arguments  source  #  MXNet.mx._backward_linalg_potrf     Method .  _backward_linalg_potrf()  Arguments  source  #  MXNet.mx._backward_linalg_potri     Method .  _backward_linalg_potri()  Arguments  source  #  MXNet.mx._backward_linalg_sumlogdiag     Method .  _backward_linalg_sumlogdiag()  Arguments  source  #  MXNet.mx._backward_linalg_syevd     Method .  _backward_linalg_syevd()  Arguments  source  #  MXNet.mx._backward_linalg_syrk     Method .  _backward_linalg_syrk()  Arguments  source  #  MXNet.mx._backward_linalg_trmm     Method .  _backward_linalg_trmm()  Arguments  source  #  MXNet.mx._backward_linalg_trsm     Method .  _backward_linalg_trsm()  Arguments  source  #  MXNet.mx._backward_log     Method .  _backward_log(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_log10     Method .  _backward_log10(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_log1p     Method .  _backward_log1p(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_log2     Method .  _backward_log2(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_log_softmax     Method .  _backward_log_softmax(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_max     Method .  _backward_max()  Arguments  source  #  MXNet.mx._backward_maximum     Method .  _backward_maximum()  Arguments  source  #  MXNet.mx._backward_maximum_scalar     Method .  _backward_maximum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_mean     Method .  _backward_mean()  Arguments  source  #  MXNet.mx._backward_min     Method .  _backward_min()  Arguments  source  #  MXNet.mx._backward_minimum     Method .  _backward_minimum()  Arguments  source  #  MXNet.mx._backward_minimum_scalar     Method .  _backward_minimum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_mod     Method .  _backward_mod()  Arguments  source  #  MXNet.mx._backward_mod_scalar     Method .  _backward_mod_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_mul     Method .  _backward_mul()  Arguments  source  #  MXNet.mx._backward_mul_scalar     Method .  _backward_mul_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._backward_nanprod     Method .  _backward_nanprod()  Arguments  source  #  MXNet.mx._backward_nansum     Method .  _backward_nansum()  Arguments  source  #  MXNet.mx._backward_pick     Method .  _backward_pick()  Arguments  source  #  MXNet.mx._backward_power     Method .  _backward_power()  Arguments  source  #  MXNet.mx._backward_power_scalar     Method .  _backward_power_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_prod     Method .  _backward_prod()  Arguments  source  #  MXNet.mx._backward_radians     Method .  _backward_radians(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_rcbrt     Method .  _backward_rcbrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_rdiv_scalar     Method .  _backward_rdiv_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_reciprocal     Method .  _backward_reciprocal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_relu     Method .  _backward_relu(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_repeat     Method .  _backward_repeat()  Arguments  source  #  MXNet.mx._backward_reverse     Method .  _backward_reverse()  Arguments  source  #  MXNet.mx._backward_rmod_scalar     Method .  _backward_rmod_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_rpower_scalar     Method .  _backward_rpower_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value   source  #  MXNet.mx._backward_rsqrt     Method .  _backward_rsqrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_sample_multinomial     Method .  _backward_sample_multinomial()  Arguments  source  #  MXNet.mx._backward_sigmoid     Method .  _backward_sigmoid(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_sign     Method .  _backward_sign(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_sin     Method .  _backward_sin(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_sinh     Method .  _backward_sinh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_slice     Method .  _backward_slice()  Arguments  source  #  MXNet.mx._backward_slice_axis     Method .  _backward_slice_axis()  Arguments  source  #  MXNet.mx._backward_smooth_l1     Method .  _backward_smooth_l1(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_softmax     Method .  _backward_softmax(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_softmax_cross_entropy     Method .  _backward_softmax_cross_entropy()  Arguments  source  #  MXNet.mx._backward_sparse_retain     Method .  _backward_sparse_retain()  Arguments  source  #  MXNet.mx._backward_sqrt     Method .  _backward_sqrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_square     Method .  _backward_square(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_square_sum     Method .  _backward_square_sum()  Arguments  source  #  MXNet.mx._backward_stack     Method .  _backward_stack()  Arguments  source  #  MXNet.mx._backward_sub     Method .  _backward_sub()  Arguments  source  #  MXNet.mx._backward_sum     Method .  _backward_sum()  Arguments  source  #  MXNet.mx._backward_take     Method .  _backward_take()  Arguments  source  #  MXNet.mx._backward_tan     Method .  _backward_tan(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_tanh     Method .  _backward_tanh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._backward_tile     Method .  _backward_tile()  Arguments  source  #  MXNet.mx._backward_topk     Method .  _backward_topk()  Arguments  source  #  MXNet.mx._backward_where     Method .  _backward_where()  Arguments  source  #  MXNet.mx._broadcast_backward     Method .  _broadcast_backward()  Arguments  source  #  MXNet.mx._contrib_CTCLoss     Method .  _contrib_CTCLoss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)  Connectionist Temporal Classification Loss.  The shapes of the inputs and outputs:   data :  (sequence_length, batch_size, alphabet_size)  label :  (batch_size, label_sequence_length)  out :  (batch_size)   The  data  tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When  blank_label  is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.  $label$ is an index matrix of integers. When  blank_label  is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when  blank_label  is $\"last\"$, the value  (alphabet_size-1)  is reserved for blank label.  If a sequence of labels is shorter than  label_sequence_length , use the special padding value at the end of the sequence to conform it to the correct length. The padding value is  0  when  blank_label  is $\"first\"$, and  -1  otherwise.  For example, suppose the vocabulary is  [a, b, c] , and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When  blank_label  is $\"first\"$, we can index the labels as  {'a': 1, 'b': 2, 'c': 3} , and we reserve the 0-th channel for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]  When  blank_label  is $\"last\"$, we can index the labels as  {'a': 0, 'b': 1, 'c': 2} , and we reserve the channel index 3 for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]  $out$ is a list of CTC loss values, one per example in the batch.  See  Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks , A. Graves  et al . for more information on the definition and the algorithm.  Defined in src/operator/contrib/ctc_loss.cc:L115  Arguments   data::NDArray-or-SymbolicNode : Input data to the ctc_loss op.  label::NDArray-or-SymbolicNode : Ground-truth labels for the loss.  data_lengths::NDArray-or-SymbolicNode : Lengths of data for each of the samples. Only required when use_data_lengths is true.  label_lengths::NDArray-or-SymbolicNode : Lengths of labels for each of the samples. Only required when use_label_lengths is true.  use_data_lengths::boolean, optional, default=0 : Whether the data lenghts are decided by  data_lengths . If false, the lengths are equal to the max sequence length.  use_label_lengths::boolean, optional, default=0 : Whether the label lenghts are decided by  label_lengths , or derived from  padding_mask . If false, the lengths are derived from the first occurrence of the value of  padding_mask . The value of  padding_mask  is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See  blank_label .  blank_label::{'first', 'last'},optional, default='first' : Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.   source  #  MXNet.mx._contrib_DeformableConvolution     Method .  _contrib_DeformableConvolution(data, offset, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, num_deformable_group, workspace, no_bias, layout)  Compute 2-D deformable convolution on 4-D input.  The deformable convolution operation is described in https://arxiv.org/abs/1703.06211  For 2-D deformable convolution, the shapes are   data :  (batch_size, channel, height, width)  offset :  (batch_size, num_deformable_group * kernel[0] * kernel[1], height, width)  weight :  (num_filter, channel, kernel[0], kernel[1])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_height, out_width) .   Define::  f(x,k,p,s,d) = floor((x+2 p-d (k-1)-1)/s)+1  then we have::  out_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])  If $no_bias$ is set to be true, then the $bias$ term is ignored.  The default data $layout$ is  NCHW , namely  (batch_size, channle, height, width) .  If $num_group$ is larger than 1, denoted by  g , then split the input $data$ evenly into  g  parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the  i -th part of the data with the  i -th weight part. The output is obtained by concating all the  g  results.  If $num_deformable_group$ is larger than 1, denoted by  dg , then split the input $offset$ evenly into  dg  parts along the channel axis, and also evenly split $out$ evenly into  dg  parts along the channel axis. Next compute the deformable convolution, apply the  i -th part of the offset part on the  i -th out.  Both $weight$ and $bias$ are learnable parameters.  Defined in src/operator/contrib/deformable_convolution.cc:L100  Arguments   data::NDArray-or-SymbolicNode : Input data to the DeformableConvolutionOp.  offset::NDArray-or-SymbolicNode : Input offset to the DeformableConvolutionOp.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : Convolution kernel size: (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=[] : Convolution stride: (h, w) or (d, h, w). Defaults to 1 for each dimension.  dilate::Shape(tuple), optional, default=[] : Convolution dilate: (h, w) or (d, h, w). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : Zero pad for convolution: (h, w) or (d, h, w). Defaults to no padding.  num_filter::int (non-negative), required : Convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions.  num_deformable_group::int (non-negative), optional, default=1 : Number of deformable group partitions.  workspace::long (non-negative), optional, default=1024 : Maximum temperal workspace allowed for convolution (MB).  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  layout::{None, 'NCDHW', 'NCHW', 'NCW'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.   source  #  MXNet.mx._contrib_DeformablePSROIPooling     Method .  _contrib_DeformablePSROIPooling(data, rois, trans, spatial_scale, output_dim, group_size, pooled_size, part_size, sample_per_part, trans_std, no_trans)  Performs deformable position-sensitive region-of-interest pooling on inputs. The DeformablePSROIPooling operation is described in https://arxiv.org/abs/1703.06211 .batch_size will change to the number of region bounding boxes after DeformablePSROIPooling  Arguments   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  trans::SymbolicNode : transition parameter  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers  output_dim::int, required : fix output dim  group_size::int, required : fix group size  pooled_size::int, required : fix pooled size  part_size::int, optional, default='0' : fix part size  sample_per_part::int, optional, default='1' : fix samples per part  trans_std::float, optional, default=0 : fix transition std  no_trans::boolean, optional, default=0 : Whether to disable trans parameter.   source  #  MXNet.mx._contrib_MultiBoxDetection     Method .  _contrib_MultiBoxDetection(cls_prob, loc_pred, anchor, clip, threshold, background_id, nms_threshold, force_suppress, variances, nms_topk)  Convert multibox detection predictions.  Arguments   cls_prob::NDArray-or-SymbolicNode : Class probabilities.  loc_pred::NDArray-or-SymbolicNode : Location regression predictions.  anchor::NDArray-or-SymbolicNode : Multibox prior anchor boxes  clip::boolean, optional, default=1 : Clip out-of-boundary boxes.  threshold::float, optional, default=0.01 : Threshold to be a positive prediction.  background_id::int, optional, default='0' : Background id.  nms_threshold::float, optional, default=0.5 : Non-maximum suppression threshold.  force_suppress::boolean, optional, default=0 : Suppress all detections regardless of class_id.  variances::tuple of  float , optional, default=[0.1,0.1,0.2,0.2] : Variances to be decoded from box regression output.  nms_topk::int, optional, default='-1' : Keep maximum top k detections before nms, -1 for no limit.   source  #  MXNet.mx._contrib_MultiBoxPrior     Method .  _contrib_MultiBoxPrior(data, sizes, ratios, clip, steps, offsets)  Generate prior(anchor) boxes from data, sizes and ratios.  Arguments   data::NDArray-or-SymbolicNode : Input data.  sizes::tuple of  float , optional, default=[1] : List of sizes of generated MultiBoxPriores.  ratios::tuple of  float , optional, default=[1] : List of aspect ratios of generated MultiBoxPriores.  clip::boolean, optional, default=0 : Whether to clip out-of-boundary boxes.  steps::tuple of  float , optional, default=[-1,-1] : Priorbox step across y and x, -1 for auto calculation.  offsets::tuple of  float , optional, default=[0.5,0.5] : Priorbox center offsets, y and x respectively   source  #  MXNet.mx._contrib_MultiBoxTarget     Method .  _contrib_MultiBoxTarget(anchor, label, cls_pred, overlap_threshold, ignore_label, negative_mining_ratio, negative_mining_thresh, minimum_negative_samples, variances)  Compute Multibox training targets  Arguments   anchor::NDArray-or-SymbolicNode : Generated anchor boxes.  label::NDArray-or-SymbolicNode : Object detection labels.  cls_pred::NDArray-or-SymbolicNode : Class predictions.  overlap_threshold::float, optional, default=0.5 : Anchor-GT overlap threshold to be regarded as a positive match.  ignore_label::float, optional, default=-1 : Label for ignored anchors.  negative_mining_ratio::float, optional, default=-1 : Max negative to positive samples ratio, use -1 to disable mining  negative_mining_thresh::float, optional, default=0.5 : Threshold used for negative mining.  minimum_negative_samples::int, optional, default='0' : Minimum number of negative samples.  variances::tuple of  float , optional, default=[0.1,0.1,0.2,0.2] : Variances to be encoded in box regression target.   source  #  MXNet.mx._contrib_MultiProposal     Method .  _contrib_MultiProposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)  Generate region proposals via RPN  Arguments   cls_score::NDArray-or-SymbolicNode : Score of how likely proposal is object.  bbox_pred::NDArray-or-SymbolicNode : BBox Predicted deltas from anchors for proposals  im_info::NDArray-or-SymbolicNode : Image size and scale.  rpn_pre_nms_top_n::int, optional, default='6000' : Number of top scoring boxes to keep after applying NMS to RPN proposals  rpn_post_nms_top_n::int, optional, default='300' : Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU  = this threshold  threshold::float, optional, default=0.7 : NMS value, below which to suppress.  rpn_min_size::int, optional, default='16' : Minimum height or width in proposal  scales::tuple of  float , optional, default=[4,8,16,32] : Used to generate anchor windows by enumerating scales  ratios::tuple of  float , optional, default=[0.5,1,2] : Used to generate anchor windows by enumerating ratios  feature_stride::int, optional, default='16' : The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.  output_score::boolean, optional, default=0 : Add score to outputs  iou_loss::boolean, optional, default=0 : Usage of IoU Loss   source  #  MXNet.mx._contrib_PSROIPooling     Method .  _contrib_PSROIPooling(data, rois, spatial_scale, output_dim, pooled_size, group_size)  Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after PSROIPooling  Arguments   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers  output_dim::int, required : fix output dim  pooled_size::int, required : fix pooled size  group_size::int, optional, default='0' : fix group size   source  #  MXNet.mx._contrib_Proposal     Method .  _contrib_Proposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)  Generate region proposals via RPN  Arguments   cls_score::NDArray-or-SymbolicNode : Score of how likely proposal is object.  bbox_pred::NDArray-or-SymbolicNode : BBox Predicted deltas from anchors for proposals  im_info::NDArray-or-SymbolicNode : Image size and scale.  rpn_pre_nms_top_n::int, optional, default='6000' : Number of top scoring boxes to keep after applying NMS to RPN proposals  rpn_post_nms_top_n::int, optional, default='300' : Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU  = this threshold  threshold::float, optional, default=0.7 : NMS value, below which to suppress.  rpn_min_size::int, optional, default='16' : Minimum height or width in proposal  scales::tuple of  float , optional, default=[4,8,16,32] : Used to generate anchor windows by enumerating scales  ratios::tuple of  float , optional, default=[0.5,1,2] : Used to generate anchor windows by enumerating ratios  feature_stride::int, optional, default='16' : The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.  output_score::boolean, optional, default=0 : Add score to outputs  iou_loss::boolean, optional, default=0 : Usage of IoU Loss   source  #  MXNet.mx._contrib_SparseEmbedding     Method .  _contrib_SparseEmbedding(data, weight, input_dim, output_dim, dtype)  Maps integer indices to vector representations (embeddings).  This operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.  For an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).  If the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).  The storage type of weight must be  row_sparse , and the gradient of the weight will be of  row_sparse  storage type, too.  .. Note::  `SparseEmbedding` is designed for the use case where `input_dim` is very large (e.g. 100k).\nThe operator is available on both CPU and GPU.  Examples::  input_dim = 4   output_dim = 5  // Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]  // Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]  // Mapped input x to its vector representation y.   SparseEmbedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                                  [ 15.,  16.,  17.,  18.,  19.]],                              [[  0.,   1.,   2.,   3.,   4.],\n                             [ 10.,  11.,  12.,  13.,  14.]]]  Defined in src/operator/tensor/indexing_op.cc:L254  Arguments   data::NDArray-or-SymbolicNode : The input array to the embedding operator.  weight::NDArray-or-SymbolicNode : The embedding weight matrix.  input_dim::int, required : Vocabulary size of the input indices.  output_dim::int, required : Dimension of the embedding vectors.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Data type of weight.   source  #  MXNet.mx._contrib_bipartite_matching     Method .  _contrib_bipartite_matching(data, is_ascend, threshold, topk)  Compute bipartite matching.   The matching is performed on score matrix with shape [B, N, M]   B: batch_size  N: number of rows to match  M: number of columns as reference to be matched against.   Returns:   x : matched column indices. -1 indicating non-matched elements in rows.   y : matched row indices.  Note::  Zero gradients are back-propagated in this op for now.  Example::  s = [[0.5, 0.6], [0.1, 0.2], [0.3, 0.4]]\nx, y = bipartite_matching(x, threshold=1e-12, is_ascend=False)\nx = [1, -1, 0]\ny = [2\uff0c 0]  Defined in src/operator/contrib/bounding_box.cc:L169  Arguments   data::NDArray-or-SymbolicNode : The input  is_ascend::boolean, optional, default=0 : Use ascend order for scores instead of descending. Please set threshold accordingly.  threshold::float, required : Ignore matching when score   thresh, if is_ascend=false, or ignore score   thresh, if is_ascend=true.  topk::int, optional, default='-1' : Limit the number of matches to topk, set -1 for no limit   source  #  MXNet.mx._contrib_box_iou     Method .  _contrib_box_iou(lhs, rhs, format)  Bounding box overlap of two arrays.   The overlap is defined as Intersection-over-Union, aka, IOU.   lhs: (a_1, a_2, ..., a_n, 4) array  rhs: (b_1, b_2, ..., b_n, 4) array  output: (a_1, a_2, ..., a_n, b_1, b_2, ..., b_n) array   Note::  Zero gradients are back-propagated in this op for now.  Example::  x = [[0.5, 0.5, 1.0, 1.0], [0.0, 0.0, 0.5, 0.5]]\ny = [0.25, 0.25, 0.75, 0.75]\nbox_iou(x, y, format='corner') = [[0.1428], [0.1428]]  Defined in src/operator/contrib/bounding_box.cc:L123  Arguments   lhs::NDArray-or-SymbolicNode : The first input  rhs::NDArray-or-SymbolicNode : The second input  format::{'center', 'corner'},optional, default='corner' : The box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].  source  #  MXNet.mx._contrib_box_nms     Method .  _contrib_box_nms(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)  Apply non-maximum suppression to input.  The output will be sorted in descending order according to  score . Boxes with overlaps larger than  overlap_thresh  and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.  During back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.  Input requirements:   Input tensor have at least 2 dimensions, (n, k), any higher dims will be regarded   as batch, e.g. (a, b, c, d, n, k) == (a b c*d, n, k)   n is the number of boxes in each batch  k is the width of each box item.   By default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.   id_index : optional, use -1 to ignore, useful if  force_suppress=False , which means   we will skip highly overlapped boxes if one is  apple  while the other is  car .   coord_start : required, default=2, the starting index of the 4 coordinates.   Two formats are supported:    corner : [xmin, ymin, xmax, ymax]    center : [x, y, width, height]   score_index : required, default=1, box score/confidence.   When two boxes overlap IOU    overlap_thresh , the one with smaller score will be suppressed.   in_format  and  out_format : default='corner', specify in/out box formats.   Examples::  x = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]  exe.backward  in_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]  Defined in src/operator/contrib/bounding_box.cc:L82  Arguments   data::NDArray-or-SymbolicNode : The input  overlap_thresh::float, optional, default=0.5 : Overlapping(IoU) threshold to suppress object with smaller score.  topk::int, optional, default='-1' : Apply nms to topk boxes with descending scores, -1 to no restriction.  coord_start::int, optional, default='2' : Start index of the consecutive 4 coordinates.  score_index::int, optional, default='1' : Index of the scores/confidence of boxes.  id_index::int, optional, default='-1' : Optional, index of the class categories, -1 to disable.  force_suppress::boolean, optional, default=0 : Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category  in_format::{'center', 'corner'},optional, default='corner' : The input box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   out_format::{'center', 'corner'},optional, default='corner' : The output box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].  source  #  MXNet.mx._contrib_box_non_maximum_suppression     Method .  _contrib_box_non_maximum_suppression(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)  _contrib_box_non_maximum_suppression is an alias of _contrib_box_nms.  Apply non-maximum suppression to input.  The output will be sorted in descending order according to  score . Boxes with overlaps larger than  overlap_thresh  and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.  During back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.  Input requirements:   Input tensor have at least 2 dimensions, (n, k), any higher dims will be regarded   as batch, e.g. (a, b, c, d, n, k) == (a b c*d, n, k)   n is the number of boxes in each batch  k is the width of each box item.   By default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.   id_index : optional, use -1 to ignore, useful if  force_suppress=False , which means   we will skip highly overlapped boxes if one is  apple  while the other is  car .   coord_start : required, default=2, the starting index of the 4 coordinates.   Two formats are supported:    corner : [xmin, ymin, xmax, ymax]    center : [x, y, width, height]   score_index : required, default=1, box score/confidence.   When two boxes overlap IOU    overlap_thresh , the one with smaller score will be suppressed.   in_format  and  out_format : default='corner', specify in/out box formats.   Examples::  x = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]  exe.backward  in_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]  Defined in src/operator/contrib/bounding_box.cc:L82  Arguments   data::NDArray-or-SymbolicNode : The input  overlap_thresh::float, optional, default=0.5 : Overlapping(IoU) threshold to suppress object with smaller score.  topk::int, optional, default='-1' : Apply nms to topk boxes with descending scores, -1 to no restriction.  coord_start::int, optional, default='2' : Start index of the consecutive 4 coordinates.  score_index::int, optional, default='1' : Index of the scores/confidence of boxes.  id_index::int, optional, default='-1' : Optional, index of the class categories, -1 to disable.  force_suppress::boolean, optional, default=0 : Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category  in_format::{'center', 'corner'},optional, default='corner' : The input box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   out_format::{'center', 'corner'},optional, default='corner' : The output box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].  source  #  MXNet.mx._contrib_count_sketch     Method .  _contrib_count_sketch(data, h, s, out_dim, processing_batch_size)  Apply CountSketch to input: map a d-dimension data to k-dimension data\"  .. note::  count_sketch  is only available on GPU.  Assume input data has shape (N, d), sign hash table s has shape (N, d), index hash table h has shape (N, d) and mapping dimension out_dim = k, each element in s is either +1 or -1, each element in h is random integer from 0 to k-1. Then the operator computs:  .. math::    out[h[i]] += data[i] * s[i]  Example::  out_dim = 5    x = [[1.2, 2.5, 3.4],[3.2, 5.7, 6.6]]    h = [[0, 3, 4]]    s = [[1, -1, 1]]    mx.contrib.ndarray.count_sketch(data=x, h=h, s=s, out_dim = 5) = [[1.2, 0, 0, -2.5, 3.4],                                                                      [3.2, 0, 0, -5.7, 6.6]]  Defined in src/operator/contrib/count_sketch.cc:L67  Arguments   data::NDArray-or-SymbolicNode : Input data to the CountSketchOp.  h::NDArray-or-SymbolicNode : The index vector  s::NDArray-or-SymbolicNode : The sign vector  out_dim::int, required : The output dimension.  processing_batch_size::int, optional, default='32' : How many sketch vectors to process at one time.   source  #  MXNet.mx._contrib_ctc_loss     Method .  _contrib_ctc_loss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)  _contrib_ctc_loss is an alias of _contrib_CTCLoss.  Connectionist Temporal Classification Loss.  The shapes of the inputs and outputs:   data :  (sequence_length, batch_size, alphabet_size)  label :  (batch_size, label_sequence_length)  out :  (batch_size)   The  data  tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When  blank_label  is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.  $label$ is an index matrix of integers. When  blank_label  is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when  blank_label  is $\"last\"$, the value  (alphabet_size-1)  is reserved for blank label.  If a sequence of labels is shorter than  label_sequence_length , use the special padding value at the end of the sequence to conform it to the correct length. The padding value is  0  when  blank_label  is $\"first\"$, and  -1  otherwise.  For example, suppose the vocabulary is  [a, b, c] , and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When  blank_label  is $\"first\"$, we can index the labels as  {'a': 1, 'b': 2, 'c': 3} , and we reserve the 0-th channel for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]  When  blank_label  is $\"last\"$, we can index the labels as  {'a': 0, 'b': 1, 'c': 2} , and we reserve the channel index 3 for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]  $out$ is a list of CTC loss values, one per example in the batch.  See  Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks , A. Graves  et al . for more information on the definition and the algorithm.  Defined in src/operator/contrib/ctc_loss.cc:L115  Arguments   data::NDArray-or-SymbolicNode : Input data to the ctc_loss op.  label::NDArray-or-SymbolicNode : Ground-truth labels for the loss.  data_lengths::NDArray-or-SymbolicNode : Lengths of data for each of the samples. Only required when use_data_lengths is true.  label_lengths::NDArray-or-SymbolicNode : Lengths of labels for each of the samples. Only required when use_label_lengths is true.  use_data_lengths::boolean, optional, default=0 : Whether the data lenghts are decided by  data_lengths . If false, the lengths are equal to the max sequence length.  use_label_lengths::boolean, optional, default=0 : Whether the label lenghts are decided by  label_lengths , or derived from  padding_mask . If false, the lengths are derived from the first occurrence of the value of  padding_mask . The value of  padding_mask  is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See  blank_label .  blank_label::{'first', 'last'},optional, default='first' : Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.   source  #  MXNet.mx._contrib_dequantize     Method .  _contrib_dequantize(input, min_range, max_range, out_type)  Dequantize the input tensor into a float tensor. [min_range, max_range] are scalar floats that spcify the range for the output data.  Each value of the tensor will undergo the following:  out[i] = min_range + (in[i] * (max_range - min_range) / range(INPUT_TYPE))  here  range(T) = numeric_limits T ::max() - numeric_limits T ::min()  Defined in src/operator/contrib/dequantize.cc:L41  Arguments   input::NDArray-or-SymbolicNode : A ndarray/symbol of type  uint8  min_range::NDArray-or-SymbolicNode : The minimum scalar value possibly produced for the input  max_range::NDArray-or-SymbolicNode : The maximum scalar value possibly produced for the input  out_type::{'float32'}, required : Output data type.   source  #  MXNet.mx._contrib_fft     Method .  _contrib_fft(data, compute_size)  Apply 1D FFT to input\"  .. note::  fft  is only available on GPU.  Currently accept 2 input data shapes: (N, d) or (N1, N2, N3, d), data can only be real numbers. The output data has shape: (N, 2 d) or (N1, N2, N3, 2 d). The format is: [real0, imag0, real1, imag1, ...].  Example::  data = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.fft(data = mx.nd.array(data,ctx = mx.gpu(0)))  Defined in src/operator/contrib/fft.cc:L56  Arguments   data::NDArray-or-SymbolicNode : Input data to the FFTOp.  compute_size::int, optional, default='128' : Maximum size of sub-batch to be forwarded at one time   source  #  MXNet.mx._contrib_ifft     Method .  _contrib_ifft(data, compute_size)  Apply 1D ifft to input\"  .. note::  ifft  is only available on GPU.  Currently accept 2 input data shapes: (N, d) or (N1, N2, N3, d). Data is in format: [real0, imag0, real1, imag1, ...]. Last dimension must be an even number. The output data has shape: (N, d/2) or (N1, N2, N3, d/2). It is only the real part of the result.  Example::  data = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.ifft(data = mx.nd.array(data,ctx = mx.gpu(0)))  Defined in src/operator/contrib/ifft.cc:L58  Arguments   data::NDArray-or-SymbolicNode : Input data to the IFFTOp.  compute_size::int, optional, default='128' : Maximum size of sub-batch to be forwarded at one time   source  #  MXNet.mx._contrib_quantize     Method .  _contrib_quantize(input, min_range, max_range, out_type)  Quantize a input tensor from float to  out_type , with user-specified  min_range  and  max_range .  [min_range, max_range] are scalar floats that spcify the range for the input data. Each value of the tensor will undergo the following:  out[i] = (in[i] - min_range) * range(OUTPUT_TYPE) / (max_range - min_range)  here  range(T) = numeric_limits T ::max() - numeric_limits T ::min()  Defined in src/operator/contrib/quantize.cc:L41  Arguments   input::NDArray-or-SymbolicNode : A ndarray/symbol of type  float32  min_range::NDArray-or-SymbolicNode : The minimum scalar value possibly produced for the input  max_range::NDArray-or-SymbolicNode : The maximum scalar value possibly produced for the input  out_type::{'uint8'},optional, default='uint8' : Output data type.   source  #  MXNet.mx._copy     Method .  _copy(data)  Returns a copy of the input.  From:src/operator/tensor/elemwise_unary_op_basic.cc:112  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._copyto     Method .  _copyto(data)  Arguments   data::NDArray : input data   source  #  MXNet.mx._crop_assign     Method .  _crop_assign(lhs, rhs, begin, end, step)  _crop_assign is an alias of _slice_assign.  Assign the rhs to a cropped subset of lhs.  Requirements   output should be explicitly given and be the same as lhs.  lhs and rhs are of the same data type, and on the same device.   From:src/operator/tensor/matrix_op.cc:381  Arguments   lhs::NDArray-or-SymbolicNode : Source input  rhs::NDArray-or-SymbolicNode : value to assign  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.   source  #  MXNet.mx._crop_assign_scalar     Method .  _crop_assign_scalar(data, scalar, begin, end, step)  _crop_assign_scalar is an alias of _slice_assign_scalar.  (Assign the scalar to a cropped subset of the input.  Requirements   output should be explicitly given and be the same as input   )  From:src/operator/tensor/matrix_op.cc:406  Arguments   data::NDArray-or-SymbolicNode : Source input  scalar::float, optional, default=0 : The scalar value for assignment.  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.   source  #  MXNet.mx._cvcopyMakeBorder     Method .  _cvcopyMakeBorder(src, top, bot, left, right, type, value, values)  Pad image border with OpenCV.   Arguments   src::NDArray : source image  top::int, required : Top margin.  bot::int, required : Bottom margin.  left::int, required : Left margin.  right::int, required : Right margin.  type::int, optional, default='0' : Filling type (default=cv2.BORDER_CONSTANT).  value::double, optional, default=0 : (Deprecated! Use $values$ instead.) Fill with single value.  values::tuple of  double , optional, default=[] : Fill with value(RGB[A] or gray), up to 4 channels.   source  #  MXNet.mx._cvimdecode     Method .  _cvimdecode(buf, flag, to_rgb)  Decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.  Arguments   buf::NDArray : Buffer containing binary encoded image  flag::int, optional, default='1' : Convert decoded image to grayscale (0) or color (1).  to_rgb::boolean, optional, default=1 : Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).   source  #  MXNet.mx._cvimread     Method .  _cvimread(filename, flag, to_rgb)  Read and decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.  Arguments   filename::string, required : Name of the image file to be loaded.  flag::int, optional, default='1' : Convert decoded image to grayscale (0) or color (1).  to_rgb::boolean, optional, default=1 : Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).   source  #  MXNet.mx._cvimresize     Method .  _cvimresize(src, w, h, interp)  Resize image with OpenCV.   Arguments   src::NDArray : source image  w::int, required : Width of resized image.  h::int, required : Height of resized image.  interp::int, optional, default='1' : Interpolation method (default=cv2.INTER_LINEAR).   source  #  MXNet.mx._div_scalar     Method .  _div_scalar(data, scalar)  Divide an array with a scalar.  $_div_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._docsig     Method .  Generate docstring from function signature  source  #  MXNet.mx._equal     Method .  _equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._equal_scalar     Method .  _equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._get_ndarray_function_def     Method .  The libxmnet APIs are automatically imported from  libmxnet.so . The functions listed here operate on  NDArray  objects. The arguments to the functions are typically ordered as    func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)  unless  NDARRAY_ARG_BEFORE_SCALAR  is not set. In this case, the scalars are put before the input arguments:    func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)  If  ACCEPT_EMPTY_MUTATE_TARGET  is set. An overloaded function without the output arguments will also be defined:    func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)  Upon calling, the output arguments will be automatically initialized with empty NDArrays.  Those functions always return the output arguments. If there is only one output (the typical situation), that object ( NDArray ) is returned. Otherwise, a tuple containing all the outputs will be returned.  source  #  MXNet.mx._grad_add     Method .  _grad_add(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._greater     Method .  _greater(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._greater_equal     Method .  _greater_equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._greater_equal_scalar     Method .  _greater_equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._greater_scalar     Method .  _greater_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._hypot     Method .  _hypot(lhs, rhs)  Given the \"legs\" of a right triangle, return its hypotenuse.  Defined in src/operator/tensor/elemwise_binary_op_extended.cc:L79  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._hypot_scalar     Method .  _hypot_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._identity_with_attr_like_rhs     Method .  _identity_with_attr_like_rhs(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : First input.  rhs::NDArray-or-SymbolicNode : Second input.   source  #  MXNet.mx._imdecode     Method .  _imdecode(mean, index, x0, y0, x1, y1, c, size)  Decode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer  Arguments   mean::NDArray-or-SymbolicNode : image mean  index::int : buffer position for output  x0::int : x0  y0::int : y0  x1::int : x1  y1::int : y1  c::int : channel  size::int : length of str_img   source  #  MXNet.mx._lesser     Method .  _lesser(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._lesser_equal     Method .  _lesser_equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._lesser_equal_scalar     Method .  _lesser_equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._lesser_scalar     Method .  _lesser_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._linalg_gelqf     Method .  _linalg_gelqf(A)  LQ factorization for general matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , we compute the LQ factorization (LAPACK  gelqf , followed by  orglq ).  A  must have shape  (x, y)  with  x  = y , and must have full rank  =x . The LQ factorization consists of  L  with shape  (x, x)  and  Q  with shape  (x, y) , so that:  A  =  L  *  Q  Here,  L  is lower triangular (upper triangle equal to zero) with nonzero diagonal, and  Q  is row-orthonormal, meaning that  Q  *  Q \\ :sup: T  is equal to the identity matrix of shape  (x, x) .  If  n 2 ,  gelqf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]  // Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]  Defined in src/operator/tensor/la_op.cc:L529  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be factorized   source  #  MXNet.mx._linalg_gemm     Method .  _linalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)  Performs general matrix multiplication and accumulation. Input are tensors  A ,  B ,  C , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B ) +  beta  *  C  Here,  alpha  and  beta  are scalar parameters, and  op()  is either the identity or matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]  // Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]  Defined in src/operator/tensor/la_op.cc:L69  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  C::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.  beta::double, optional, default=1 : Scalar factor multiplied with C.   source  #  MXNet.mx._linalg_gemm2     Method .  _linalg_gemm2(A, B, transpose_a, transpose_b, alpha)  Performs general matrix multiplication. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B )  Here  alpha  is a scalar parameter and  op()  is either the identity or the matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]  // Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]  Defined in src/operator/tensor/la_op.cc:L128  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.   source  #  MXNet.mx._linalg_potrf     Method .  _linalg_potrf(A)  Performs Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the Cholesky factor  L  of the symmetric, positive definite matrix  A  is computed.  L  is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:  A  =  L  *  L \\ :sup: T  If  n 2 ,  potrf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]  // Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]  Defined in src/operator/tensor/la_op.cc:L178  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be decomposed   source  #  MXNet.mx._linalg_potri     Method .  _linalg_potri(A)  Performs matrix inversion from a Cholesky factorization. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:  out  =  A \\ :sup: -T  *  A \\ :sup: -1  In other words, if  A  is the Cholesky factor of a symmetric positive definite matrix  B  (obtained by  potrf ), then  out  =  B \\ :sup: -1  If  n 2 ,  potri  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  .. note:: Use this operator only if you are certain you need the inverse of  B , and           cannot use the Cholesky factor  A  ( potrf ), together with backsubstitution           ( trsm ). The latter is numerically much safer, and also cheaper.  Examples::  // Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]  // Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]  Defined in src/operator/tensor/la_op.cc:L236  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices   source  #  MXNet.mx._linalg_sumlogdiag     Method .  _linalg_sumlogdiag(A)  Computes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).  If  n 2 ,  sumlogdiag  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]  // Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]  Defined in src/operator/tensor/la_op.cc:L405  Arguments   A::NDArray-or-SymbolicNode : Tensor of square matrices   source  #  MXNet.mx._linalg_syevd     Method .  _linalg_syevd(A)  Eigendecomposition for symmetric matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  must be symmetric, of shape  (x, x) . We compute the eigendecomposition, resulting in the orthonormal matrix  U  of eigenvectors, shape  (x, x) , and the vector  L  of eigenvalues, shape  (x,) , so that:  U  *  A  =  diag(L)  *  U  Here:  U  *  U \\ :sup: T  =  U \\ :sup: T  *  U  =  I  where  I  is the identity matrix. Also,  L(0)  = L(1)  = L(2)  = ...  (ascending order).  If  n 2 ,  syevd  is performed separately on the trailing two dimensions of  A  (batch mode). In this case,  U  has  n  dimensions like  A , and  L  has  n-1  dimensions.  .. note:: The operator supports float32 and float64 data types only.  .. note:: Derivatives for this operator are defined only if  A  is such that all its           eigenvalues are distinct, and the eigengaps are not too small. If you need           gradients, do not apply this operator to matrices with multiple eigenvalues.  Examples::  // Single symmetric eigendecomposition    A = [[1., 2.], [2., 4.]]    U, L = syevd(A)    U = [[0.89442719, -0.4472136],         [0.4472136, 0.89442719]]    L = [0., 5.]  // Batch symmetric eigendecomposition    A = [[[1., 2.], [2., 4.]],         [[1., 2.], [2., 5.]]]    U, L = syevd(A)    U = [[[0.89442719, -0.4472136],          [0.4472136, 0.89442719]],         [[0.92387953, -0.38268343],          [0.38268343, 0.92387953]]]    L = [[0., 5.],         [0.17157288, 5.82842712]]  Defined in src/operator/tensor/la_op.cc:L598  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be factorized   source  #  MXNet.mx._linalg_syrk     Method .  _linalg_syrk(A, transpose, alpha)  Multiplication of matrix with its transpose. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the operator performs the BLAS3 function  syrk :  out  =  alpha  *  A  *  A \\ :sup: T  if  transpose=False , or  out  =  alpha  *  A \\ :sup: T  \\ *  A  if  transpose=True .  If  n 2 ,  syrk  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]  // Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]  Defined in src/operator/tensor/la_op.cc:L461  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  transpose::boolean, optional, default=0 : Use transpose of input matrix.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.   source  #  MXNet.mx._linalg_trmm     Method .  _linalg_trmm(A, B, transpose, rightside, alpha)  Performs multiplication with a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trmm :  out  =  alpha  *  op \\ ( A ) *  B  if  rightside=False , or  out  =  alpha  *  B  *  op \\ ( A )  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trmm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]  // Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L293  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.   source  #  MXNet.mx._linalg_trsm     Method .  _linalg_trsm(A, B, transpose, rightside, alpha)  Solves matrix equation involving a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trsm , solving for  out  in:  op \\ ( A ) *  out  =  alpha  *  B  if  rightside=False , or  out  *  op \\ ( A ) =  alpha  *  B  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trsm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]  // Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L356  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.   source  #  MXNet.mx._maximum     Method .  _maximum(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._maximum_scalar     Method .  _maximum_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._minimum     Method .  _minimum(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._minimum_scalar     Method .  _minimum_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._minus!     Method .  _minus!(x::NDArray, y::NDArray)  source  #  MXNet.mx._minus     Method .  _minus(x::NDArray, y::NDArray)  source  #  MXNet.mx._minus_scalar     Method .  _minus_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._mod!     Method .  _mod!(x::NDArray, y::NDArray)  source  #  MXNet.mx._mod     Method .  _mod(x::NDArray, y::NDArray)  source  #  MXNet.mx._mod_scalar!     Method .  _mod_scalar!(x::NDArray, y::Real)  source  #  MXNet.mx._mod_scalar     Method .  _mod_scalar(x::NDArray, y::Real)  source  #  MXNet.mx._mul     Method .  _mul(lhs, rhs)  _mul is an alias of elemwise_mul.  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._mul_scalar     Method .  _mul_scalar(data, scalar)  Multiply an array with a scalar.  $_mul_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._not_equal     Method .  _not_equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._not_equal_scalar     Method .  _not_equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._onehot_encode     Method .  _onehot_encode(lhs, rhs)  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx._plus!     Method .  _plus!(x::NDArray, y::NDArray)  source  #  MXNet.mx._plus     Method .  _plus(x::NDArray, y::NDArray)  source  #  MXNet.mx._plus_scalar     Method .  _plus_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._power     Method .  _power(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._power_scalar     Method .  _power_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._random_exponential     Method .  _random_exponential(lam, shape, ctx, dtype)  Draw random samples from an exponential distribution.  Samples are distributed according to an exponential distribution parametrized by  lambda  (rate).  Example::  exponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]  Defined in src/operator/random/sample_op.cc:L115  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the exponential distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._random_gamma     Method .  _random_gamma(alpha, beta, shape, ctx, dtype)  Draw random samples from a gamma distribution.  Samples are distributed according to a gamma distribution parametrized by  alpha  (shape) and  beta  (scale).  Example::  gamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]  Defined in src/operator/random/sample_op.cc:L100  Arguments   alpha::float, optional, default=1 : Alpha parameter (shape) of the gamma distribution.  beta::float, optional, default=1 : Beta parameter (scale) of the gamma distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._random_generalized_negative_binomial     Method .  _random_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)  Draw random samples from a generalized negative binomial distribution.  Samples are distributed according to a generalized negative binomial distribution parametrized by  mu  (mean) and  alpha  (dispersion).  alpha  is defined as  1/k  where  k  is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.  Example::  generalized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]  Defined in src/operator/random/sample_op.cc:L168  Arguments   mu::float, optional, default=1 : Mean of the negative binomial distribution.  alpha::float, optional, default=1 : Alpha (dispersion) parameter of the negative binomial distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._random_negative_binomial     Method .  _random_negative_binomial(k, p, shape, ctx, dtype)  Draw random samples from a negative binomial distribution.  Samples are distributed according to a negative binomial distribution parametrized by  k  (limit of unsuccessful experiments) and  p  (failure probability in each experiment). Samples will always be returned as a floating point data type.  Example::  negative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]  Defined in src/operator/random/sample_op.cc:L149  Arguments   k::int, optional, default='1' : Limit of unsuccessful experiments.  p::float, optional, default=1 : Failure probability in each experiment.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._random_normal     Method .  _random_normal(loc, scale, shape, ctx, dtype)  Draw random samples from a normal (Gaussian) distribution.  .. note:: The existing alias $normal$ is deprecated.  Samples are distributed according to a normal distribution parametrized by  loc  (mean) and  scale  (standard deviation).  Example::  normal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]  Defined in src/operator/random/sample_op.cc:L85  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._random_poisson     Method .  _random_poisson(lam, shape, ctx, dtype)  Draw random samples from a Poisson distribution.  Samples are distributed according to a Poisson distribution parametrized by  lambda  (rate). Samples will always be returned as a floating point data type.  Example::  poisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]  Defined in src/operator/random/sample_op.cc:L132  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the Poisson distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._random_uniform     Method .  _random_uniform(low, high, shape, ctx, dtype)  Draw random samples from a uniform distribution.  .. note:: The existing alias $uniform$ is deprecated.  Samples are uniformly distributed over the half-open interval  [low, high)  (includes  low , but excludes  high ).  Example::  uniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]  Defined in src/operator/random/sample_op.cc:L66  Arguments   low::float, optional, default=0 : Lower bound of the distribution.  high::float, optional, default=1 : Upper bound of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._rdiv_scalar     Method .  _rdiv_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._rminus_scalar     Method .  _rminus_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._rmod_scalar!     Method .  _rmod_scalar!(x::NDArray, y::Real)  source  #  MXNet.mx._rmod_scalar     Method .  _rmod_scalar(x::NDArray, y::Real)  source  #  MXNet.mx._rpower_scalar     Method .  _rpower_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._sample_exponential     Method .  _sample_exponential(lam, shape, dtype)  Concurrent sampling from multiple exponential distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]  // Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]  Defined in src/operator/random/multisample_op.cc:L284  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._sample_gamma     Method .  _sample_gamma(alpha, shape, dtype, beta)  Concurrent sampling from multiple gamma distributions with parameters  alpha  (shape) and  beta  (scale).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  alpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]  // Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]  // Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]  Defined in src/operator/random/multisample_op.cc:L282  Arguments   alpha::NDArray-or-SymbolicNode : Alpha (shape) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  beta::NDArray-or-SymbolicNode : Beta (scale) parameters of the distributions.   source  #  MXNet.mx._sample_generalized_negative_binomial     Method .  _sample_generalized_negative_binomial(mu, shape, dtype, alpha)  Concurrent sampling from multiple generalized negative binomial distributions with parameters  mu  (mean) and  alpha  (dispersion).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  mu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]  // Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]  // Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]  Defined in src/operator/random/multisample_op.cc:L293  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  alpha::NDArray-or-SymbolicNode : Alpha (dispersion) parameters of the distributions.   source  #  MXNet.mx._sample_multinomial     Method .  _sample_multinomial(data, shape, get_prob, dtype)  Concurrent sampling from multiple multinomial distributions.  data  is an  n  dimensional array whose last dimension has length  k , where  k  is the number of possible outcomes of each multinomial distribution. This operator will draw  shape  samples from each distribution. If shape is empty one sample will be drawn from each distribution.  If  get_prob  is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.  Note that the input distribution must be normalized, i.e.  data  must sum to 1 along its last axis.  Examples::  probs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]  // Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]  // Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]  // requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]  Arguments   data::NDArray-or-SymbolicNode : Distribution probabilities. Must sum to one on the last axis.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  get_prob::boolean, optional, default=0 : Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.  dtype::{'int32'},optional, default='int32' : DType of the output in case this can't be inferred. Only support int32 for now.   source  #  MXNet.mx._sample_negative_binomial     Method .  _sample_negative_binomial(k, shape, dtype, p)  Concurrent sampling from multiple negative binomial distributions with parameters  k  (failure limit) and  p  (failure probability).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  k = [ 20, 49 ]    p = [ 0.4 , 0.77 ]  // Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]  // Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]  Defined in src/operator/random/multisample_op.cc:L289  Arguments   k::NDArray-or-SymbolicNode : Limits of unsuccessful experiments.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  p::NDArray-or-SymbolicNode : Failure probabilities in each experiment.   source  #  MXNet.mx._sample_normal     Method .  _sample_normal(mu, shape, dtype, sigma)  Concurrent sampling from multiple normal distributions with parameters  mu  (mean) and  sigma  (standard deviation).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  mu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]  // Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]  Defined in src/operator/random/multisample_op.cc:L279  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  sigma::NDArray-or-SymbolicNode : Standard deviations of the distributions.   source  #  MXNet.mx._sample_poisson     Method .  _sample_poisson(lam, shape, dtype)  Concurrent sampling from multiple Poisson distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Samples will always be returned as a floating point data type.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]  // Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]  Defined in src/operator/random/multisample_op.cc:L286  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx._sample_uniform     Method .  _sample_uniform(low, shape, dtype, high)  Concurrent sampling from multiple uniform distributions on the intervals given by  [low,high) .  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  low = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]  // Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]  Defined in src/operator/random/multisample_op.cc:L277  Arguments   low::NDArray-or-SymbolicNode : Lower bounds of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  high::NDArray-or-SymbolicNode : Upper bounds of the distributions.   source  #  MXNet.mx._scatter_elemwise_div     Method .  _scatter_elemwise_div(lhs, rhs)  Divides arguments element-wise.  If the left-hand-side input is 'row_sparse', then only the values which exist in the left-hand sparse array are computed.  The 'missing' values are ignored.  The storage type of $_scatter_elemwise_div$ output depends on storage types of inputs   _scatter_elemwise_div(row_sparse, row_sparse) = row_sparse  _scatter_elemwise_div(row_sparse, dense) = row_sparse  _scatter_elemwise_div(row_sparse, csr) = row_sparse  otherwise, $_scatter_elemwise_div$ behaves exactly like elemwise_div and generates output   with default storage  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._scatter_minus_scalar     Method .  _scatter_minus_scalar(data, scalar)  Subtracts a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.  The storage type of $_scatter_minus_scalar$ output depends on storage types of inputs   _scatter_minus_scalar(row_sparse, scalar) = row_sparse  _scatter_minus_scalar(csr, scalar) = csr  otherwise, $_scatter_minus_scalar$ behaves exactly like _minus_scalar and generates output   with default storage  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._scatter_plus_scalar     Method .  _scatter_plus_scalar(data, scalar)  Adds a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.  The storage type of $_scatter_plus_scalar$ output depends on storage types of inputs   _scatter_plus_scalar(row_sparse, scalar) = row_sparse  _scatter_plus_scalar(csr, scalar) = csr  otherwise, $_scatter_plus_scalar$ behaves exactly like _plus_scalar and generates output   with default storage  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx._scatter_set_nd     Method .  _scatter_set_nd(data, indices, shape)  This operator has the same functionality as scatter_nd except that it does not reset the elements not indexed by the input index  NDArray  in the input data  NDArray .  .. note:: This operator is for internal use only.  Examples::  data = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   out = [[1, 1], [1, 1]]   scatter_nd(data=data, indices=indices, out=out)   out = [[0, 1], [2, 3]]  Arguments   data::NDArray-or-SymbolicNode : data  indices::NDArray-or-SymbolicNode : indices  shape::Shape(tuple), required : Shape of output.   source  #  MXNet.mx._set_value     Method .  _set_value(src)  Arguments   src::real_t : Source input to the function.   source  #  MXNet.mx._slice_assign     Method .  _slice_assign(lhs, rhs, begin, end, step)  Assign the rhs to a cropped subset of lhs.  Requirements   output should be explicitly given and be the same as lhs.  lhs and rhs are of the same data type, and on the same device.   From:src/operator/tensor/matrix_op.cc:381  Arguments   lhs::NDArray-or-SymbolicNode : Source input  rhs::NDArray-or-SymbolicNode : value to assign  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.   source  #  MXNet.mx._slice_assign_scalar     Method .  _slice_assign_scalar(data, scalar, begin, end, step)  (Assign the scalar to a cropped subset of the input.  Requirements   output should be explicitly given and be the same as input   )  From:src/operator/tensor/matrix_op.cc:406  Arguments   data::NDArray-or-SymbolicNode : Source input  scalar::float, optional, default=0 : The scalar value for assignment.  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.   source  #  MXNet.mx._sparse_ElementWiseSum     Method .  _sparse_ElementWiseSum(args)  _sparse_ElementWiseSum is an alias of add_n.  Note : _sparse_ElementWiseSum takes variable number of positional inputs. So instead of calling as _sparse_ElementWiseSum([x, y, z], num_args=3), one should call via _sparse_ElementWiseSum(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments   source  #  MXNet.mx._sparse_abs     Method .  _sparse_abs(data)  _sparse_abs is an alias of abs.  Returns element-wise absolute value of the input.  Example::  abs([-2, 0, 3]) = [2, 0, 3]  The storage type of $abs$ output depends upon the input storage type:   abs(default) = default  abs(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L386  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_adam_update     Method .  _sparse_adam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)  _sparse_adam_update is an alias of adam_update.  Update function for Adam optimizer. Adam is seen as a generalization of AdaGrad.  Adam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).  .. math::  g_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }  It updates the weights using::  m = beta1 m + (1-beta1) grad  v = beta2 v + (1-beta2) (grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)  If w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::  for row in grad.indices:      m[row] = beta1 m[row] + (1-beta1) grad[row]      v[row] = beta2 v[row] + (1-beta2) (grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)  Defined in src/operator/optimizer_op.cc:L208  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mean::NDArray-or-SymbolicNode : Moving mean  var::NDArray-or-SymbolicNode : Moving variance  lr::float, required : Learning rate  beta1::float, optional, default=0.9 : The decay rate for the 1st moment estimates.  beta2::float, optional, default=0.999 : The decay rate for the 2nd moment estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx._sparse_add_n     Method .  _sparse_add_n(args)  _sparse_add_n is an alias of add_n.  Note : _sparse_add_n takes variable number of positional inputs. So instead of calling as _sparse_add_n([x, y, z], num_args=3), one should call via _sparse_add_n(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments   source  #  MXNet.mx._sparse_arccos     Method .  _sparse_arccos(data)  _sparse_arccos is an alias of arccos.  Returns element-wise inverse cosine of the input array.  The input should be in range  [-1, 1] . The output is in the closed interval :math: [0, \\pi]  .. math::    arccos([-1, -.707, 0, .707, 1]) = [\\pi, 3\\pi/4, \\pi/2, \\pi/4, 0]  The storage type of $arccos$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L123  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_arccosh     Method .  _sparse_arccosh(data)  _sparse_arccosh is an alias of arccosh.  Returns the element-wise inverse hyperbolic cosine of the input array, \ncomputed element-wise.  The storage type of $arccosh$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L264  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_arcsin     Method .  _sparse_arcsin(data)  _sparse_arcsin is an alias of arcsin.  Returns element-wise inverse sine of the input array.  The input should be in the range  [-1, 1] . The output is in the closed interval of [:math: -\\pi/2 , :math: \\pi/2 ].  .. math::    arcsin([-1, -.707, 0, .707, 1]) = [-\\pi/2, -\\pi/4, 0, \\pi/4, \\pi/2]  The storage type of $arcsin$ output depends upon the input storage type:   arcsin(default) = default  arcsin(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L104  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_arcsinh     Method .  _sparse_arcsinh(data)  _sparse_arcsinh is an alias of arcsinh.  Returns the element-wise inverse hyperbolic sine of the input array, \ncomputed element-wise.  The storage type of $arcsinh$ output depends upon the input storage type:   arcsinh(default) = default  arcsinh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L250  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_arctan     Method .  _sparse_arctan(data)  _sparse_arctan is an alias of arctan.  Returns element-wise inverse tangent of the input array.  The output is in the closed interval :math: [-\\pi/2, \\pi/2]  .. math::    arctan([-1, 0, 1]) = [-\\pi/4, 0, \\pi/4]  The storage type of $arctan$ output depends upon the input storage type:   arctan(default) = default  arctan(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L144  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_arctanh     Method .  _sparse_arctanh(data)  _sparse_arctanh is an alias of arctanh.  Returns the element-wise inverse hyperbolic tangent of the input array, \ncomputed element-wise.  The storage type of $arctanh$ output depends upon the input storage type:   arctanh(default) = default  arctanh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L281  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_cast_storage     Method .  _sparse_cast_storage(data, stype)  _sparse_cast_storage is an alias of cast_storage.  Casts tensor storage type to the new type.  When an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:   for csr, zero values will not be retained  for row_sparse, row slices of all zeros will not be retained   The storage type of $cast_storage$ output depends on stype parameter:   cast_storage(csr, 'default') = default  cast_storage(row_sparse, 'default') = default  cast_storage(default, 'csr') = csr  cast_storage(default, 'row_sparse') = row_sparse   Example::  dense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]  Defined in src/operator/tensor/cast_storage.cc:L69  Arguments   data::NDArray-or-SymbolicNode : The input.  stype::{'csr', 'default', 'row_sparse'}, required : Output storage type.   source  #  MXNet.mx._sparse_ceil     Method .  _sparse_ceil(data)  _sparse_ceil is an alias of ceil.  Returns element-wise ceiling of the input.  The ceil of the scalar x is the smallest integer i, such that i  = x.  Example::  ceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]  The storage type of $ceil$ output depends upon the input storage type:   ceil(default) = default  ceil(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L464  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_clip     Method .  _sparse_clip(data, a_min, a_max)  _sparse_clip is an alias of clip.  Clips (limits) the values in an array.  Given an interval, values outside the interval are clipped to the interval edges. Clipping $x$ between  a_min  and  a_x  would be::  clip(x, a_min, a_max) = max(min(x, a_max), a_min))  Example::  x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nclip(x,1,8) = [ 1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  8.]  The storage type of $clip$ output depends on storage types of inputs and the a_min, a_max \nparameter values:   clip(default) = default  clip(row_sparse, a_min  = 0, a_max  = 0) = row_sparse  clip(csr, a_min  = 0, a_max  = 0) = csr  clip(row_sparse, a_min   0, a_max   0) = default  clip(row_sparse, a_min   0, a_max   0) = default  clip(csr, a_min   0, a_max   0) = csr  clip(csr, a_min   0, a_max   0) = csr   Defined in src/operator/tensor/matrix_op.cc:L486  Arguments   data::NDArray-or-SymbolicNode : Input array.  a_min::float, required : Minimum value  a_max::float, required : Maximum value   source  #  MXNet.mx._sparse_cos     Method .  _sparse_cos(data)  _sparse_cos is an alias of cos.  Computes the element-wise cosine of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    cos([0, \\pi/4, \\pi/2]) = [1, 0.707, 0]  The storage type of $cos$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L63  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_cosh     Method .  _sparse_cosh(data)  _sparse_cosh is an alias of cosh.  Returns the hyperbolic cosine  of the input array, computed element-wise.  .. math::    cosh(x) = 0.5\\times(exp(x) + exp(-x))  The storage type of $cosh$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L216  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_degrees     Method .  _sparse_degrees(data)  _sparse_degrees is an alias of degrees.  Converts each element of the input array from radians to degrees.  .. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]  The storage type of $degrees$ output depends upon the input storage type:   degrees(default) = default  degrees(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L163  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_dot     Method .  _sparse_dot(lhs, rhs, transpose_a, transpose_b)  _sparse_dot is an alias of dot.  Dot product of two arrays.  $dot$'s behavior depends on the input array dimensions:   1-D arrays: inner product of vectors  2-D arrays: matrix multiplication   N-D arrays: a sum product over the last axis of the first input and the first axis of the second input  For example, given 3-D $x$ with shape  (n,m,k)  and $y$ with shape  (k,r,s) , the result array will have shape  (n,m,r,s) . It is computed by::  dot(x,y)[i,j,a,b] = sum(x[i,j,:]*y[:,a,b])  Example::  x = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))   y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))   dot(x,y)[0,0,1,1] = 0   sum(x[0,0,:]*y[:,1,1]) = 0    The storage type of $dot$ output depends on storage types of inputs and transpose options:   dot(csr, default) = default  dot(csr.T, default) = row_sparse  dot(csr, row_sparse) = default  dot(default, csr) = csr  otherwise, $dot$ generates output with default storage   Defined in src/operator/tensor/dot.cc:L62  Arguments   lhs::NDArray-or-SymbolicNode : The first input  rhs::NDArray-or-SymbolicNode : The second input  transpose_a::boolean, optional, default=0 : If true then transpose the first input before dot.  transpose_b::boolean, optional, default=0 : If true then transpose the second input before dot.   source  #  MXNet.mx._sparse_elemwise_add     Method .  _sparse_elemwise_add(lhs, rhs)  _sparse_elemwise_add is an alias of elemwise_add.  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._sparse_elemwise_div     Method .  _sparse_elemwise_div(lhs, rhs)  _sparse_elemwise_div is an alias of elemwise_div.  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._sparse_elemwise_mul     Method .  _sparse_elemwise_mul(lhs, rhs)  _sparse_elemwise_mul is an alias of elemwise_mul.  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._sparse_elemwise_sub     Method .  _sparse_elemwise_sub(lhs, rhs)  _sparse_elemwise_sub is an alias of elemwise_sub.  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx._sparse_exp     Method .  _sparse_exp(data)  _sparse_exp is an alias of exp.  Returns element-wise exponential value of the input.  .. math::    exp(x) = e^x \\approx 2.718^x  Example::  exp([0, 1, 2]) = [1., 2.71828175, 7.38905621]  The storage type of $exp$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L642  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_expm1     Method .  _sparse_expm1(data)  _sparse_expm1 is an alias of expm1.  Returns $exp(x) - 1$ computed element-wise on the input.  This function provides greater precision than $exp(x) - 1$ for small values of $x$.  The storage type of $expm1$ output depends upon the input storage type:   expm1(default) = default  expm1(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L721  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_fix     Method .  _sparse_fix(data)  _sparse_fix is an alias of fix.  Returns element-wise rounded value to the nearest \ninteger towards zero of the input.  Example::  fix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]  The storage type of $fix$ output depends upon the input storage type:   fix(default) = default  fix(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L521  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_floor     Method .  _sparse_floor(data)  _sparse_floor is an alias of floor.  Returns element-wise floor of the input.  The floor of the scalar x is the largest integer i, such that i  = x.  Example::  floor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]  The storage type of $floor$ output depends upon the input storage type:   floor(default) = default  floor(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L483  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_ftrl_update     Method .  _sparse_ftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)  _sparse_ftrl_update is an alias of ftrl_update.  Update function for Ftrl optimizer. Referenced from  Ad Click Prediction: a View from the Trenches , available at http://dl.acm.org/citation.cfm?id=2488200.  It updates the weights using::  rescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad 2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad 2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z)   lamda1)  If w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::  for row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row] 2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row] 2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row])   lamda1)  Defined in src/operator/optimizer_op.cc:L341  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  z::NDArray-or-SymbolicNode : z  n::NDArray-or-SymbolicNode : Square of grad  lr::float, required : Learning rate  lamda1::float, optional, default=0.01 : The L1 regularization coefficient.  beta::float, optional, default=1 : Per-Coordinate Learning Rate beta.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx._sparse_gamma     Method .  _sparse_gamma(data)  _sparse_gamma is an alias of gamma.  Returns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.  The storage type of $gamma$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_gammaln     Method .  _sparse_gammaln(data)  _sparse_gammaln is an alias of gammaln.  Returns element-wise log of the absolute value of the gamma function \nof the input.  The storage type of $gammaln$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_log     Method .  _sparse_log(data)  _sparse_log is an alias of log.  Returns element-wise Natural logarithmic value of the input.  The natural logarithm is logarithm in base  e , so that $log(exp(x)) = x$  The storage type of $log$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L654  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_log10     Method .  _sparse_log10(data)  _sparse_log10 is an alias of log10.  Returns element-wise Base-10 logarithmic value of the input.  $10**log10(x) = x$  The storage type of $log10$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L666  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_log1p     Method .  _sparse_log1p(data)  _sparse_log1p is an alias of log1p.  Returns element-wise $log(1 + x)$ value of the input.  This function is more accurate than $log(1 + x)$  for small $x$ so that :math: 1+x\\approx 1  The storage type of $log1p$ output depends upon the input storage type:   log1p(default) = default  log1p(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L703  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_log2     Method .  _sparse_log2(data)  _sparse_log2 is an alias of log2.  Returns element-wise Base-2 logarithmic value of the input.  $2**log2(x) = x$  The storage type of $log2$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L678  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_make_loss     Method .  _sparse_make_loss(data)  _sparse_make_loss is an alias of make_loss.  Make your own loss function in network construction.  This operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.  For example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::  cross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)  We will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.  The storage type of $make_loss$ output depends upon the input storage type:   make_loss(default) = default  make_loss(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L200  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_mean     Method .  _sparse_mean(data, axis, keepdims, exclude)  _sparse_mean is an alias of mean.  Computes the mean of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L101  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.    source  #  MXNet.mx._sparse_negative     Method .  _sparse_negative(data)  _sparse_negative is an alias of negative.  Numerical negative of the argument, element-wise.  The storage type of $negative$ output depends upon the input storage type:   negative(default) = default  negative(row_sparse) = row_sparse  negative(csr) = csr   Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_radians     Method .  _sparse_radians(data)  _sparse_radians is an alias of radians.  Converts each element of the input array from degrees to radians.  .. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]  The storage type of $radians$ output depends upon the input storage type:   radians(default) = default  radians(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L182  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_relu     Method .  _sparse_relu(data)  _sparse_relu is an alias of relu.  Computes rectified linear.  .. math::    max(features, 0)  The storage type of $relu$ output depends upon the input storage type:   relu(default) = default  relu(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L84  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_retain     Method .  _sparse_retain(data, indices)  pick rows specified by user input index array from a row sparse matrix and save them in the output sparse matrix.  Example::  data = [[1, 2], [3, 4], [5, 6]]   indices = [0, 1, 3]   shape = (4, 2)   rsp_in = row_sparse(data, indices)   to_retain = [0, 3]   rsp_out = retain(rsp_in, to_retain)   rsp_out.values = [[1, 2], [5, 6]]   rsp_out.indices = [0, 3]  The storage type of $retain$ output depends on storage types of inputs   retain(row_sparse, default) = row_sparse  otherwise, $retain$ is not supported   Defined in src/operator/tensor/sparse_retain.cc:L53  Arguments   data::NDArray-or-SymbolicNode : The input array for sparse_retain operator.  indices::NDArray-or-SymbolicNode : The index array of rows ids that will be retained.   source  #  MXNet.mx._sparse_rint     Method .  _sparse_rint(data)  _sparse_rint is an alias of rint.  Returns element-wise rounded value to the nearest integer of the input.  .. note::   For input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.  For input $-n.5$ both $rint$ and $round$ returns $-n-1$.   Example::  rint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]  The storage type of $rint$ output depends upon the input storage type:   rint(default) = default  rint(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L445  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_round     Method .  _sparse_round(data)  _sparse_round is an alias of round.  Returns element-wise rounded value to the nearest integer of the input.  Example::  round([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]  The storage type of $round$ output depends upon the input storage type:   round(default) = default  round(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L424  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_rsqrt     Method .  _sparse_rsqrt(data)  _sparse_rsqrt is an alias of rsqrt.  Returns element-wise inverse square-root value of the input.  .. math::    rsqrt(x) = 1/\\sqrt{x}  Example::  rsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]  The storage type of $rsqrt$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L585  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_sgd_mom_update     Method .  _sparse_sgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)  _sparse_sgd_mom_update is an alias of sgd_mom_update.  Momentum update function for Stochastic Gradient Descent (SDG) optimizer.  Momentum update has better convergence rates on neural networks. Mathematically it looks like below:  .. math::  v_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t  It updates the weights using::  v = momentum * v - learning_rate * gradient   weight += v  Where the parameter $momentum$ is the decay rate of momentum estimates at each epoch.  If weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::  for row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]  Defined in src/operator/optimizer_op.cc:L94  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mom::NDArray-or-SymbolicNode : Momentum  lr::float, required : Learning rate  momentum::float, optional, default=0 : The decay rate of momentum estimates at each epoch.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx._sparse_sgd_update     Method .  _sparse_sgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)  _sparse_sgd_update is an alias of sgd_update.  Update function for Stochastic Gradient Descent (SDG) optimizer.  It updates the weights using::  weight = weight - learning_rate * gradient  If weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::  for row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]  Defined in src/operator/optimizer_op.cc:L54  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  lr::float, required : Learning rate  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx._sparse_sigmoid     Method .  _sparse_sigmoid(data)  _sparse_sigmoid is an alias of sigmoid.  Computes sigmoid of x element-wise.  .. math::    y = 1 / (1 + exp(-x))  The storage type of $sigmoid$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L103  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_sign     Method .  _sparse_sign(data)  _sparse_sign is an alias of sign.  Returns element-wise sign of the input.  Example::  sign([-2, 0, 3]) = [-1, 0, 1]  The storage type of $sign$ output depends upon the input storage type:   sign(default) = default  sign(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L405  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_sin     Method .  _sparse_sin(data)  _sparse_sin is an alias of sin.  Computes the element-wise sine of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    sin([0, \\pi/4, \\pi/2]) = [0, 0.707, 1]  The storage type of $sin$ output depends upon the input storage type:   sin(default) = default  sin(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L46  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_sinh     Method .  _sparse_sinh(data)  _sparse_sinh is an alias of sinh.  Returns the hyperbolic sine of the input array, computed element-wise.  .. math::    sinh(x) = 0.5\\times(exp(x) - exp(-x))  The storage type of $sinh$ output depends upon the input storage type:   sinh(default) = default  sinh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L201  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_slice     Method .  _sparse_slice(data, begin, end, step)  _sparse_slice is an alias of slice.  Slices a region of the array.  .. note:: $crop$ is deprecated. Use $slice$ instead.  This function returns a sliced array between the indices given by  begin  and  end  with the corresponding  step .  For an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m  = n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.  The resulting array's  k -th dimension contains elements from the  k -th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).  If the  k -th elements are  None  in the sequence of  begin ,  end , and  step , the following rule will be used to set default values. If  s_k  is  None , set  s_k=1 . If  s_k   0 , set  b_k=0 ,  e_k=d_k ; else, set  b_k=d_k-1 ,  e_k=-1 .  The storage type of $slice$ output depends on storage types of inputs   slice(csr) = csr  otherwise, $slice$ generates output with default storage   .. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.  Example::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]  Defined in src/operator/tensor/matrix_op.cc:L355  Arguments   data::NDArray-or-SymbolicNode : Source input  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.   source  #  MXNet.mx._sparse_sqrt     Method .  _sparse_sqrt(data)  _sparse_sqrt is an alias of sqrt.  Returns element-wise square-root value of the input.  .. math::    \\textrm{sqrt}(x) = \\sqrt{x}  Example::  sqrt([4, 9, 16]) = [2, 3, 4]  The storage type of $sqrt$ output depends upon the input storage type:   sqrt(default) = default  sqrt(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L565  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_square     Method .  _sparse_square(data)  _sparse_square is an alias of square.  Returns element-wise squared value of the input.  .. math::    square(x) = x^2  Example::  square([2, 3, 4]) = [4, 9, 16]  The storage type of $square$ output depends upon the input storage type:   square(default) = default  square(row_sparse) = row_sparse  square(csr) = csr   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L542  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_stop_gradient     Method .  _sparse_stop_gradient(data)  _sparse_stop_gradient is an alias of BlockGrad.  Stops gradient computation.  Stops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.  Example::  v1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)  executor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]  executor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L167  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_sum     Method .  _sparse_sum(data, axis, keepdims, exclude)  _sparse_sum is an alias of sum.  Computes the sum of array elements over given axes.  .. Note::  sum  and  sum_axis  are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.  Example::  data = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]  sum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]  sum(data, axis=[1,2])   [ 12.  19.  27.]  data = [[1,2,0],           [3,0,1],           [4,1,0]]  csr = cast_storage(data, 'csr')  sum(csr, axis=0)   [ 8.  3.  1.]  sum(csr, axis=1)   [ 3.  4.  5.]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L85  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.    source  #  MXNet.mx._sparse_tan     Method .  _sparse_tan(data)  _sparse_tan is an alias of tan.  Computes the element-wise tangent of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    tan([0, \\pi/4, \\pi/2]) = [0, 1, -inf]  The storage type of $tan$ output depends upon the input storage type:   tan(default) = default  tan(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L83  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_tanh     Method .  _sparse_tanh(data)  _sparse_tanh is an alias of tanh.  Returns the hyperbolic tangent of the input array, computed element-wise.  .. math::    tanh(x) = sinh(x) / cosh(x)  The storage type of $tanh$ output depends upon the input storage type:   tanh(default) = default  tanh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L234  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_trunc     Method .  _sparse_trunc(data)  _sparse_trunc is an alias of trunc.  Return the element-wise truncated value of the input.  The truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.  Example::  trunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]  The storage type of $trunc$ output depends upon the input storage type:   trunc(default) = default  trunc(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L503  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx._sparse_zeros_like     Method .  _sparse_zeros_like(data)  _sparse_zeros_like is an alias of zeros_like.  Return an array of zeros with the same shape and type as the input array.  The storage type of $zeros_like$ output depends on the storage type of the input   zeros_like(row_sparse) = row_sparse  zeros_like(csr) = csr  zeros_like(default) = default   Examples::  x = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]  zeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]  Arguments   data::NDArray-or-SymbolicNode : The input   source  #  MXNet.mx._square_sum     Method .  _square_sum(data, axis, keepdims, exclude)  Computes the square sum of array elements over a given axis for row-sparse matrix. This is a temporary solution for fusing ops square and sum together for row-sparse matrix to save memory for storing gradients. It will become deprecated once the functionality of fusing operators is finished in the future.  Example::  dns = mx.nd.array([[0, 0], [1, 2], [0, 0], [3, 4], [0, 0]])   rsp = dns.tostype('row_sparse')   sum = mx.nd._internal._square_sum(rsp, axis=1)   sum = [0, 5, 0, 25, 0]  Defined in src/operator/tensor/square_sum.cc:L63  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.    source  #  MXNet.mx.adam_update     Method .  adam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)  Update function for Adam optimizer. Adam is seen as a generalization of AdaGrad.  Adam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).  .. math::  g_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }  It updates the weights using::  m = beta1 m + (1-beta1) grad  v = beta2 v + (1-beta2) (grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)  If w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::  for row in grad.indices:      m[row] = beta1 m[row] + (1-beta1) grad[row]      v[row] = beta2 v[row] + (1-beta2) (grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)  Defined in src/operator/optimizer_op.cc:L208  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mean::NDArray-or-SymbolicNode : Moving mean  var::NDArray-or-SymbolicNode : Moving variance  lr::float, required : Learning rate  beta1::float, optional, default=0.9 : The decay rate for the 1st moment estimates.  beta2::float, optional, default=0.999 : The decay rate for the 2nd moment estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx.add_n     Method .  add_n(args)  Note : add_n takes variable number of positional inputs. So instead of calling as add_n([x, y, z], num_args=3), one should call via add_n(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments   source  #  MXNet.mx.add_to!     Method .  add_to!(dst::NDArray, args::NDArrayOrReal...)  Add a bunch of arguments into  dst . Inplace updating.  source  #  MXNet.mx.argmax     Method .  argmax(data, axis, keepdims)  Returns indices of the maximum values along an axis.  In the case of multiple occurrences of maximum values, the indices corresponding to the first occurrence are returned.  Examples::  x = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]  // argmax along axis 0   argmax(x, axis=0) = [ 1.,  1.,  1.]  // argmax along axis 1   argmax(x, axis=1) = [ 2.,  2.]  // argmax along axis 1 keeping same dims as an input array   argmax(x, axis=1, keepdims=True) = [[ 2.],                                       [ 2.]]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L52  Arguments   data::NDArray-or-SymbolicNode : The input  axis::int or None, optional, default='None' : The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$  keepdims::boolean, optional, default=0 : If this is set to  True , the reduced axis is left in the result as dimension with size one.   source  #  MXNet.mx.argmax_channel     Method .  argmax_channel(data)  Returns argmax indices of each channel from the input array.  The result will be an NDArray of shape (num_channel,).  In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.  Examples::  x = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]  argmax_channel(x) = [ 2.,  2.]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L97  Arguments   data::NDArray-or-SymbolicNode : The input array   source  #  MXNet.mx.argmin     Method .  argmin(data, axis, keepdims)  Returns indices of the minimum values along an axis.  In the case of multiple occurrences of minimum values, the indices corresponding to the first occurrence are returned.  Examples::  x = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]  // argmin along axis 0   argmin(x, axis=0) = [ 0.,  0.,  0.]  // argmin along axis 1   argmin(x, axis=1) = [ 0.,  0.]  // argmin along axis 1 keeping same dims as an input array   argmin(x, axis=1, keepdims=True) = [[ 0.],                                       [ 0.]]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L77  Arguments   data::NDArray-or-SymbolicNode : The input  axis::int or None, optional, default='None' : The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$  keepdims::boolean, optional, default=0 : If this is set to  True , the reduced axis is left in the result as dimension with size one.   source  #  MXNet.mx.argsort     Method .  argsort(data, axis, is_ascend)  Returns the indices that would sort an input array along the given axis.  This function performs sorting along the given axis and returns an array of indices having same shape as an input array that index data in sorted order.  Examples::  x = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]  // sort along axis -1   argsort(x) = [[ 1.,  0.,  2.],                 [ 0.,  2.,  1.]]  // sort along axis 0   argsort(x, axis=0) = [[ 1.,  0.,  1.]                         [ 0.,  1.,  0.]]  // flatten and then sort   argsort(x) = [ 3.,  1.,  5.,  0.,  4.,  2.]  Defined in src/operator/tensor/ordering_op.cc:L176  Arguments   data::NDArray-or-SymbolicNode : The input array  axis::int or None, optional, default='-1' : Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=1 : Whether to sort in ascending or descending order.   source  #  MXNet.mx.batch_dot     Method .  batch_dot(lhs, rhs, transpose_a, transpose_b)  Batchwise dot product.  $batch_dot$ is used to compute dot product of $x$ and $y$ when $x$ and $y$ are data in batch, namely 3D arrays in shape of  (batch_size, :, :) .  For example, given $x$ with shape  (batch_size, n, m)  and $y$ with shape  (batch_size, m, k) , the result array will have shape  (batch_size, n, k) , which is computed by::  batch_dot(x,y)[i,:,:] = dot(x[i,:,:], y[i,:,:])  Defined in src/operator/tensor/dot.cc:L110  Arguments   lhs::NDArray-or-SymbolicNode : The first input  rhs::NDArray-or-SymbolicNode : The second input  transpose_a::boolean, optional, default=0 : If true then transpose the first input before dot.  transpose_b::boolean, optional, default=0 : If true then transpose the second input before dot.   source  #  MXNet.mx.batch_take     Method .  batch_take(a, indices)  Takes elements from a data batch.  .. note::    batch_take  is deprecated. Use  pick  instead.  Given an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::  output[i] = input[i, indices[i]]  Examples::  x = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]  // takes elements with specified indices   batch_take(x, [0,1,0]) = [ 1.  4.  5.]  Defined in src/operator/tensor/indexing_op.cc:L382  Arguments   a::NDArray-or-SymbolicNode : The input array  indices::NDArray-or-SymbolicNode : The index array   source  #  MXNet.mx.broadcast_add     Method .  broadcast_add(lhs, rhs)  Returns element-wise sum of the input arrays with broadcasting.  broadcast_plus  is an alias to the function  broadcast_add .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]  broadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_axes     Method .  broadcast_axes(data, axis, size)  broadcast_axes is an alias of broadcast_axis.  Broadcasts the input array over particular axes.  Broadcasting is allowed on axes with size 1, such as from  (2,1,3,1)  to  (2,8,3,9) . Elements will be duplicated on the broadcasted axes.  Example::  // given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]  // broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L207  Arguments   data::NDArray-or-SymbolicNode : The input  axis::Shape(tuple), optional, default=[] : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=[] : Target sizes of the broadcasting axes.   source  #  MXNet.mx.broadcast_axis     Method .  broadcast_axis(data, axis, size)  Broadcasts the input array over particular axes.  Broadcasting is allowed on axes with size 1, such as from  (2,1,3,1)  to  (2,8,3,9) . Elements will be duplicated on the broadcasted axes.  Example::  // given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]  // broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L207  Arguments   data::NDArray-or-SymbolicNode : The input  axis::Shape(tuple), optional, default=[] : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=[] : Target sizes of the broadcasting axes.   source  #  MXNet.mx.broadcast_div     Method .  broadcast_div(lhs, rhs)  Returns element-wise division of the input arrays with broadcasting.  Example::  x = [[ 6.,  6.,  6.],         [ 6.,  6.,  6.]]  y = [[ 2.],         [ 3.]]  broadcast_div(x, y) = [[ 3.,  3.,  3.],                           [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L157  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_equal     Method .  broadcast_equal(lhs, rhs)  Returns the result of element-wise  equal to  (==) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_equal(x, y) = [[ 0.,  0.,  0.],                             [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L46  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_greater     Method .  broadcast_greater(lhs, rhs)  Returns the result of element-wise  greater than  ( ) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_greater(x, y) = [[ 1.,  1.,  1.],                               [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L82  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_greater_equal     Method .  broadcast_greater_equal(lhs, rhs)  Returns the result of element-wise  greater than or equal to  ( =) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_greater_equal(x, y) = [[ 1.,  1.,  1.],                                     [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L100  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_hypot     Method .  broadcast_hypot(lhs, rhs)  Returns the hypotenuse of a right angled triangle, given its \"legs\" with broadcasting.  It is equivalent to doing :math: sqrt(x_1^2 + x_2^2) .  Example::  x = [[ 3.,  3.,  3.]]  y = [[ 4.],         [ 4.]]  broadcast_hypot(x, y) = [[ 5.,  5.,  5.],                             [ 5.,  5.,  5.]]  z = [[ 0.],         [ 4.]]  broadcast_hypot(x, z) = [[ 3.,  3.,  3.],                             [ 5.,  5.,  5.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L156  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_lesser     Method .  broadcast_lesser(lhs, rhs)  Returns the result of element-wise  lesser than  ( ) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_lesser(x, y) = [[ 0.,  0.,  0.],                              [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L118  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_lesser_equal     Method .  broadcast_lesser_equal(lhs, rhs)  Returns the result of element-wise  lesser than or equal to  ( =) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_lesser_equal(x, y) = [[ 0.,  0.,  0.],                                    [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L136  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_maximum     Method .  broadcast_maximum(lhs, rhs)  Returns element-wise maximum of the input arrays with broadcasting.  This function compares two input arrays and returns a new array having the element-wise maxima.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_maximum(x, y) = [[ 1.,  1.,  1.],                               [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L80  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_minimum     Method .  broadcast_minimum(lhs, rhs)  Returns element-wise minimum of the input arrays with broadcasting.  This function compares two input arrays and returns a new array having the element-wise minima.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_maximum(x, y) = [[ 0.,  0.,  0.],                               [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L115  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_minus     Method .  broadcast_minus(lhs, rhs)  broadcast_minus is an alias of broadcast_sub.  Returns element-wise difference of the input arrays with broadcasting.  broadcast_minus  is an alias to the function  broadcast_sub .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]  broadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_mod     Method .  broadcast_mod(lhs, rhs)  Returns element-wise modulo of the input arrays with broadcasting.  Example::  x = [[ 8.,  8.,  8.],         [ 8.,  8.,  8.]]  y = [[ 2.],         [ 3.]]  broadcast_mod(x, y) = [[ 0.,  0.,  0.],                           [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L190  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_mul     Method .  broadcast_mul(lhs, rhs)  Returns element-wise product of the input arrays with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_mul(x, y) = [[ 0.,  0.,  0.],                           [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L123  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_not_equal     Method .  broadcast_not_equal(lhs, rhs)  Returns the result of element-wise  not equal to  (!=) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_not_equal(x, y) = [[ 1.,  1.,  1.],                                 [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L64  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_plus     Method .  broadcast_plus(lhs, rhs)  broadcast_plus is an alias of broadcast_add.  Returns element-wise sum of the input arrays with broadcasting.  broadcast_plus  is an alias to the function  broadcast_add .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]  broadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_power     Method .  broadcast_power(lhs, rhs)  Returns result of first array elements raised to powers from second array, element-wise with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_power(x, y) = [[ 2.,  2.,  2.],                             [ 4.,  4.,  4.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L45  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_sub     Method .  broadcast_sub(lhs, rhs)  Returns element-wise difference of the input arrays with broadcasting.  broadcast_minus  is an alias to the function  broadcast_sub .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]  broadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function   source  #  MXNet.mx.broadcast_to     Method .  broadcast_to(data, shape)  Broadcasts the input array to a new shape.  Broadcasting is a mechanism that allows NDArrays to perform arithmetic operations with arrays of different shapes efficiently without creating multiple copies of arrays. Also see,  Broadcasting  https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html _ for more explanation.  Broadcasting is allowed on axes with size 1, such as from  (2,1,3,1)  to  (2,8,3,9) . Elements will be duplicated on the broadcasted axes.  For example::  broadcast_to([[1,2,3]], shape=(2,3)) = [[ 1.,  2.,  3.],                                            [ 1.,  2.,  3.]])  The dimension which you do not want to change can also be kept as  0  which means copy the original value. So with  shape=(2,0) , we will obtain the same result as in the above example.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L231  Arguments   data::NDArray-or-SymbolicNode : The input  shape::Shape(tuple), optional, default=[] : The shape of the desired array. We can set the dim to zero if it's same as the original. E.g  A = broadcast_to(B, shape=(10, 0, 0))  has the same meaning as  A = broadcast_axis(B, axis=0, size=10) .   source  #  MXNet.mx.cast     Method .  cast(data, dtype)  cast is an alias of Cast.  Casts all elements of the input to a new type.  .. note:: $Cast$ is deprecated. Use $cast$ instead.  Example::  cast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L311  Arguments   data::NDArray-or-SymbolicNode : The input.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Output data type.   source  #  MXNet.mx.cast_storage     Method .  cast_storage(data, stype)  Casts tensor storage type to the new type.  When an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:   for csr, zero values will not be retained  for row_sparse, row slices of all zeros will not be retained   The storage type of $cast_storage$ output depends on stype parameter:   cast_storage(csr, 'default') = default  cast_storage(row_sparse, 'default') = default  cast_storage(default, 'csr') = csr  cast_storage(default, 'row_sparse') = row_sparse   Example::  dense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]  Defined in src/operator/tensor/cast_storage.cc:L69  Arguments   data::NDArray-or-SymbolicNode : The input.  stype::{'csr', 'default', 'row_sparse'}, required : Output storage type.   source  #  MXNet.mx.choose_element_0index     Method .  choose_element_0index(lhs, rhs)  Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.concat     Method .  concat(data, num_args, dim)  concat is an alias of Concat.  Note : concat takes variable number of positional inputs. So instead of calling as concat([x, y, z], num_args=3), one should call via concat(x, y, z), and num_args will be determined automatically.  Joins input arrays along a given axis.  .. note::  Concat  is deprecated. Use  concat  instead.  The dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.  Example::  x = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]  concat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]  Note that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.  concat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]  Defined in src/operator/concat.cc:L104  Arguments   data::NDArray-or-SymbolicNode[] : List of arrays to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.   source  #  MXNet.mx.crop     Method .  crop(data, begin, end, step)  crop is an alias of slice.  Slices a region of the array.  .. note:: $crop$ is deprecated. Use $slice$ instead.  This function returns a sliced array between the indices given by  begin  and  end  with the corresponding  step .  For an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m  = n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.  The resulting array's  k -th dimension contains elements from the  k -th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).  If the  k -th elements are  None  in the sequence of  begin ,  end , and  step , the following rule will be used to set default values. If  s_k  is  None , set  s_k=1 . If  s_k   0 , set  b_k=0 ,  e_k=d_k ; else, set  b_k=d_k-1 ,  e_k=-1 .  The storage type of $slice$ output depends on storage types of inputs   slice(csr) = csr  otherwise, $slice$ generates output with default storage   .. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.  Example::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]  Defined in src/operator/tensor/matrix_op.cc:L355  Arguments   data::NDArray-or-SymbolicNode : Source input  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.   source  #  MXNet.mx.degrees     Method .  degrees(data)  Converts each element of the input array from radians to degrees.  .. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]  The storage type of $degrees$ output depends upon the input storage type:   degrees(default) = default  degrees(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L163  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.div_from!     Method .  div_from!(dst::NDArray, arg::NDArrayOrReal)  Elementwise divide a scalar or an  NDArray  of the same shape from  dst . Inplace updating.  source  #  MXNet.mx.elemwise_add     Method .  elemwise_add(lhs, rhs)  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx.elemwise_div     Method .  elemwise_div(lhs, rhs)  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx.elemwise_mul     Method .  elemwise_mul(lhs, rhs)  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx.elemwise_sub     Method .  elemwise_sub(lhs, rhs)  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input   source  #  MXNet.mx.fill     Method .  fill(x, dims, ctx=cpu())\nfill(x, dims...)  Create an  NDArray  filled with the value  x , like  Base.fill .  source  #  MXNet.mx.fill_element_0index     Method .  fill_element_0index(lhs, mhs, rhs)  Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  mhs::NDArray : Middle operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.fix     Method .  fix(data)  Returns element-wise rounded value to the nearest \ninteger towards zero of the input.  Example::  fix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]  The storage type of $fix$ output depends upon the input storage type:   fix(default) = default  fix(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L521  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.flatten     Method .  flatten(data)  flatten is an alias of Flatten.  Flattens the input array into a 2-D array by collapsing the higher dimensions.  .. note::  Flatten  is deprecated. Use  flatten  instead.  For an input array with shape $(d1, d2, ..., dk)$,  flatten  operation reshapes the input array into an output array of shape $(d1, d2 ... dk)$.  Example::  x = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]  Defined in src/operator/tensor/matrix_op.cc:L208  Arguments   data::NDArray-or-SymbolicNode : Input array.   source  #  MXNet.mx.flip     Method .  flip(data, axis)  flip is an alias of reverse.  Reverses the order of elements along given axis while preserving array shape.  Note: reverse and flip are equivalent. We use reverse in the following examples.  Examples::  x = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]  reverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]  reverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]  Defined in src/operator/tensor/matrix_op.cc:L662  Arguments   data::NDArray-or-SymbolicNode : Input data array  axis::Shape(tuple), required : The axis which to reverse elements.   source  #  MXNet.mx.ftml_update     Method .  ftml_update(weight, grad, d, v, z, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)  The FTML optimizer described in  FTML - Follow the Moving Leader in Deep Learning , available at http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf.  .. math::  g_t = \\nabla J(W_{t-1})\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n d_t = \\frac{ (1 - \\beta_1^t) }{ \\eta_t } (\\sqrt{ \\frac{ v_t }{ 1 - \\beta_2^t } } + \\epsilon)  \\sigma_t = d_t - \\beta_1 d_{t-1}  z_t = \\beta_1 z_{ t-1 } + (1 - \\beta_1^t) g_t - \\sigma_t W_{t-1}  W_t = - \\frac{ z_t }{ d_t }  Defined in src/operator/optimizer_op.cc:L161  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  d::NDArray-or-SymbolicNode : Internal state $d_t$  v::NDArray-or-SymbolicNode : Internal state $v_t$  z::NDArray-or-SymbolicNode : Internal state $z_t$  lr::float, required : Learning rate  beta1::float, optional, default=0.9 : The decay rate for the 1st moment estimates.  beta2::float, optional, default=0.999 : The decay rate for the 2nd moment estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx.ftrl_update     Method .  ftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)  Update function for Ftrl optimizer. Referenced from  Ad Click Prediction: a View from the Trenches , available at http://dl.acm.org/citation.cfm?id=2488200.  It updates the weights using::  rescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad 2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad 2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z)   lamda1)  If w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::  for row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row] 2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row] 2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row])   lamda1)  Defined in src/operator/optimizer_op.cc:L341  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  z::NDArray-or-SymbolicNode : z  n::NDArray-or-SymbolicNode : Square of grad  lr::float, required : Learning rate  lamda1::float, optional, default=0.01 : The L1 regularization coefficient.  beta::float, optional, default=1 : Per-Coordinate Learning Rate beta.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx.gammaln     Method .  gammaln(data)  Returns element-wise log of the absolute value of the gamma function \nof the input.  The storage type of $gammaln$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.gather_nd     Method .  gather_nd(data, indices)  Gather elements or slices from  data  and store to a tensor whose shape is defined by  indices .  gather_nd  and  scatter_nd  are inverse functions to each other.  Given  data  with shape  (X_0, X_1, ..., X_{N-1})  and indices with shape  (M, Y_0, ..., Y_{K-1}) , the output will have shape  (Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1}) , where  M  = N . If  M == N , output shape will simply be  (Y_0, ..., Y_{K-1}) .  The elements in output is defined as follows::  output[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}] = data[indices[0, y_0, ..., y_{K-1}],                                                       ...,                                                       indices[M-1, y_0, ..., y_{K-1}],                                                       x_M, ..., x_{N-1}]  Examples::  data = [[0, 1], [2, 3]]   indices = [[1, 1, 0], [0, 1, 0]]   gather_nd(data, indices) = [2, 3, 0]  Arguments   data::NDArray-or-SymbolicNode : data  indices::NDArray-or-SymbolicNode : indices   source  #  MXNet.mx.is_shared     Method .  is_shared(j_arr, arr)  Test whether  j_arr  is sharing data with  arr .  Arguments:   j_arr::Array : the Julia Array.  arr::NDArray : the  NDArray .   source  #  MXNet.mx.khatri_rao     Method .  khatri_rao(args)  Note : khatri_rao takes variable number of positional inputs. So instead of calling as khatri_rao([x, y, z], num_args=3), one should call via khatri_rao(x, y, z), and num_args will be determined automatically.  Computes the Khatri-Rao product of the input matrices.  Given a collection of :math: n  input matrices,  .. math::    A_1 \\in \\mathbb{R}^{M_1 \\times M}, \\ldots, A_n \\in \\mathbb{R}^{M_n \\times N},  the (column-wise) Khatri-Rao product is defined as the matrix,  .. math::    X = A_1 \\otimes \\cdots \\otimes A_n \\in \\mathbb{R}^{(M_1 \\cdots M_n) \\times N},  where the :math: k th column is equal to the column-wise outer product :math: {A_1}_k \\otimes \\cdots \\otimes {A_n}_k  where :math: {A_i}_k  is the kth column of the ith matrix.  Example::     A = mx.nd.array([[1, -1],                  [2, -3]]) B = mx.nd.array([[1, 4],                  [2, 5],                  [3, 6]]) C = mx.nd.khatri_rao(A, B) print(C.asnumpy())     [[  1.  -4.]    [  2.  -5.]    [  3.  -6.]    [  2. -12.]    [  4. -15.]    [  6. -18.]]  Defined in src/operator/contrib/krprod.cc:L108  Arguments   args::NDArray-or-SymbolicNode[] : Positional input matrices   source  #  MXNet.mx.linalg_gelqf     Method .  linalg_gelqf(A)  linalg_gelqf is an alias of _linalg_gelqf.  LQ factorization for general matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , we compute the LQ factorization (LAPACK  gelqf , followed by  orglq ).  A  must have shape  (x, y)  with  x  = y , and must have full rank  =x . The LQ factorization consists of  L  with shape  (x, x)  and  Q  with shape  (x, y) , so that:  A  =  L  *  Q  Here,  L  is lower triangular (upper triangle equal to zero) with nonzero diagonal, and  Q  is row-orthonormal, meaning that  Q  *  Q \\ :sup: T  is equal to the identity matrix of shape  (x, x) .  If  n 2 ,  gelqf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]  // Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]  Defined in src/operator/tensor/la_op.cc:L529  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be factorized   source  #  MXNet.mx.linalg_gemm     Method .  linalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)  linalg_gemm is an alias of _linalg_gemm.  Performs general matrix multiplication and accumulation. Input are tensors  A ,  B ,  C , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B ) +  beta  *  C  Here,  alpha  and  beta  are scalar parameters, and  op()  is either the identity or matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]  // Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]  Defined in src/operator/tensor/la_op.cc:L69  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  C::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.  beta::double, optional, default=1 : Scalar factor multiplied with C.   source  #  MXNet.mx.linalg_gemm2     Method .  linalg_gemm2(A, B, transpose_a, transpose_b, alpha)  linalg_gemm2 is an alias of _linalg_gemm2.  Performs general matrix multiplication. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B )  Here  alpha  is a scalar parameter and  op()  is either the identity or the matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]  // Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]  Defined in src/operator/tensor/la_op.cc:L128  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.   source  #  MXNet.mx.linalg_potrf     Method .  linalg_potrf(A)  linalg_potrf is an alias of _linalg_potrf.  Performs Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the Cholesky factor  L  of the symmetric, positive definite matrix  A  is computed.  L  is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:  A  =  L  *  L \\ :sup: T  If  n 2 ,  potrf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]  // Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]  Defined in src/operator/tensor/la_op.cc:L178  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be decomposed   source  #  MXNet.mx.linalg_potri     Method .  linalg_potri(A)  linalg_potri is an alias of _linalg_potri.  Performs matrix inversion from a Cholesky factorization. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:  out  =  A \\ :sup: -T  *  A \\ :sup: -1  In other words, if  A  is the Cholesky factor of a symmetric positive definite matrix  B  (obtained by  potrf ), then  out  =  B \\ :sup: -1  If  n 2 ,  potri  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  .. note:: Use this operator only if you are certain you need the inverse of  B , and           cannot use the Cholesky factor  A  ( potrf ), together with backsubstitution           ( trsm ). The latter is numerically much safer, and also cheaper.  Examples::  // Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]  // Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]  Defined in src/operator/tensor/la_op.cc:L236  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices   source  #  MXNet.mx.linalg_sumlogdiag     Method .  linalg_sumlogdiag(A)  linalg_sumlogdiag is an alias of _linalg_sumlogdiag.  Computes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).  If  n 2 ,  sumlogdiag  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]  // Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]  Defined in src/operator/tensor/la_op.cc:L405  Arguments   A::NDArray-or-SymbolicNode : Tensor of square matrices   source  #  MXNet.mx.linalg_syrk     Method .  linalg_syrk(A, transpose, alpha)  linalg_syrk is an alias of _linalg_syrk.  Multiplication of matrix with its transpose. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the operator performs the BLAS3 function  syrk :  out  =  alpha  *  A  *  A \\ :sup: T  if  transpose=False , or  out  =  alpha  *  A \\ :sup: T  \\ *  A  if  transpose=True .  If  n 2 ,  syrk  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]  // Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]  Defined in src/operator/tensor/la_op.cc:L461  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  transpose::boolean, optional, default=0 : Use transpose of input matrix.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.   source  #  MXNet.mx.linalg_trmm     Method .  linalg_trmm(A, B, transpose, rightside, alpha)  linalg_trmm is an alias of _linalg_trmm.  Performs multiplication with a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trmm :  out  =  alpha  *  op \\ ( A ) *  B  if  rightside=False , or  out  =  alpha  *  B  *  op \\ ( A )  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trmm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]  // Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L293  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.   source  #  MXNet.mx.linalg_trsm     Method .  linalg_trsm(A, B, transpose, rightside, alpha)  linalg_trsm is an alias of _linalg_trsm.  Solves matrix equation involving a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trsm , solving for  out  in:  op \\ ( A ) *  out  =  alpha  *  B  if  rightside=False , or  out  *  op \\ ( A ) =  alpha  *  B  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trsm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]  // Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L356  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.   source  #  MXNet.mx.load     Method .  load(filename, ::Type{NDArray})  Load NDArrays from binary file.  Arguments:   filename::String : the path of the file to load. It could be S3 or HDFS address.   Returns either  Dict{Symbol, NDArray}  or  Vector{NDArray} .  filename  can point to  s3  or  hdfs  resources if the  libmxnet  is built with the corresponding components enabled. Examples:   s3://my-bucket/path/my-s3-ndarray  hdfs://my-bucket/path/my-hdfs-ndarray  /path-to/my-local-ndarray   source  #  MXNet.mx.make_loss     Method .  make_loss(data)  Make your own loss function in network construction.  This operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.  For example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::  cross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)  We will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.  The storage type of $make_loss$ output depends upon the input storage type:   make_loss(default) = default  make_loss(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L200  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.mod_from!     Method .  mod_from!(x::NDArray, y::NDArray)\nmod_from!(x::NDArray, y::Real)  Elementwise modulo for  NDArray . Inplace updating.  source  #  MXNet.mx.mp_sgd_mom_update     Method .  mp_sgd_mom_update(weight, grad, mom, weight32, lr, momentum, wd, rescale_grad, clip_gradient)  Updater function for multi-precision sgd optimizer  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mom::NDArray-or-SymbolicNode : Momentum  weight32::NDArray-or-SymbolicNode : Weight32  lr::float, required : Learning rate  momentum::float, optional, default=0 : The decay rate of momentum estimates at each epoch.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx.mp_sgd_update     Method .  mp_sgd_update(weight, grad, weight32, lr, wd, rescale_grad, clip_gradient)  Updater function for multi-precision sgd optimizer  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : gradient  weight32::NDArray-or-SymbolicNode : Weight32  lr::float, required : Learning rate  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx.mul_to!     Method .  mul_to!(dst::NDArray, arg::NDArrayOrReal)  Elementwise multiplication into  dst  of either a scalar or an  NDArray  of the same shape. Inplace updating.  source  #  MXNet.mx.nanprod     Method .  nanprod(data, axis, keepdims, exclude)  Computes the product of array elements over given axes treating Not a Numbers ($NaN$) as one.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L146  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.    source  #  MXNet.mx.nansum     Method .  nansum(data, axis, keepdims, exclude)  Computes the sum of array elements over given axes treating Not a Numbers ($NaN$) as zero.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L131  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.    source  #  MXNet.mx.negative     Method .  negative(data)  Numerical negative of the argument, element-wise.  The storage type of $negative$ output depends upon the input storage type:   negative(default) = default  negative(row_sparse) = row_sparse  negative(csr) = csr   Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.normal     Method .  normal(loc, scale, shape, ctx, dtype)  normal is an alias of _random_normal.  Draw random samples from a normal (Gaussian) distribution.  .. note:: The existing alias $normal$ is deprecated.  Samples are distributed according to a normal distribution parametrized by  loc  (mean) and  scale  (standard deviation).  Example::  normal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]  Defined in src/operator/random/sample_op.cc:L85  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.one_hot     Method .  one_hot(indices, depth, on_value, off_value, dtype)  Returns a one-hot array.  The locations represented by  indices  take value  on_value , while all other locations take value  off_value .  one_hot  operation with  indices  of shape $(i0, i1)$ and  depth   of $d$ would result in an output array of shape $(i0, i1, d)$ with::  output[i,j,:] = off_value   output[i,j,indices[i,j]] = on_value  Examples::  one_hot([1,0,2,0], 3) = [[ 0.  1.  0.]                            [ 1.  0.  0.]                            [ 0.  0.  1.]                            [ 1.  0.  0.]]  one_hot([1,0,2,0], 3, on_value=8, off_value=1,           dtype='int32') = [[1 8 1]                             [8 1 1]                             [1 1 8]                             [8 1 1]]  one_hot([[1,0],[1,0],[2,0]], 3) = [[[ 0.  1.  0.]                                       [ 1.  0.  0.]]                                   [[ 0.  1.  0.]\n                                  [ 1.  0.  0.]]\n\n                                 [[ 0.  0.  1.]\n                                  [ 1.  0.  0.]]]  Defined in src/operator/tensor/indexing_op.cc:L428  Arguments   indices::NDArray-or-SymbolicNode : array of locations where to set on_value  depth::int, required : Depth of the one hot dimension.  on_value::double, optional, default=1 : The value assigned to the locations represented by indices.  off_value::double, optional, default=0 : The value assigned to the locations not represented by indices.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : DType of the output   source  #  MXNet.mx.ones     Method .  ones([DType], dims, [ctx::Context = cpu()])\nones([DType], dims...)\nones(x::NDArray)  Create an  NDArray  with specific shape   type, and initialize with 1.  source  #  MXNet.mx.ones_like     Method .  ones_like(data)  Return an array of ones with the same shape and type as the input array.  Examples::  x = [[ 0.,  0.,  0.],        [ 0.,  0.,  0.]]  ones_like(x) = [[ 1.,  1.,  1.],                   [ 1.,  1.,  1.]]  Arguments   data::NDArray-or-SymbolicNode : The input   source  #  MXNet.mx.pad     Method .  pad(data, mode, pad_width, constant_value)  pad is an alias of Pad.  Pads an input array with a constant or edge values of the array.  .. note::  Pad  is deprecated. Use  pad  instead.  .. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in  pad_width  to be zero.  This operation pads an input array with either a  constant_value  or edge values along each axis of the input array. The amount of padding is specified by  pad_width .  pad_width  is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The  pad_width  should be of length $2*N$ where $N$ is the number of dimensions of the array.  For dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.  Example::  x = [[[[  1.   2.   3.]           [  4.   5.   6.]]       [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]  pad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]  pad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]  Defined in src/operator/pad.cc:L766  Arguments   data::NDArray-or-SymbolicNode : An n-dimensional input array.  mode::{'constant', 'edge', 'reflect'}, required : Padding type to use. \"constant\" pads with  constant_value  \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.  pad_width::Shape(tuple), required : Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.  constant_value::double, optional, default=0 : The value used for padding when  mode  is \"constant\".   source  #  MXNet.mx.pick     Method .  pick(data, index, axis, keepdims)  Picks elements from an input array according to the input indices along the given axis.  Given an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::  output[i] = input[i, indices[i]]  By default, if any index mentioned is too large, it is replaced by the index that addresses the last element along an axis (the  clip  mode).  This function supports n-dimensional input and (n-1)-dimensional indices arrays.  Examples::  x = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]  // picks elements with specified indices along axis 0   pick(x, y=[0,1], 0) = [ 1.,  4.]  // picks elements with specified indices along axis 1   pick(x, y=[0,1,0], 1) = [ 1.,  4.,  5.]  y = [[ 1.],        [ 0.],        [ 2.]]  // picks elements with specified indices along axis 1 and dims are maintained   pick(x,y, 1, keepdims=True) = [[ 2.],                                  [ 3.],                                  [ 6.]]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L145  Arguments   data::NDArray-or-SymbolicNode : The input array  index::NDArray-or-SymbolicNode : The index array  axis::int or None, optional, default='None' : The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$  keepdims::boolean, optional, default=0 : If this is set to  True , the reduced axis is left in the result as dimension with size one.   source  #  MXNet.mx.radians     Method .  radians(data)  Converts each element of the input array from degrees to radians.  .. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]  The storage type of $radians$ output depends upon the input storage type:   radians(default) = default  radians(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L182  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.random_exponential     Method .  random_exponential(lam, shape, ctx, dtype)  random_exponential is an alias of _random_exponential.  Draw random samples from an exponential distribution.  Samples are distributed according to an exponential distribution parametrized by  lambda  (rate).  Example::  exponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]  Defined in src/operator/random/sample_op.cc:L115  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the exponential distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.random_gamma     Method .  random_gamma(alpha, beta, shape, ctx, dtype)  random_gamma is an alias of _random_gamma.  Draw random samples from a gamma distribution.  Samples are distributed according to a gamma distribution parametrized by  alpha  (shape) and  beta  (scale).  Example::  gamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]  Defined in src/operator/random/sample_op.cc:L100  Arguments   alpha::float, optional, default=1 : Alpha parameter (shape) of the gamma distribution.  beta::float, optional, default=1 : Beta parameter (scale) of the gamma distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.random_generalized_negative_binomial     Method .  random_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)  random_generalized_negative_binomial is an alias of _random_generalized_negative_binomial.  Draw random samples from a generalized negative binomial distribution.  Samples are distributed according to a generalized negative binomial distribution parametrized by  mu  (mean) and  alpha  (dispersion).  alpha  is defined as  1/k  where  k  is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.  Example::  generalized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]  Defined in src/operator/random/sample_op.cc:L168  Arguments   mu::float, optional, default=1 : Mean of the negative binomial distribution.  alpha::float, optional, default=1 : Alpha (dispersion) parameter of the negative binomial distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.random_negative_binomial     Method .  random_negative_binomial(k, p, shape, ctx, dtype)  random_negative_binomial is an alias of _random_negative_binomial.  Draw random samples from a negative binomial distribution.  Samples are distributed according to a negative binomial distribution parametrized by  k  (limit of unsuccessful experiments) and  p  (failure probability in each experiment). Samples will always be returned as a floating point data type.  Example::  negative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]  Defined in src/operator/random/sample_op.cc:L149  Arguments   k::int, optional, default='1' : Limit of unsuccessful experiments.  p::float, optional, default=1 : Failure probability in each experiment.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.random_normal     Method .  random_normal(loc, scale, shape, ctx, dtype)  random_normal is an alias of _random_normal.  Draw random samples from a normal (Gaussian) distribution.  .. note:: The existing alias $normal$ is deprecated.  Samples are distributed according to a normal distribution parametrized by  loc  (mean) and  scale  (standard deviation).  Example::  normal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]  Defined in src/operator/random/sample_op.cc:L85  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.random_poisson     Method .  random_poisson(lam, shape, ctx, dtype)  random_poisson is an alias of _random_poisson.  Draw random samples from a Poisson distribution.  Samples are distributed according to a Poisson distribution parametrized by  lambda  (rate). Samples will always be returned as a floating point data type.  Example::  poisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]  Defined in src/operator/random/sample_op.cc:L132  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the Poisson distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.random_uniform     Method .  random_uniform(low, high, shape, ctx, dtype)  random_uniform is an alias of _random_uniform.  Draw random samples from a uniform distribution.  .. note:: The existing alias $uniform$ is deprecated.  Samples are uniformly distributed over the half-open interval  [low, high)  (includes  low , but excludes  high ).  Example::  uniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]  Defined in src/operator/random/sample_op.cc:L66  Arguments   low::float, optional, default=0 : Lower bound of the distribution.  high::float, optional, default=1 : Upper bound of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.rcbrt     Method .  rcbrt(data)  Returns element-wise inverse cube-root value of the input.  .. math::    rcbrt(x) = 1/\\sqrt[3]{x}  Example::  rcbrt([1,8,-125]) = [1.0, 0.5, -0.2]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L619  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.rdiv_from!     Method .  rdiv_from!(x:: Real, y::NDArray)  Elementwise divide a scalar by an  NDArray . Inplace updating.  source  #  MXNet.mx.reciprocal     Method .  reciprocal(data)  Returns the reciprocal of the argument, element-wise.  Calculates 1/x.  Example::  reciprocal([-2, 1, 3, 1.6, 0.2]) = [-0.5, 1.0, 0.33333334, 0.625, 5.0]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L364  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.reshape_like     Method .  reshape_like(lhs, rhs)  Reshape lhs to have the same shape as rhs.  Arguments   lhs::NDArray-or-SymbolicNode : First input.  rhs::NDArray-or-SymbolicNode : Second input.   source  #  MXNet.mx.rint     Method .  rint(data)  Returns element-wise rounded value to the nearest integer of the input.  .. note::   For input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.  For input $-n.5$ both $rint$ and $round$ returns $-n-1$.   Example::  rint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]  The storage type of $rint$ output depends upon the input storage type:   rint(default) = default  rint(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L445  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.rmod_from!     Method .  rmod_from!(y::Real, x::NDArray)  Elementwise modulo for  NDArray . Inplace updating.  source  #  MXNet.mx.rmsprop_update     Method .  rmsprop_update(weight, grad, n, lr, gamma1, epsilon, wd, rescale_grad, clip_gradient, clip_weights)  Update function for  RMSProp  optimizer.  RMSprop  is a variant of stochastic gradient descent where the gradients are divided by a cache which grows with the sum of squares of recent gradients?  RMSProp  is similar to  AdaGrad , a popular variant of  SGD  which adaptively tunes the learning rate of each parameter.  AdaGrad  lowers the learning rate for each parameter monotonically over the course of training. While this is analytically motivated for convex optimizations, it may not be ideal for non-convex problems.  RMSProp  deals with this heuristically by allowing the learning rates to rebound as the denominator decays over time.  Define the Root Mean Square (RMS) error criterion of the gradient as :math: RMS[g]_t = \\sqrt{E[g^2]_t + \\epsilon} , where :math: g  represents gradient and :math: E[g^2]_t  is the decaying average over past squared gradient.  The :math: E[g^2]_t  is given by:  .. math::   E[g^2] t = \\gamma * E[g^2]  + (1-\\gamma) * g_t^2  The update step is  .. math::   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{RMS[g]_t} g_t  The RMSProp code follows the version in http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf Tieleman   Hinton, 2012.  Hinton suggests the momentum term :math: \\gamma  to be 0.9 and the learning rate :math: \\eta  to be 0.001.  Defined in src/operator/optimizer_op.cc:L262  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  n::NDArray-or-SymbolicNode : n  lr::float, required : Learning rate  gamma1::float, optional, default=0.95 : The decay rate of momentum estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  clip_weights::float, optional, default=-1 : Clip weights to the range of [-clip_weights, clip_weights] If clip_weights  = 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).   source  #  MXNet.mx.rmspropalex_update     Method .  rmspropalex_update(weight, grad, n, g, delta, lr, gamma1, gamma2, epsilon, wd, rescale_grad, clip_gradient, clip_weights)  Update function for RMSPropAlex optimizer.  RMSPropAlex  is non-centered version of  RMSProp .  Define :math: E[g^2]_t  is the decaying average over past squared gradient and :math: E[g]_t  is the decaying average over past gradient.  .. math::   E[g^2] t = \\gamma_1 * E[g^2]  + (1 - \\gamma_1) * g_t^2\\\n  E[g] t = \\gamma_1 * E[g]  + (1 - \\gamma_1) * g_t\\\n  \\Delta_t = \\gamma_2 * \\Delta_{t-1} - \\frac{\\eta}{\\sqrt{E[g^2]_t - E[g]_t^2 + \\epsilon}} g_t\\\n The update step is  .. math::   \\theta_{t+1} = \\theta_t + \\Delta_t  The RMSPropAlex code follows the version in http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.  Graves suggests the momentum term :math: \\gamma_1  to be 0.95, :math: \\gamma_2  to be 0.9 and the learning rate :math: \\eta  to be 0.0001.  Defined in src/operator/optimizer_op.cc:L301  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  n::NDArray-or-SymbolicNode : n  g::NDArray-or-SymbolicNode : g  delta::NDArray-or-SymbolicNode : delta  lr::float, required : Learning rate  gamma1::float, optional, default=0.95 : Decay rate.  gamma2::float, optional, default=0.9 : Decay rate.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  clip_weights::float, optional, default=-1 : Clip weights to the range of [-clip_weights, clip_weights] If clip_weights  = 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).   source  #  MXNet.mx.rsqrt     Method .  rsqrt(data)  Returns element-wise inverse square-root value of the input.  .. math::    rsqrt(x) = 1/\\sqrt{x}  Example::  rsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]  The storage type of $rsqrt$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L585  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.sample_exponential     Method .  sample_exponential(lam, shape, dtype)  sample_exponential is an alias of _sample_exponential.  Concurrent sampling from multiple exponential distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]  // Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]  Defined in src/operator/random/multisample_op.cc:L284  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.sample_gamma     Method .  sample_gamma(alpha, shape, dtype, beta)  sample_gamma is an alias of _sample_gamma.  Concurrent sampling from multiple gamma distributions with parameters  alpha  (shape) and  beta  (scale).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  alpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]  // Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]  // Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]  Defined in src/operator/random/multisample_op.cc:L282  Arguments   alpha::NDArray-or-SymbolicNode : Alpha (shape) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  beta::NDArray-or-SymbolicNode : Beta (scale) parameters of the distributions.   source  #  MXNet.mx.sample_generalized_negative_binomial     Method .  sample_generalized_negative_binomial(mu, shape, dtype, alpha)  sample_generalized_negative_binomial is an alias of _sample_generalized_negative_binomial.  Concurrent sampling from multiple generalized negative binomial distributions with parameters  mu  (mean) and  alpha  (dispersion).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  mu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]  // Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]  // Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]  Defined in src/operator/random/multisample_op.cc:L293  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  alpha::NDArray-or-SymbolicNode : Alpha (dispersion) parameters of the distributions.   source  #  MXNet.mx.sample_multinomial     Method .  sample_multinomial(data, shape, get_prob, dtype)  sample_multinomial is an alias of _sample_multinomial.  Concurrent sampling from multiple multinomial distributions.  data  is an  n  dimensional array whose last dimension has length  k , where  k  is the number of possible outcomes of each multinomial distribution. This operator will draw  shape  samples from each distribution. If shape is empty one sample will be drawn from each distribution.  If  get_prob  is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.  Note that the input distribution must be normalized, i.e.  data  must sum to 1 along its last axis.  Examples::  probs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]  // Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]  // Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]  // requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]  Arguments   data::NDArray-or-SymbolicNode : Distribution probabilities. Must sum to one on the last axis.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  get_prob::boolean, optional, default=0 : Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.  dtype::{'int32'},optional, default='int32' : DType of the output in case this can't be inferred. Only support int32 for now.   source  #  MXNet.mx.sample_negative_binomial     Method .  sample_negative_binomial(k, shape, dtype, p)  sample_negative_binomial is an alias of _sample_negative_binomial.  Concurrent sampling from multiple negative binomial distributions with parameters  k  (failure limit) and  p  (failure probability).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  k = [ 20, 49 ]    p = [ 0.4 , 0.77 ]  // Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]  // Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]  Defined in src/operator/random/multisample_op.cc:L289  Arguments   k::NDArray-or-SymbolicNode : Limits of unsuccessful experiments.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  p::NDArray-or-SymbolicNode : Failure probabilities in each experiment.   source  #  MXNet.mx.sample_normal     Method .  sample_normal(mu, shape, dtype, sigma)  sample_normal is an alias of _sample_normal.  Concurrent sampling from multiple normal distributions with parameters  mu  (mean) and  sigma  (standard deviation).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  mu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]  // Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]  Defined in src/operator/random/multisample_op.cc:L279  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  sigma::NDArray-or-SymbolicNode : Standard deviations of the distributions.   source  #  MXNet.mx.sample_poisson     Method .  sample_poisson(lam, shape, dtype)  sample_poisson is an alias of _sample_poisson.  Concurrent sampling from multiple Poisson distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Samples will always be returned as a floating point data type.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]  // Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]  Defined in src/operator/random/multisample_op.cc:L286  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.sample_uniform     Method .  sample_uniform(low, shape, dtype, high)  sample_uniform is an alias of _sample_uniform.  Concurrent sampling from multiple uniform distributions on the intervals given by  [low,high) .  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  low = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]  // Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]  Defined in src/operator/random/multisample_op.cc:L277  Arguments   low::NDArray-or-SymbolicNode : Lower bounds of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  high::NDArray-or-SymbolicNode : Upper bounds of the distributions.   source  #  MXNet.mx.save     Method .  save(filename::AbstractString, data)  Save NDarrays to binary file. Filename could be S3 or HDFS address, if  libmxnet  is built with corresponding support (see  load ).   filename::String : path to the binary file to write to.  data : data to save to file. Data can be a NDArray , a  Vector  of  NDArray , or a  Dict{Symbol}  contains  NDArray s.   source  #  MXNet.mx.scatter_nd     Method .  scatter_nd(data, indices, shape)  Scatters data into a new tensor according to indices.  gather_nd  and  scatter_nd  are inverse functions to each other.  Given  data  with shape  (Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1})  and indices with shape  (M, Y_0, ..., Y_{K-1}) , the output will have shape  (X_0, X_1, ..., X_{N-1}) , where  M  = N . If  M == N , data shape should simply be  (Y_0, ..., Y_{K-1}) .  The elements in output is defined as follows::  output[indices[0, y_0, ..., y_{K-1}],          ...,          indices[M-1, y_0, ..., y_{K-1}],          x_M, ..., x_{N-1}] = data[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}]  all other entries in output are 0.  Examples::  data = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   shape = (2, 2)   scatter_nd(data, indices, shape) = [[0, 0], [2, 3]]  Arguments   data::NDArray-or-SymbolicNode : data  indices::NDArray-or-SymbolicNode : indices  shape::Shape(tuple), required : Shape of output.   source  #  MXNet.mx.sgd_mom_update     Method .  sgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)  Momentum update function for Stochastic Gradient Descent (SDG) optimizer.  Momentum update has better convergence rates on neural networks. Mathematically it looks like below:  .. math::  v_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t  It updates the weights using::  v = momentum * v - learning_rate * gradient   weight += v  Where the parameter $momentum$ is the decay rate of momentum estimates at each epoch.  If weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::  for row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]  Defined in src/operator/optimizer_op.cc:L94  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mom::NDArray-or-SymbolicNode : Momentum  lr::float, required : Learning rate  momentum::float, optional, default=0 : The decay rate of momentum estimates at each epoch.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx.sgd_update     Method .  sgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)  Update function for Stochastic Gradient Descent (SDG) optimizer.  It updates the weights using::  weight = weight - learning_rate * gradient  If weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::  for row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]  Defined in src/operator/optimizer_op.cc:L54  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  lr::float, required : Learning rate  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).   source  #  MXNet.mx.slice     Method .  slice(arr :: NDArray, start:stop)  Create a view into a sub-slice of an  NDArray . Note only slicing at the slowest changing dimension is supported. In Julia's column-major perspective, this is the last dimension. For example, given an  NDArray  of shape (2,3,4),  slice(array, 2:3)  will create a  NDArray  of shape (2,3,2), sharing the data with the original array. This operation is used in data parallelization to split mini-batch into sub-batches for different devices.  source  #  MXNet.mx.slice     Method .  slice(data, begin, end, step)  Slices a region of the array.  .. note:: $crop$ is deprecated. Use $slice$ instead.  This function returns a sliced array between the indices given by  begin  and  end  with the corresponding  step .  For an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m  = n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.  The resulting array's  k -th dimension contains elements from the  k -th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).  If the  k -th elements are  None  in the sequence of  begin ,  end , and  step , the following rule will be used to set default values. If  s_k  is  None , set  s_k=1 . If  s_k   0 , set  b_k=0 ,  e_k=d_k ; else, set  b_k=d_k-1 ,  e_k=-1 .  The storage type of $slice$ output depends on storage types of inputs   slice(csr) = csr  otherwise, $slice$ generates output with default storage   .. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.  Example::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]  Defined in src/operator/tensor/matrix_op.cc:L355  Arguments   data::NDArray-or-SymbolicNode : Source input  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.   source  #  MXNet.mx.slice_axis     Method .  slice_axis(data, axis, begin, end)  Slices along a given axis.  Returns an array slice along a given  axis  starting from the  begin  index to the  end  index.  Examples::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice_axis(x, axis=0, begin=1, end=3) = [[  5.,   6.,   7.,   8.],                                            [  9.,  10.,  11.,  12.]]  slice_axis(x, axis=1, begin=0, end=2) = [[  1.,   2.],                                            [  5.,   6.],                                            [  9.,  10.]]  slice_axis(x, axis=1, begin=-3, end=-1) = [[  2.,   3.],                                              [  6.,   7.],                                              [ 10.,  11.]]  Defined in src/operator/tensor/matrix_op.cc:L442  Arguments   data::NDArray-or-SymbolicNode : Source input  axis::int, required : Axis along which to be sliced, supports negative indexes.  begin::int, required : The beginning index along the axis to be sliced,  supports negative indexes.  end::int or None, required : The ending index along the axis to be sliced,  supports negative indexes.   source  #  MXNet.mx.smooth_l1     Method .  smooth_l1(data, scalar)  Calculate Smooth L1 Loss(lhs, scalar) by summing  .. math::  f(x) =\n\\begin{cases}\n(\\sigma x)^2/2,  \\text{if }x   1/\\sigma^2\\\\\n|x|-0.5/\\sigma^2,  \\text{otherwise}\n\\end{cases}  where :math: x  is an element of the tensor  lhs  and :math: \\sigma  is the scalar.  Example::  smooth_l1([1, 2, 3, 4], sigma=1) = [0.5, 1.5, 2.5, 3.5]  Defined in src/operator/tensor/elemwise_binary_scalar_op_extended.cc:L103  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input   source  #  MXNet.mx.softmax_cross_entropy     Method .  softmax_cross_entropy(data, label)  Calculate cross entropy of softmax output and one-hot label.    This operator computes the cross entropy in two steps:   Applies softmax function on the input array.  Computes and returns the cross entropy loss between the softmax output and the labels.   The softmax function and cross entropy loss is given by:    Softmax Function:    .. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}   Cross Entropy Function:   .. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)    Example::  x = [[1, 2, 3],        [11, 7, 5]]  label = [2, 0]  softmax(x) = [[0.09003057, 0.24472848, 0.66524094],                 [0.97962922, 0.01794253, 0.00242826]]  softmax_cross_entropy(data, label) = - log(0.66524084) - log(0.97962922) = 0.4281871  Defined in src/operator/loss_binary_op.cc:L59  Arguments   data::NDArray-or-SymbolicNode : Input data  label::NDArray-or-SymbolicNode : Input label   source  #  MXNet.mx.square     Method .  square(data)  Returns element-wise squared value of the input.  .. math::    square(x) = x^2  Example::  square([2, 3, 4]) = [4, 9, 16]  The storage type of $square$ output depends upon the input storage type:   square(default) = default  square(row_sparse) = row_sparse  square(csr) = csr   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L542  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.stack     Method .  stack(data, axis, num_args)  Note : stack takes variable number of positional inputs. So instead of calling as stack([x, y, z], num_args=3), one should call via stack(x, y, z), and num_args will be determined automatically.  Join a sequence of arrays along a new axis.  The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.  Examples::  x = [1, 2]   y = [3, 4]  stack(x, y) = [[1, 2],                  [3, 4]]   stack(x, y, axis=1) = [[1, 3],                          [2, 4]]  Arguments   data::NDArray-or-SymbolicNode[] : List of arrays to stack  axis::int, optional, default='0' : The axis in the result array along which the input arrays are stacked.  num_args::int, required : Number of inputs to be stacked.   source  #  MXNet.mx.stop_gradient     Method .  stop_gradient(data)  stop_gradient is an alias of BlockGrad.  Stops gradient computation.  Stops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.  Example::  v1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)  executor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]  executor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L167  Arguments   data::NDArray-or-SymbolicNode : The input array.   source  #  MXNet.mx.sub_from!     Method .  sub_from!(dst::NDArray, args::NDArrayOrReal...)  Subtract a bunch of arguments from  dst . Inplace updating.  source  #  MXNet.mx.sum_axis     Method .  sum_axis(data, axis, keepdims, exclude)  sum_axis is an alias of sum.  Computes the sum of array elements over given axes.  .. Note::  sum  and  sum_axis  are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.  Example::  data = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]  sum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]  sum(data, axis=[1,2])   [ 12.  19.  27.]  data = [[1,2,0],           [3,0,1],           [4,1,0]]  csr = cast_storage(data, 'csr')  sum(csr, axis=0)   [ 8.  3.  1.]  sum(csr, axis=1)   [ 3.  4.  5.]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L85  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0`: Whether to perform reduction on axis that are NOT in axis instead.    source  #  MXNet.mx.swapaxes     Method .  swapaxes(data, dim1, dim2)  swapaxes is an alias of SwapAxis.  Interchanges two axes of an array.  Examples::  x = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]  x = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array  swapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]  Defined in src/operator/swapaxis.cc:L70  Arguments   data::NDArray-or-SymbolicNode : Input array.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.   source  #  MXNet.mx.take     Method .  take(a, indices, axis, mode)  Takes elements from an input array along the given axis.  This function slices the input array along a particular axis with the provided indices.  Given an input array with shape $(d0, d1, d2)$ and indices with shape $(i0, i1)$, the output will have shape $(i0, i1, d1, d2)$, computed by::  output[i,j,:,:] = input[indices[i,j],:,:]  .. note::   axis - Only slicing along axis 0 is supported for now.  mode - Only  clip  mode is supported for now.   Examples::  x = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]  // takes elements with specified indices along axis 0   take(x, [[0,1],[1,2]]) = [[[ 1.,  2.],                              [ 3.,  4.]],                          [[ 3.,  4.],\n                         [ 5.,  6.]]]  Defined in src/operator/tensor/indexing_op.cc:L327  Arguments   a::NDArray-or-SymbolicNode : The input array.  indices::NDArray-or-SymbolicNode : The indices of the values to be extracted.  axis::int, optional, default='0' : The axis of input array to be taken.  mode::{'clip', 'raise', 'wrap'},optional, default='clip' : Specify how out-of-bound indices bahave. \"clip\" means clip to the range. So, if all indices mentioned are too large, they are replaced by the index that addresses the last element along an axis.  \"wrap\" means to wrap around.  \"raise\" means to raise an error.   source  #  MXNet.mx.tile     Method .  tile(data, reps)  Repeats the whole array multiple times.  If $reps$ has length  d , and input array has dimension of  n . There are three cases:    n=d . Repeat  i -th dimension of the input by $reps[i]$ times::  x = [[1, 2],        [3, 4]]  tile(x, reps=(2,3)) = [[ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.],                          [ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.]]\n  *  n d . $reps$ is promoted to length  n  by pre-pending 1's to it. Thus for an input shape $(2,3)$, $repos=(2,)$ is treated as $(1,2)$::    tile(x, reps=(2,)) = [[ 1.,  2.,  1.,  2.],\n                      [ 3.,  4.,  3.,  4.]]    n d . The input is promoted to be d-dimensional by prepending new axes. So a shape $(2,2)$ array is promoted to $(1,2,2)$ for 3-D replication::  tile(x, reps=(2,2,3)) = [[[ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.],                             [ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.]],  [[ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.],\n                        [ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.]]]    Defined in src/operator/tensor/matrix_op.cc:L621  Arguments   data::NDArray-or-SymbolicNode : Input data array  reps::Shape(tuple), required : The number of times for repeating the tensor a. If reps has length d, the result will have dimension of max(d, a.ndim); If a.ndim   d, a is promoted to be d-dimensional by prepending new axes. If a.ndim   d, reps is promoted to a.ndim by pre-pending 1's to it.   source  #  MXNet.mx.topk     Method .  topk(data, axis, k, ret_typ, is_ascend)  Returns the top  k  elements in an input array along the given axis.  Examples::  x = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]  // returns an index of the largest element on last axis   topk(x) = [[ 2.],              [ 1.]]  // returns the value of top-2 largest elements on last axis   topk(x, ret_typ='value', k=2) = [[ 0.4,  0.3],                                    [ 0.3,  0.2]]  // returns the value of top-2 smallest elements on last axis   topk(x, ret_typ='value', k=2, is_ascend=1) = [[ 0.2 ,  0.3],                                                [ 0.1 ,  0.2]]  // returns the value of top-2 largest elements on axis 0   topk(x, axis=0, ret_typ='value', k=2) = [[ 0.3,  0.3,  0.4],                                            [ 0.1,  0.2,  0.2]]  // flattens and then returns list of both values and indices   topk(x, ret_typ='both', k=2) = [[[ 0.4,  0.3], [ 0.3,  0.2]] ,  [[ 2.,  0.], [ 1.,  2.]]]  Defined in src/operator/tensor/ordering_op.cc:L63  Arguments   data::NDArray-or-SymbolicNode : The input array  axis::int or None, optional, default='-1' : Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.  k::int, optional, default='1' : Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k   1.  ret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices' : The return type.   \"value\" means to return the top k values, \"indices\" means to return the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return a list of both values and indices of top k elements.   is_ascend::boolean, optional, default=0 : Whether to choose k largest or k smallest elements. Top K largest elements will be chosen if set to false.   source  #  MXNet.mx.try_get_shared     Method .  try_get_shared(arr; sync=:nop)  Try to create a Julia array by sharing the data with the underlying  NDArray .  Arguments:   arr::NDArray : the array to be shared.    Note  The returned array does not guarantee to share data with the underlying  NDArray . In particular, data sharing is possible only when the  NDArray  lives on CPU.    sync::Symbol :  :nop , :write ,  :read  On CPU, invoke  _wait_to_read  if  :read ; invoke  _wait_to_write  if  :write .   source  #  MXNet.mx.uniform     Method .  uniform(low, high, shape, ctx, dtype)  uniform is an alias of _random_uniform.  Draw random samples from a uniform distribution.  .. note:: The existing alias $uniform$ is deprecated.  Samples are uniformly distributed over the half-open interval  [low, high)  (includes  low , but excludes  high ).  Example::  uniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]  Defined in src/operator/random/sample_op.cc:L66  Arguments   low::float, optional, default=0 : Lower bound of the distribution.  high::float, optional, default=1 : Upper bound of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).   source  #  MXNet.mx.where     Method .  where(condition, x, y)  Given three ndarrays, condition, x, and y, return an ndarray with the elements from x or y, depending on the elements from condition are true or false. x and y must have the same shape. If condition has the same shape as x, each element in the output array is from x if the corresponding element in the condition is true, and from y if false. If condition does not have the same shape as x, it must be a 1D array whose size is the same as x's first dimension size. Each row of the output array is from x's row if the corresponding element from condition is true, and from y's row if false.  From:src/operator/tensor/control_flow_op.cc:40  Arguments   condition::NDArray-or-SymbolicNode : condition array  x::NDArray-or-SymbolicNode :  y::NDArray-or-SymbolicNode :   source  #  MXNet.mx.zeros     Method .  zeros([DType], dims, [ctx::Context = cpu()])\nzeros([DType], dims...)\nzeros(x::NDArray)  Create zero-ed  NDArray  with specific shape and type.  source  #  MXNet.mx.zeros_like     Method .  zeros_like(data)  Return an array of zeros with the same shape and type as the input array.  The storage type of $zeros_like$ output depends on the storage type of the input   zeros_like(row_sparse) = row_sparse  zeros_like(csr) = csr  zeros_like(default) = default   Examples::  x = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]  zeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]  Arguments   data::NDArray-or-SymbolicNode : The input   source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  TakingBroadcastSeriously.broadcast_     Method .  source  #  MXNet.mx.@nd_as_jl     Macro .  Manipulating as Julia Arrays  @nd_as_jl(captures..., statement)  A convenient macro that allows to operate  NDArray  as Julia Arrays. For example,    x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end  Under the hood, the macro convert all the declared captures from  NDArray  into Julia Arrays, by using  try_get_shared . And automatically commit the modifications back into the  NDArray  that is declared as  rw . This is useful for fast prototyping and when implement non-critical computations, such as  AbstractEvalMetric .   Note    Multiple  rw  and / or  ro  capture declaration could be made.  The macro does  not  check to make sure that  ro  captures are not modified. If the original  NDArray  lives in CPU memory, then it is very likely the corresponding Julia Array shares data with the  NDArray , so modifying the Julia Array will also modify the underlying  NDArray .  More importantly, since the  NDArray  is asynchronized, we will wait for  writing  for  rw  variables but wait only for  reading  in  ro  variables. If we write into those  ro  variables,  and  if the memory is shared, racing condition might happen, and the behavior is undefined.  When an  NDArray  is declared to be captured as  rw , its contents is always sync back in the end.  The execution results of the expanded macro is always  nothing .  The statements are wrapped in a  let , thus locally introduced new variables will not be available after the statements. So you will need to declare the variables before calling the macro if needed.   source", 
            "title": "notice that the first row is all 0 because label[0] is 1, which is equal to ignore_label."
        }, 
        {
            "location": "/api/symbolic-node/", 
            "text": "Symbolic API\n\n\n#\n\n\nMXNet.mx.SymbolicNode\n \n \nType\n.\n\n\nSymbolicNode\n\n\n\n\nSymbolicNode is the basic building block of the symbolic graph in MXNet.jl.\n\n\n(self :: SymbolicNode)(args :: SymbolicNode...)\n(self :: SymbolicNode)(; kwargs...)\n\n\n\n\nMake a new node by composing \nself\n with \nargs\n. Or the arguments can be specified using keyword arguments.\n\n\nsource\n\n\n#\n\n\nBase.cos\n \n \nMethod\n.\n\n\ncos(data)\n\n\n\n\nComputes the element-wise cosine of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    cos([0, \\pi/4, \\pi/2]) = [1, 0.707, 0]\n\n\nThe storage type of $cos$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L63\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.cosh\n \n \nMethod\n.\n\n\ncosh(data)\n\n\n\n\nReturns the hyperbolic cosine  of the input array, computed element-wise.\n\n\n.. math::    cosh(x) = 0.5\\times(exp(x) + exp(-x))\n\n\nThe storage type of $cosh$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L216\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.reshape\n \n \nMethod\n.\n\n\nreshape(sym::SymbolicNode, dim; reverse=false, name)\nreshape(sym::SymbolicNode, dim...; reverse=false, name)\n\n\n\n\nReshape SymbolicNode operator\n\n\nSome dimensions of the shape can take special values from the set {0, -1, -2, -3, -4}. The significance of each is explained below:\n\n\n\n\n\n\n0\n  copy this dimension from the input to the output shape.\n\n\nExample:\n\n\n\n\ninput shape = (2,3,4), shape = (4,0,2), output shape = (4,3,2)\n\n\ninput shape = (2,3,4), shape = (2,0,0), output shape = (2,3,4)\n\n\n-1\n infers the dimension of the output shape by using the remainder of the input dimensions keeping the size of the new array same as that of the input array. At most one dimension of shape can be -1.\n\n\n\n\nExample:\n\n\n\n\ninput shape = (2,3,4), shape = (6,1,-1), output shape = (6,1,4)\n\n\ninput shape = (2,3,4), shape = (3,-1,8), output shape = (3,1,8)\n\n\ninput shape = (2,3,4), shape=(-1,), output shape = (24,)\n\n\n-2\n copy all/remainder of the input dimensions to the output shape.\n\n\n\n\nExample:\n\n\n\n\ninput shape = (2,3,4), shape = (-2,), output shape = (2,3,4)\n\n\ninput shape = (2,3,4), shape = (2,-2), output shape = (2,3,4)\n\n\ninput shape = (2,3,4), shape = (-2,1,1), output shape = (2,3,4,1,1)\n\n\n-3\n use the product of two consecutive dimensions of the input shape as the output dimension.\n\n\n\n\nExample:\n\n\n\n\ninput shape = (2,3,4), shape = (-3,4), output shape = (6,4)\n\n\ninput shape = (2,3,4,5), shape = (-3,-3), output shape = (6,20)\n\n\ninput shape = (2,3,4), shape = (0,-3), output shape = (2,12)\n\n\ninput shape = (2,3,4), shape = (-3,-2), output shape = (6,4)\n\n\n-4\n split one dimension of the input into two dimensions passed subsequent to -4 in shape (can contain -1).\n\n\n\n\nExample:\n\n\n\n\ninput shape = (2,3,4), shape = (-4,1,2,-2), output shape = (1,2,3,4)\n\n\ninput shape = (2,3,4), shape = (2,-4,-1,3,-2), output shape = (2,1,3,4)\n\n\n\n\n\n\n\n\nIf the argument \nreverse\n is set to \n1\n, then the special values are inferred from right to left.\n\n\nExample:\n\n\n\n\nwith \nreverse=false\n, for input shape = (10,5,4), shape = (-1,0), output shape would be (40,5)\n\n\nwith \nreverse=true\n, output shape will be (50,4).\n\n\n\n\nsource\n\n\n#\n\n\nBase.sin\n \n \nMethod\n.\n\n\nsin(data)\n\n\n\n\nComputes the element-wise sine of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    sin([0, \\pi/4, \\pi/2]) = [0, 0.707, 1]\n\n\nThe storage type of $sin$ output depends upon the input storage type:\n\n\n\n\nsin(default) = default\n\n\nsin(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L46\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sinh\n \n \nMethod\n.\n\n\nsinh(data)\n\n\n\n\nReturns the hyperbolic sine of the input array, computed element-wise.\n\n\n.. math::    sinh(x) = 0.5\\times(exp(x) - exp(-x))\n\n\nThe storage type of $sinh$ output depends upon the input storage type:\n\n\n\n\nsinh(default) = default\n\n\nsinh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L201\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.tan\n \n \nMethod\n.\n\n\ntan(data)\n\n\n\n\nComputes the element-wise tangent of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    tan([0, \\pi/4, \\pi/2]) = [0, 1, -inf]\n\n\nThe storage type of $tan$ output depends upon the input storage type:\n\n\n\n\ntan(default) = default\n\n\ntan(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L83\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.tanh\n \n \nMethod\n.\n\n\ntanh(data)\n\n\n\n\nReturns the hyperbolic tangent of the input array, computed element-wise.\n\n\n.. math::    tanh(x) = sinh(x) / cosh(x)\n\n\nThe storage type of $tanh$ output depends upon the input storage type:\n\n\n\n\ntanh(default) = default\n\n\ntanh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L234\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Variable\n \n \nMethod\n.\n\n\nVariable(name :: Union{Symbol, AbstractString})\n\n\n\n\nCreate a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.\n\n\nArguments\n\n\n\n\nDict{Symbol, AbstractString} attrs: The attributes associated with this \nVariable\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arccos\n \n \nMethod\n.\n\n\narccos(data)\n\n\n\n\nReturns element-wise inverse cosine of the input array.\n\n\nThe input should be in range \n[-1, 1]\n. The output is in the closed interval :math:\n[0, \\pi]\n\n\n.. math::    arccos([-1, -.707, 0, .707, 1]) = [\\pi, 3\\pi/4, \\pi/2, \\pi/4, 0]\n\n\nThe storage type of $arccos$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L123\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arccosh\n \n \nMethod\n.\n\n\narccosh(data)\n\n\n\n\nReturns the element-wise inverse hyperbolic cosine of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arccosh$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L264\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arcsin\n \n \nMethod\n.\n\n\narcsin(data)\n\n\n\n\nReturns element-wise inverse sine of the input array.\n\n\nThe input should be in the range \n[-1, 1]\n. The output is in the closed interval of [:math:\n-\\pi/2\n, :math:\n\\pi/2\n].\n\n\n.. math::    arcsin([-1, -.707, 0, .707, 1]) = [-\\pi/2, -\\pi/4, 0, \\pi/4, \\pi/2]\n\n\nThe storage type of $arcsin$ output depends upon the input storage type:\n\n\n\n\narcsin(default) = default\n\n\narcsin(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arcsinh\n \n \nMethod\n.\n\n\narcsinh(data)\n\n\n\n\nReturns the element-wise inverse hyperbolic sine of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arcsinh$ output depends upon the input storage type:\n\n\n\n\narcsinh(default) = default\n\n\narcsinh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L250\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arctan\n \n \nMethod\n.\n\n\narctan(data)\n\n\n\n\nReturns element-wise inverse tangent of the input array.\n\n\nThe output is in the closed interval :math:\n[-\\pi/2, \\pi/2]\n\n\n.. math::    arctan([-1, 0, 1]) = [-\\pi/4, 0, \\pi/4]\n\n\nThe storage type of $arctan$ output depends upon the input storage type:\n\n\n\n\narctan(default) = default\n\n\narctan(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L144\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.arctanh\n \n \nMethod\n.\n\n\narctanh(data)\n\n\n\n\nReturns the element-wise inverse hyperbolic tangent of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arctanh$ output depends upon the input storage type:\n\n\n\n\narctanh(default) = default\n\n\narctanh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L281\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.clip\n \n \nMethod\n.\n\n\nclip(data, a_min, a_max)\n\n\n\n\nClips (limits) the values in an array.\n\n\nGiven an interval, values outside the interval are clipped to the interval edges. Clipping $x$ between \na_min\n and \na_x\n would be::\n\n\nclip(x, a_min, a_max) = max(min(x, a_max), a_min))\n\n\nExample::\n\n\nx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nclip(x,1,8) = [ 1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  8.]\n\n\n\n\nThe storage type of $clip$ output depends on storage types of inputs and the a_min, a_max \nparameter values:\n\n\n\n\nclip(default) = default\n\n\nclip(row_sparse, a_min \n= 0, a_max \n= 0) = row_sparse\n\n\nclip(csr, a_min \n= 0, a_max \n= 0) = csr\n\n\nclip(row_sparse, a_min \n 0, a_max \n 0) = default\n\n\nclip(row_sparse, a_min \n 0, a_max \n 0) = default\n\n\nclip(csr, a_min \n 0, a_max \n 0) = csr\n\n\nclip(csr, a_min \n 0, a_max \n 0) = csr\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L486\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\na_min::float, required\n: Minimum value\n\n\na_max::float, required\n: Maximum value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.expand_dims\n \n \nMethod\n.\n\n\nexpand_dims(data, axis)\n\n\n\n\nInserts a new axis of size 1 into the array shape\n\n\nFor example, given $x$ with shape $(2,3,4)$, then $expand_dims(x, axis=1)$ will return a new array with shape $(2,1,3,4)$.\n\n\nDefined in src/operator/tensor/matrix_op.cc:L289\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\naxis::int, required\n: Position where new axis is to be inserted. Suppose that the input \nNDArray\n's dimension is \nndim\n, the range of the inserted axis is \n[-ndim, ndim]\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.log_softmax\n \n \nMethod\n.\n\n\nlog_softmax(data, axis)\n\n\n\n\nComputes the log softmax of the input. This is equivalent to computing softmax followed by log.\n\n\nExamples::\n\n\n\n\n\n\n\n\nx = mx.nd.array([1, 2, .1]) mx.nd.log_softmax(x).asnumpy()\n\n\n\n\n\n\n\n\narray([-1.41702998, -0.41702995, -2.31702995], dtype=float32)\n\n\n\n\n\n\n\n\nx = mx.nd.array( [[1, 2, .1],[.1, 2, 1]] ) mx.nd.log_softmax(x, axis=0).asnumpy()\n\n\n\n\n\n\n\n\narray([[-0.34115392, -0.69314718, -1.24115396],          [-1.24115396, -0.69314718, -0.34115392]], dtype=float32)\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\naxis::int, optional, default='-1'\n: The axis along which to compute softmax.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.relu\n \n \nMethod\n.\n\n\nrelu(data)\n\n\n\n\nComputes rectified linear.\n\n\n.. math::    max(features, 0)\n\n\nThe storage type of $relu$ output depends upon the input storage type:\n\n\n\n\nrelu(default) = default\n\n\nrelu(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L84\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sigmoid\n \n \nMethod\n.\n\n\nsigmoid(data)\n\n\n\n\nComputes sigmoid of x element-wise.\n\n\n.. math::    y = 1 / (1 + exp(-x))\n\n\nThe storage type of $sigmoid$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L103\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax\n \n \nMethod\n.\n\n\nsoftmax(data, axis)\n\n\n\n\nApplies the softmax function.\n\n\nThe resulting array contains elements in the range (0,1) and the elements along the given axis sum up to 1.\n\n\n.. math::    softmax(\\mathbf{z})\nj = \\frac{e^{z_j}}{\\sum\n^K e^{z_k}}\n\n\nfor :math:\nj = 1, ..., K\n\n\nExample::\n\n\nx = [[ 1.  1.  1.]        [ 1.  1.  1.]]\n\n\nsoftmax(x,axis=0) = [[ 0.5  0.5  0.5]                        [ 0.5  0.5  0.5]]\n\n\nsoftmax(x,axis=1) = [[ 0.33333334,  0.33333334,  0.33333334],                        [ 0.33333334,  0.33333334,  0.33333334]]\n\n\nDefined in src/operator/nn/softmax.cc:L54\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\naxis::int, optional, default='-1'\n: The axis along which to compute softmax.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@var\n \n \nMacro\n.\n\n\n@var \nsymbols\n...\n\n\n\n\nA handy macro for creating \nmx.Variable\n.\n\n\njulia\n x = @mx.var x\nMXNet.mx.SymbolicNode x\n\njulia\n x, y, z = @mx.var x y z\n(MXNet.mx.SymbolicNode x, MXNet.mx.SymbolicNode y, MXNet.mx.SymbolicNode z)\n\n\n\n\nsource\n\n\n#\n\n\nBase.Iterators.Flatten\n \n \nMethod\n.\n\n\nFlatten(data)\n\n\n\n\nFlattens the input array into a 2-D array by collapsing the higher dimensions.\n\n\n.. note:: \nFlatten\n is deprecated. Use \nflatten\n instead.\n\n\nFor an input array with shape $(d1, d2, ..., dk)$, \nflatten\n operation reshapes the input array into an output array of shape $(d1, d2\n...\ndk)$.\n\n\nExample::\n\n\nx = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L208\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.:*\n \n \nMethod\n.\n\n\n.*(x, y)\n\n\n\n\nElementwise multiplication of \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nBase.:+\n \n \nMethod\n.\n\n\n+(args...)\n.+(args...)\n\n\n\n\nElementwise summation of \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nBase.:-\n \n \nMethod\n.\n\n\n-(x, y)\n.-(x, y)\n\n\n\n\nElementwise substraction of \nSymbolicNode\n. Operating with \nReal\n is available.\n\n\nsource\n\n\n#\n\n\nBase.:/\n \n \nMethod\n.\n\n\n./(x, y)\n\n\n\n\n\n\nElementwise dividing a \nSymbolicNode\n by a scalar or another \nSymbolicNode\n\n\n\n\nof the same shape.\n\n\n\n\nElementwise divide a scalar by an \nSymbolicNode\n.\n\n\nMatrix division (solving linear systems) is not implemented yet.\n\n\n\n\nsource\n\n\n#\n\n\nBase.:^\n \n \nFunction\n.\n\n\n.^(x, y)\n\n\n\n\nElementwise power of \nSymbolicNode\n and \nNDArray\n. Operating with \nReal\n is available.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.dot\n \n \nMethod\n.\n\n\ndot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\nDot product of two arrays.\n\n\n$dot$'s behavior depends on the input array dimensions:\n\n\n\n\n1-D arrays: inner product of vectors\n\n\n2-D arrays: matrix multiplication\n\n\n\n\nN-D arrays: a sum product over the last axis of the first input and the first axis of the second input\n\n\nFor example, given 3-D $x$ with shape \n(n,m,k)\n and $y$ with shape \n(k,r,s)\n, the result array will have shape \n(n,m,r,s)\n. It is computed by::\n\n\ndot(x,y)[i,j,a,b] = sum(x[i,j,:]*y[:,a,b])\n\n\nExample::\n\n\nx = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))   y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))   dot(x,y)[0,0,1,1] = 0   sum(x[0,0,:]*y[:,1,1]) = 0\n\n\n\n\n\n\nThe storage type of $dot$ output depends on storage types of inputs and transpose options:\n\n\n\n\ndot(csr, default) = default\n\n\ndot(csr.T, default) = row_sparse\n\n\ndot(csr, row_sparse) = default\n\n\ndot(default, csr) = csr\n\n\notherwise, $dot$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/dot.cc:L62\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: The first input\n\n\nrhs::NDArray-or-SymbolicNode\n: The second input\n\n\ntranspose_a::boolean, optional, default=0\n: If true then transpose the first input before dot.\n\n\ntranspose_b::boolean, optional, default=0\n: If true then transpose the second input before dot.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.norm\n \n \nMethod\n.\n\n\nnorm(data)\n\n\n\n\nFlattens the input array and then computes the l2 norm.\n\n\nExamples::\n\n\nx = [[1, 2],        [3, 4]]\n\n\nnorm(x) = [5.47722578]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L257\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.Math.cbrt\n \n \nMethod\n.\n\n\ncbrt(data)\n\n\n\n\nReturns element-wise cube-root value of the input.\n\n\n.. math::    cbrt(x) = \\sqrt[3]{x}\n\n\nExample::\n\n\ncbrt([1, 8, -125]) = [1, 2, -5]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L602\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.Math.gamma\n \n \nMethod\n.\n\n\ngamma(data)\n\n\n\n\nReturns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.\n\n\nThe storage type of $gamma$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase._div\n \n \nMethod\n.\n\n\n_div(lhs, rhs)\n\n\n\n\n_div is an alias of elemwise_div.\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase._sub\n \n \nMethod\n.\n\n\n_sub(lhs, rhs)\n\n\n\n\n_sub is an alias of elemwise_sub.\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.abs\n \n \nMethod\n.\n\n\nabs(data)\n\n\n\n\nReturns element-wise absolute value of the input.\n\n\nExample::\n\n\nabs([-2, 0, 3]) = [2, 0, 3]\n\n\nThe storage type of $abs$ output depends upon the input storage type:\n\n\n\n\nabs(default) = default\n\n\nabs(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L386\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.ceil\n \n \nMethod\n.\n\n\nceil(data)\n\n\n\n\nReturns element-wise ceiling of the input.\n\n\nThe ceil of the scalar x is the smallest integer i, such that i \n= x.\n\n\nExample::\n\n\nceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]\n\n\nThe storage type of $ceil$ output depends upon the input storage type:\n\n\n\n\nceil(default) = default\n\n\nceil(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L464\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\ncopy(self :: SymbolicNode)\n\n\n\n\nMake a copy of a SymbolicNode. The same as making a deep copy.\n\n\nsource\n\n\n#\n\n\nBase.deepcopy\n \n \nMethod\n.\n\n\ndeepcopy(self :: SymbolicNode)\n\n\n\n\nMake a deep copy of a SymbolicNode.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nMethod\n.\n\n\nexp(data)\n\n\n\n\nReturns element-wise exponential value of the input.\n\n\n.. math::    exp(x) = e^x \\approx 2.718^x\n\n\nExample::\n\n\nexp([0, 1, 2]) = [1., 2.71828175, 7.38905621]\n\n\nThe storage type of $exp$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L642\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.expm1\n \n \nMethod\n.\n\n\nexpm1(data)\n\n\n\n\nReturns $exp(x) - 1$ computed element-wise on the input.\n\n\nThis function provides greater precision than $exp(x) - 1$ for small values of $x$.\n\n\nThe storage type of $expm1$ output depends upon the input storage type:\n\n\n\n\nexpm1(default) = default\n\n\nexpm1(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L721\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.floor\n \n \nMethod\n.\n\n\nfloor(data)\n\n\n\n\nReturns element-wise floor of the input.\n\n\nThe floor of the scalar x is the largest integer i, such that i \n= x.\n\n\nExample::\n\n\nfloor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]\n\n\nThe storage type of $floor$ output depends upon the input storage type:\n\n\n\n\nfloor(default) = default\n\n\nfloor(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L483\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})\n\n\n\n\nGet a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of \nlist_outputs\n.\n\n\nsource\n\n\n#\n\n\nBase.identity\n \n \nMethod\n.\n\n\nidentity(data)\n\n\n\n\nidentity is an alias of _copy.\n\n\nReturns a copy of the input.\n\n\nFrom:src/operator/tensor/elemwise_unary_op_basic.cc:112\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nMethod\n.\n\n\nlog(data)\n\n\n\n\nReturns element-wise Natural logarithmic value of the input.\n\n\nThe natural logarithm is logarithm in base \ne\n, so that $log(exp(x)) = x$\n\n\nThe storage type of $log$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L654\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log10\n \n \nMethod\n.\n\n\nlog10(data)\n\n\n\n\nReturns element-wise Base-10 logarithmic value of the input.\n\n\n$10**log10(x) = x$\n\n\nThe storage type of $log10$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L666\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log1p\n \n \nMethod\n.\n\n\nlog1p(data)\n\n\n\n\nReturns element-wise $log(1 + x)$ value of the input.\n\n\nThis function is more accurate than $log(1 + x)$  for small $x$ so that :math:\n1+x\\approx 1\n\n\nThe storage type of $log1p$ output depends upon the input storage type:\n\n\n\n\nlog1p(default) = default\n\n\nlog1p(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L703\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.log2\n \n \nMethod\n.\n\n\nlog2(data)\n\n\n\n\nReturns element-wise Base-2 logarithmic value of the input.\n\n\n$2**log2(x) = x$\n\n\nThe storage type of $log2$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L678\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.mean\n \n \nMethod\n.\n\n\nmean(data, axis, keepdims, exclude)\n\n\n\n\nComputes the mean of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L101\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.print\n \n \nFunction\n.\n\n\nprint([io::IO], sym::SymbolicNode)\n\n\n\n\nPrint the content of symbol, used for debug.\n\n\njulia\n layer = @mx.chain mx.Variable(:data)           =\n\n         mx.FullyConnected(name=:fc1, num_hidden=128) =\n\n         mx.Activation(name=:relu1, act_type=:relu)\nMXNet.mx.SymbolicNode(MXNet.mx.MX_SymbolHandle(Ptr{Void} @0x000055b29b9c3520))\n\njulia\n print(layer)\nSymbol Outputs:\n        output[0]=relu1(0)\nVariable:data\nVariable:fc1_weight\nVariable:fc1_bias\n--------------------\nOp:FullyConnected, Name=fc1\nInputs:\n        arg[0]=data(0) version=0\n        arg[1]=fc1_weight(0) version=0\n        arg[2]=fc1_bias(0) version=0\nAttrs:\n        num_hidden=128\n--------------------\nOp:Activation, Name=relu1\nInputs:\n        arg[0]=fc1(0)\nAttrs:\n        act_type=relu\n\n\n\n\nsource\n\n\n#\n\n\nBase.prod\n \n \nMethod\n.\n\n\nprod(data, axis, keepdims, exclude)\n\n\n\n\nComputes the product of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L116\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.repeat\n \n \nMethod\n.\n\n\nrepeat(data, repeats, axis)\n\n\n\n\nRepeats elements of an array.\n\n\nBy default, $repeat$ flattens the input array into 1-D and then repeats the elements::\n\n\nx = [[ 1, 2],        [ 3, 4]]\n\n\nrepeat(x, repeats=2) = [ 1.,  1.,  2.,  2.,  3.,  3.,  4.,  4.]\n\n\nThe parameter $axis$ specifies the axis along which to perform repeat::\n\n\nrepeat(x, repeats=2, axis=1) = [[ 1.,  1.,  2.,  2.],                                   [ 3.,  3.,  4.,  4.]]\n\n\nrepeat(x, repeats=2, axis=0) = [[ 1.,  2.],                                   [ 1.,  2.],                                   [ 3.,  4.],                                   [ 3.,  4.]]\n\n\nrepeat(x, repeats=2, axis=-1) = [[ 1.,  1.,  2.,  2.],                                    [ 3.,  3.,  4.,  4.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L560\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\nrepeats::int, required\n: The number of repetitions for each element.\n\n\naxis::int or None, optional, default='None'\n: The axis along which to repeat values. The negative numbers are interpreted counting from the backward. By default, use the flattened input array, and return a flat output array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.reverse\n \n \nMethod\n.\n\n\nreverse(data, axis)\n\n\n\n\nReverses the order of elements along given axis while preserving array shape.\n\n\nNote: reverse and flip are equivalent. We use reverse in the following examples.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]\n\n\nreverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]\n\n\nreverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L662\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\naxis::Shape(tuple), required\n: The axis which to reverse elements.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.round\n \n \nMethod\n.\n\n\nround(data)\n\n\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\nExample::\n\n\nround([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]\n\n\nThe storage type of $round$ output depends upon the input storage type:\n\n\n\n\nround(default) = default\n\n\nround(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L424\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sign\n \n \nMethod\n.\n\n\nsign(data)\n\n\n\n\nReturns element-wise sign of the input.\n\n\nExample::\n\n\nsign([-2, 0, 3]) = [-1, 0, 1]\n\n\nThe storage type of $sign$ output depends upon the input storage type:\n\n\n\n\nsign(default) = default\n\n\nsign(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L405\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sort\n \n \nMethod\n.\n\n\nsort(data, axis, is_ascend)\n\n\n\n\nReturns a sorted copy of an input array along the given axis.\n\n\nExamples::\n\n\nx = [[ 1, 4],        [ 3, 1]]\n\n\n// sorts along the last axis   sort(x) = [[ 1.,  4.],              [ 1.,  3.]]\n\n\n// flattens and then sorts   sort(x) = [ 1.,  1.,  3.,  4.]\n\n\n// sorts along the first axis   sort(x, axis=0) = [[ 1.,  1.],                      [ 3.,  4.]]\n\n\n// in a descend order   sort(x, is_ascend=0) = [[ 4.,  1.],                           [ 3.,  1.]]\n\n\nDefined in src/operator/tensor/ordering_op.cc:L126\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=1\n: Whether to sort in ascending or descending order.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.split\n \n \nMethod\n.\n\n\nsplit(data, num_outputs, axis, squeeze_axis)\n\n\n\n\nsplit is an alias of SliceChannel.\n\n\nSplits an array along a particular axis into multiple sub-arrays.\n\n\n.. note:: $SliceChannel$ is deprecated. Use $split$ instead.\n\n\nNote\n that \nnum_outputs\n should evenly divide the length of the axis along which to split the array.\n\n\nExample::\n\n\nx  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)\n\n\ny = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]\n\n\n   [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]\n\n\n\n\ny[0].shape = (3, 1, 1)\n\n\nz = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]\n\n\n   [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]\n\n\n\n\nz[0].shape = (1, 2, 1)\n\n\nsqueeze_axis=1\n removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $1$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to true only if $input.shape[axis] == num_outputs$.\n\n\nExample::\n\n\nz = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]\n\n\n   [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]\n\n\n\n\nz[0].shape = (2 ,1 )\n\n\nDefined in src/operator/slice_channel.cc:L107\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nnum_outputs::int, required\n: Number of splits. Note that this should evenly divide the length of the \naxis\n.\n\n\naxis::int, optional, default='1'\n: Axis along which to split.\n\n\nsqueeze_axis::boolean, optional, default=0\n: If true, Removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $true$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to $true$ only if $input.shape[axis] == num_outputs$.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sqrt\n \n \nMethod\n.\n\n\nsqrt(data)\n\n\n\n\nReturns element-wise square-root value of the input.\n\n\n.. math::    \\textrm{sqrt}(x) = \\sqrt{x}\n\n\nExample::\n\n\nsqrt([4, 9, 16]) = [2, 3, 4]\n\n\nThe storage type of $sqrt$ output depends upon the input storage type:\n\n\n\n\nsqrt(default) = default\n\n\nsqrt(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L565\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\nsum(data, axis, keepdims, exclude)\n\n\n\n\nComputes the sum of array elements over given axes.\n\n\n.. Note::\n\n\nsum\n and \nsum_axis\n are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.\n\n\nExample::\n\n\ndata = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]\n\n\nsum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]\n\n\nsum(data, axis=[1,2])   [ 12.  19.  27.]\n\n\ndata = [[1,2,0],           [3,0,1],           [4,1,0]]\n\n\ncsr = cast_storage(data, 'csr')\n\n\nsum(csr, axis=0)   [ 8.  3.  1.]\n\n\nsum(csr, axis=1)   [ 3.  4.  5.]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L85\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\ntranspose(data, axes)\n\n\n\n\nPermutes the dimensions of an array.\n\n\nExamples::\n\n\nx = [[ 1, 2],        [ 3, 4]]\n\n\ntranspose(x) = [[ 1.,  3.],                   [ 2.,  4.]]\n\n\nx = [[[ 1.,  2.],         [ 3.,  4.]],\n\n\n   [[ 5.,  6.],\n    [ 7.,  8.]]]\n\n\n\n\ntranspose(x) = [[[ 1.,  5.],                    [ 3.,  7.]],\n\n\n              [[ 2.,  6.],\n               [ 4.,  8.]]]\n\n\n\n\ntranspose(x, axes=(1,0,2)) = [[[ 1.,  2.],                                  [ 5.,  6.]],\n\n\n                            [[ 3.,  4.],\n                             [ 7.,  8.]]]\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L253\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\naxes::Shape(tuple), optional, default=[]\n: Target axis order. By default the axes will be inverted.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.trunc\n \n \nMethod\n.\n\n\ntrunc(data)\n\n\n\n\nReturn the element-wise truncated value of the input.\n\n\nThe truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.\n\n\nExample::\n\n\ntrunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]\n\n\nThe storage type of $trunc$ output depends upon the input storage type:\n\n\n\n\ntrunc(default) = default\n\n\ntrunc(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L503\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Activation\n \n \nMethod\n.\n\n\nActivation(data, act_type)\n\n\n\n\nApplies an activation function element-wise to the input.\n\n\nThe following activation functions are supported:\n\n\n\n\nrelu\n: Rectified Linear Unit, :math:\ny = max(x, 0)\n\n\nsigmoid\n: :math:\ny = \\frac{1}{1 + exp(-x)}\n\n\ntanh\n: Hyperbolic tangent, :math:\ny = \\frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}\n\n\nsoftrelu\n: Soft ReLU, or SoftPlus, :math:\ny = log(1 + exp(x))\n\n\n\n\nDefined in src/operator/nn/activation.cc:L92\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to activation function.\n\n\nact_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required\n: Activation function to be applied.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm\n \n \nMethod\n.\n\n\nBatchNorm(data, gamma, beta, moving_mean, moving_var, eps, momentum, fix_gamma, use_global_stats, output_mean_var, axis, cudnn_off)\n\n\n\n\nBatch normalization.\n\n\nNormalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.\n\n\nAssume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:\n\n\n.. math::\n\n\ndata_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])\n\n\nThen compute the normalized output, which has the same shape as input, as following:\n\n\n.. math::\n\n\nout[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]\n\n\nBoth \nmean\n and \nvar\n returns a scalar by treating the input as a vector.\n\n\nAssume the input has size \nk\n on axis 1, then both $gamma$ and $beta$ have shape \n(k,)\n. If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.\n\n\nBesides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are \nk\n-length vectors. They are global statistics for the whole dataset, which are updated by::\n\n\nmoving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)\n\n\nIf $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.\n\n\nThe parameter $axis$ specifies which axis of the input shape denotes the 'channel' (separately normalized groups).  The default is 1.  Specifying -1 sets the channel axis to be the last item in the input shape.\n\n\nBoth $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.\n\n\nDefined in src/operator/nn/batch_norm.cc:L400\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to batch normalization\n\n\ngamma::NDArray-or-SymbolicNode\n: gamma array\n\n\nbeta::NDArray-or-SymbolicNode\n: beta array\n\n\nmoving_mean::NDArray-or-SymbolicNode\n: running mean of input\n\n\nmoving_var::NDArray-or-SymbolicNode\n: running variance of input\n\n\neps::double, optional, default=0.001\n: Epsilon to prevent div 0. Must be no less than CUDNN_BN_MIN_EPSILON defined in cudnn.h when using cudnn (usually 1e-5)\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=1\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=0\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\noutput_mean_var::boolean, optional, default=0\n: Output All,normal mean and var\n\n\naxis::int, optional, default='1'\n: Specify which shape axis the channel is specified\n\n\ncudnn_off::boolean, optional, default=0\n: Do not select CUDNN operator, if available\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm_v1\n \n \nMethod\n.\n\n\nBatchNorm_v1(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)\n\n\n\n\nBatch normalization.\n\n\nNormalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.\n\n\nAssume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:\n\n\n.. math::\n\n\ndata_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])\n\n\nThen compute the normalized output, which has the same shape as input, as following:\n\n\n.. math::\n\n\nout[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]\n\n\nBoth \nmean\n and \nvar\n returns a scalar by treating the input as a vector.\n\n\nAssume the input has size \nk\n on axis 1, then both $gamma$ and $beta$ have shape \n(k,)\n. If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.\n\n\nBesides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are \nk\n-length vectors. They are global statistics for the whole dataset, which are updated by::\n\n\nmoving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)\n\n\nIf $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.\n\n\nBoth $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.\n\n\nDefined in src/operator/batch_norm_v1.cc:L90\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to batch normalization\n\n\ngamma::NDArray-or-SymbolicNode\n: gamma array\n\n\nbeta::NDArray-or-SymbolicNode\n: beta array\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent div 0\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=1\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=0\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\noutput_mean_var::boolean, optional, default=0\n: Output All,normal mean and var\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BilinearSampler\n \n \nMethod\n.\n\n\nBilinearSampler(data, grid)\n\n\n\n\nApplies bilinear sampling to input feature map.\n\n\nBilinear Sampling is the key of  [NIPS2015] \\\"Spatial Transformer Networks\\\". The usage of the operator is very similar to remap function in OpenCV, except that the operator has the backward pass.\n\n\nGiven :math:\ndata\n and :math:\ngrid\n, then the output is computed by\n\n\n.. math::   x_{src} = grid[batch, 0, y_{dst}, x_{dst}] \\\n  y_{src} = grid[batch, 1, y_{dst}, x_{dst}] \\\n  output[batch, channel, y_{dst}, x_{dst}] = G(data[batch, channel, y_{src}, x_{src})\n\n\n:math:\nx_{dst}\n, :math:\ny_{dst}\n enumerate all spatial locations in :math:\noutput\n, and :math:\nG()\n denotes the bilinear interpolation kernel. The out-boundary points will be padded with zeros.The shape of the output will be (data.shape[0], data.shape[1], grid.shape[2], grid.shape[3]).\n\n\nThe operator assumes that :math:\ndata\n has 'NCHW' layout and :math:\ngrid\n has been normalized to [-1, 1].\n\n\nBilinearSampler often cooperates with GridGenerator which generates sampling grids for BilinearSampler. GridGenerator supports two kinds of transformation: $affine$ and $warp$. If users want to design a CustomOp to manipulate :math:\ngrid\n, please firstly refer to the code of GridGenerator.\n\n\nExample 1::\n\n\nZoom out data two times\n\n\ndata = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])\n\n\naffine_matrix = array([[2, 0, 0],                          [0, 2, 0]])\n\n\naffine_matrix = reshape(affine_matrix, shape=(1, 6))\n\n\ngrid = GridGenerator(data=affine_matrix, transform_type='affine', target_shape=(4, 4))\n\n\nout = BilinearSampler(data, grid)\n\n\nout   [[[[ 0,   0,     0,   0],      [ 0,   3.5,   6.5, 0],      [ 0,   1.25,  2.5, 0],      [ 0,   0,     0,   0]]]\n\n\nExample 2::\n\n\nshift data horizontally by -1 pixel\n\n\ndata = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])\n\n\nwarp_maxtrix = array([[[[1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1]],                          [[0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0]]]])\n\n\ngrid = GridGenerator(data=warp_matrix, transform_type='warp')   out = BilinearSampler(data, grid)\n\n\nout   [[[[ 4,  3,  6,  0],      [ 8,  8,  9,  0],      [ 4,  1,  5,  0],      [ 0,  1,  3,  0]]]\n\n\nDefined in src/operator/bilinear_sampler.cc:L245\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the BilinearsamplerOp.\n\n\ngrid::NDArray-or-SymbolicNode\n: Input grid to the BilinearsamplerOp.grid has two channels: x_src, y_src\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BlockGrad\n \n \nMethod\n.\n\n\nBlockGrad(data)\n\n\n\n\nStops gradient computation.\n\n\nStops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.\n\n\nExample::\n\n\nv1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)\n\n\nexecutor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]\n\n\nexecutor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L167\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Cast\n \n \nMethod\n.\n\n\nCast(data, dtype)\n\n\n\n\nCasts all elements of the input to a new type.\n\n\n.. note:: $Cast$ is deprecated. Use $cast$ instead.\n\n\nExample::\n\n\ncast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L311\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Output data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Concat\n \n \nMethod\n.\n\n\nConcat(data, num_args, dim)\n\n\n\n\nNote\n: Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.\n\n\nJoins input arrays along a given axis.\n\n\n.. note:: \nConcat\n is deprecated. Use \nconcat\n instead.\n\n\nThe dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.\n\n\nExample::\n\n\nx = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]\n\n\nconcat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]\n\n\nNote that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.\n\n\nconcat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]\n\n\nDefined in src/operator/concat.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: List of arrays to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution\n \n \nMethod\n.\n\n\nConvolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nCompute \nN\n-D convolution on \n(N+2)\n-D input.\n\n\nIn the 2-D convolution, given input data with shape \n(batch_size, channel, height, width)\n, the output is computed by\n\n\n.. math::\n\n\nout[n,i,:,:] = bias[i] + \\sum_{j=0}^{channel} data[n,j,:,:] \\star    weight[i,j,:,:]\n\n\nwhere :math:\n\\star\n is the 2-D cross-correlation operator.\n\n\nFor general 2-D convolution, the shapes are\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0], kernel[1])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n.\n\n\n\n\nDefine::\n\n\nf(x,k,p,s,d) = floor((x+2\np-d\n(k-1)-1)/s)+1\n\n\nthen we have::\n\n\nout_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])\n\n\nIf $no_bias$ is set to be true, then the $bias$ term is ignored.\n\n\nThe default data $layout$ is \nNCHW\n, namely \n(batch_size, channel, height, width)\n. We can choose other layouts such as \nNHWC\n.\n\n\nIf $num_group$ is larger than 1, denoted by \ng\n, then split the input $data$ evenly into \ng\n parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the \ni\n-th part of the data with the \ni\n-th weight part. The output is obtained by concatenating all the \ng\n results.\n\n\n1-D convolution does not have \nheight\n dimension but only \nwidth\n in space.\n\n\n\n\ndata\n: \n(batch_size, channel, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_width)\n.\n\n\n\n\n3-D convolution adds an additional \ndepth\n dimension besides \nheight\n and \nwidth\n. The shapes are\n\n\n\n\ndata\n: \n(batch_size, channel, depth, height, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0], kernel[1], kernel[2])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_depth, out_height, out_width)\n.\n\n\n\n\nBoth $weight$ and $bias$ are learnable parameters.\n\n\nThere are other options to tune the performance.\n\n\n\n\n\n\ncudnn_tune\n: enable this option leads to higher startup time but may give faster speed. Options are\n\n\n\n\noff\n: no tuning\n\n\nlimited_workspace\n:run test and pick the fastest algorithm that doesn't exceed workspace limit.\n\n\nfastest\n: pick the fastest algorithm and ignore workspace limit.\n\n\nNone\n (default): the behavior is determined by environment variable $MXNET_CUDNN_AUTOTUNE_DEFAULT$. 0 for off, 1 for limited workspace (default), 2 for fastest.\n\n\nworkspace\n: A large number leads to more (GPU) memory usage but may improve the performance.\n\n\n\n\n\n\n\n\nDefined in src/operator/nn/convolution.cc:L170\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: Convolution kernel size: (w,), (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=[]\n: Convolution stride: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\ndilate::Shape(tuple), optional, default=[]\n: Convolution dilate: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: Zero pad for convolution: (w,), (h, w) or (d, h, w). Defaults to no padding.\n\n\nnum_filter::int (non-negative), required\n: Convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions.\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum temporary workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algo by running performance test.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution_v1\n \n \nMethod\n.\n\n\nConvolution_v1(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nThis operator is DEPRECATED. Apply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ConvolutionV1Op.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: convolution kernel size: (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=[]\n: convolution stride: (h, w) or (d, h, w)\n\n\ndilate::Shape(tuple), optional, default=[]\n: convolution dilate: (h, w) or (d, h, w)\n\n\npad::Shape(tuple), optional, default=[]\n: pad for convolution: (h, w) or (d, h, w)\n\n\nnum_filter::int (non-negative), required\n: convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum tmp workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Correlation\n \n \nMethod\n.\n\n\nCorrelation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)\n\n\n\n\nApplies correlation to inputs.\n\n\nThe correlation layer performs multiplicative patch comparisons between two feature maps.\n\n\nGiven two multi-channel feature maps :math:\nf_{1}, f_{2}\n, with :math:\nw\n, :math:\nh\n, and :math:\nc\n being their width, height, and number of channels, the correlation layer lets the network compare each patch from :math:\nf_{1}\n with each patch from :math:\nf_{2}\n.\n\n\nFor now we consider only a single comparison of two patches. The 'correlation' of two patches centered at :math:\nx_{1}\n in the first map and :math:\nx_{2}\n in the second map is then defined as:\n\n\n.. math::    c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k] \\times [-k,k]} \n\n\nfor a square patch of size :math:\nK:=2k+1\n.\n\n\nNote that the equation above is identical to one step of a convolution in neural networks, but instead of convolving data with a filter, it convolves data with other data. For this reason, it has no training weights.\n\n\nComputing :math:\nc(x_{1}, x_{2})\n involves :math:\nc * K^{2}\n multiplications. Comparing all patch combinations involves :math:\nw^{2}*h^{2}\n such computations.\n\n\nGiven a maximum displacement :math:\nd\n, for each location :math:\nx_{1}\n it computes correlations :math:\nc(x_{1}, x_{2})\n only in a neighborhood of size :math:\nD:=2d+1\n, by limiting the range of :math:\nx_{2}\n. We use strides :math:\ns_{1}, s_{2}\n, to quantize :math:\nx_{1}\n globally and to quantize :math:\nx_{2}\n within the neighborhood centered around :math:\nx_{1}\n.\n\n\nThe final output is defined by the following expression:\n\n\n.. math::   out[n, q, i, j] = c(x_{i, j}, x_{q})\n\n\nwhere :math:\ni\n and :math:\nj\n enumerate spatial locations in :math:\nf_{1}\n, and :math:\nq\n denotes the :math:\nq^{th}\n neighborhood of :math:\nx_{i,j}\n.\n\n\nDefined in src/operator/correlation.cc:L192\n\n\nArguments\n\n\n\n\ndata1::NDArray-or-SymbolicNode\n: Input data1 to the correlation.\n\n\ndata2::NDArray-or-SymbolicNode\n: Input data2 to the correlation.\n\n\nkernel_size::int (non-negative), optional, default=1\n: kernel size for Correlation must be an odd number\n\n\nmax_displacement::int (non-negative), optional, default=1\n: Max displacement of Correlation\n\n\nstride1::int (non-negative), optional, default=1\n: stride1 quantize data1 globally\n\n\nstride2::int (non-negative), optional, default=1\n: stride2 quantize data2 within the neighborhood centered around data1\n\n\npad_size::int (non-negative), optional, default=0\n: pad for Correlation\n\n\nis_multiply::boolean, optional, default=1\n: operation type is either multiplication or subduction\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Crop\n \n \nMethod\n.\n\n\nCrop(data, num_args, offset, h_w, center_crop)\n\n\n\n\nNote\n: Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.\n\n\n.. note:: \nCrop\n is deprecated. Use \nslice\n instead.\n\n\nCrop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used\n\n\nDefined in src/operator/crop.cc:L50\n\n\nArguments\n\n\n\n\ndata::SymbolicNode or SymbolicNode[]\n: Tensor or List of Tensors, the second input will be used as crop_like shape reference\n\n\nnum_args::int, required\n: Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here\n\n\noffset::Shape(tuple), optional, default=[0,0]\n: crop offset coordinate: (y, x)\n\n\nh_w::Shape(tuple), optional, default=[0,0]\n: crop height and width: (h, w)\n\n\ncenter_crop::boolean, optional, default=0\n: If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Custom\n \n \nMethod\n.\n\n\nCustom(data, op_type)\n\n\n\n\nApply a custom operator implemented in a frontend language (like Python).\n\n\nCustom operators should override required methods like \nforward\n and \nbackward\n. The custom operator must be registered before it can be used. Please check the tutorial here: http://mxnet.io/how_to/new_op.html.\n\n\nDefined in src/operator/custom/custom.cc:L378\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Input data for the custom operator.\n\n\nop_type::string\n: Name of the custom operator. This is the name that is passed to \nmx.operator.register\n to register the operator.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Deconvolution\n \n \nMethod\n.\n\n\nDeconvolution(data, weight, bias, kernel, stride, dilate, pad, adj, target_shape, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)\n\n\n\n\nComputes 1D or 2D transposed convolution (aka fractionally strided convolution) of the input tensor. This operation can be seen as the gradient of Convolution operation with respect to its input. Convolution usually reduces the size of the input. Transposed convolution works the other way, going from a smaller input to a larger output while preserving the connectivity pattern.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input tensor to the deconvolution operation.\n\n\nweight::NDArray-or-SymbolicNode\n: Weights representing the kernel.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias added to the result after the deconvolution operation.\n\n\nkernel::Shape(tuple), required\n: Deconvolution kernel size: (w,), (h, w) or (d, h, w). This is same as the kernel size used for the corresponding convolution\n\n\nstride::Shape(tuple), optional, default=[]\n: The stride used for the corresponding convolution: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\ndilate::Shape(tuple), optional, default=[]\n: Dilation factor for each dimension of the input: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: The amount of implicit zero padding added during convolution for each dimension of the input: (w,), (h, w) or (d, h, w). $(kernel-1)/2$ is usually a good choice. If \ntarget_shape\n is set, \npad\n will be ignored and a padding that will generate the target shape will be used. Defaults to no padding.\n\n\nadj::Shape(tuple), optional, default=[]\n: Adjustment for output shape: (w,), (h, w) or (d, h, w). If \ntarget_shape\n is set, \nadj\n will be ignored and computed accordingly.\n\n\ntarget_shape::Shape(tuple), optional, default=[]\n: Shape of the output tensor: (w,), (h, w) or (d, h, w).\n\n\nnum_filter::int (non-negative), required\n: Number of output filters.\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of groups partition.\n\n\nworkspace::long (non-negative), optional, default=512\n: Maximum temporal workspace allowed for deconvolution (MB).\n\n\nno_bias::boolean, optional, default=1\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None'\n: Whether to pick convolution algorithm by running performance test.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn for this layer.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None'\n: Set layout for input, output and weight. Empty for default layout, NCW for 1d, NCHW for 2d and NCDHW for 3d.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Dropout\n \n \nMethod\n.\n\n\nDropout(data, p, mode)\n\n\n\n\nApplies dropout operation to input array.\n\n\n\n\nDuring training, each element of the input is set to zero with probability p. The whole array is rescaled by :math:\n1/(1-p)\n to keep the expected sum of the input unchanged.\n\n\nDuring testing, this operator does not change the input if mode is 'training'. If mode is 'always', the same computaion as during training will be applied.\n\n\n\n\nExample::\n\n\nrandom.seed(998)   input_array = array([[3., 0.5,  -0.5,  2., 7.],                       [2., -0.4,   7.,  3., 0.2]])   a = symbol.Variable('a')   dropout = symbol.Dropout(a, p = 0.2)   executor = dropout.simple_bind(a = input_array.shape)\n\n\nIf training\n\n\nexecutor.forward(is_train = True, a = input_array)   executor.outputs   [[ 3.75   0.625 -0.     2.5    8.75 ]    [ 2.5   -0.5    8.75   3.75   0.   ]]\n\n\nIf testing\n\n\nexecutor.forward(is_train = False, a = input_array)   executor.outputs   [[ 3.     0.5   -0.5    2.     7.   ]    [ 2.    -0.4    7.     3.     0.2  ]]\n\n\nDefined in src/operator/nn/dropout.cc:L78\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to which dropout will be applied.\n\n\np::float, optional, default=0.5\n: Fraction of the input that gets dropped out during training time.\n\n\nmode::{'always', 'training'},optional, default='training'\n: Whether to only turn on dropout during training or to also turn on for inference.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ElementWiseSum\n \n \nMethod\n.\n\n\nElementWiseSum(args)\n\n\n\n\nElementWiseSum is an alias of add_n.\n\n\nNote\n: ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Embedding\n \n \nMethod\n.\n\n\nEmbedding(data, weight, input_dim, output_dim, dtype)\n\n\n\n\nMaps integer indices to vector representations (embeddings).\n\n\nThis operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.\n\n\nFor an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).\n\n\nIf the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).\n\n\nBy default, if any index mentioned is too large, it is replaced by the index that addresses the last vector in an embedding matrix.\n\n\nExamples::\n\n\ninput_dim = 4   output_dim = 5\n\n\n// Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]\n\n\n// Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]\n\n\n// Mapped input x to its vector representation y.   Embedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                             [ 15.,  16.,  17.,  18.,  19.]],\n\n\n                       [[  0.,   1.,   2.,   3.,   4.],\n                        [ 10.,  11.,  12.,  13.,  14.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L185\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array to the embedding operator.\n\n\nweight::NDArray-or-SymbolicNode\n: The embedding weight matrix.\n\n\ninput_dim::int, required\n: Vocabulary size of the input indices.\n\n\noutput_dim::int, required\n: Dimension of the embedding vectors.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Data type of weight.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FullyConnected\n \n \nMethod\n.\n\n\nFullyConnected(data, weight, bias, num_hidden, no_bias, flatten)\n\n\n\n\nApplies a linear transformation: :math:\nY = XW^T + b\n.\n\n\nIf $flatten$ is set to be true, then the shapes are:\n\n\n\n\ndata\n: \n(batch_size, x1, x2, ..., xn)\n\n\nweight\n: \n(num_hidden, x1 * x2 * ... * xn)\n\n\nbias\n: \n(num_hidden,)\n\n\nout\n: \n(batch_size, num_hidden)\n\n\n\n\nIf $flatten$ is set to be false, then the shapes are:\n\n\n\n\ndata\n: \n(x1, x2, ..., xn, input_dim)\n\n\nweight\n: \n(num_hidden, input_dim)\n\n\nbias\n: \n(num_hidden,)\n\n\nout\n: \n(x1, x2, ..., xn, num_hidden)\n\n\n\n\nThe learnable parameters include both $weight$ and $bias$.\n\n\nIf $no_bias$ is set to be true, then the $bias$ term is ignored.\n\n\nDefined in src/operator/nn/fully_connected.cc:L98\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nnum_hidden::int, required\n: Number of hidden nodes of the output.\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\nflatten::boolean, optional, default=1\n: Whether to collapse all but the first axis of the input data tensor.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.GridGenerator\n \n \nMethod\n.\n\n\nGridGenerator(data, transform_type, target_shape)\n\n\n\n\nGenerates 2D sampling grid for bilinear sampling.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\ntransform_type::{'affine', 'warp'}, required\n: The type of transformation. For \naffine\n, input data should be an affine matrix of size (batch, 6). For \nwarp\n, input data should be an optical flow of size (batch, 2, h, w).\n\n\ntarget_shape::Shape(tuple), optional, default=[0,0]\n: Specifies the output shape (H, W). This is required if transformation type is \naffine\n. If transformation type is \nwarp\n, this parameter is ignored.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Group\n \n \nMethod\n.\n\n\nGroup(nodes :: SymbolicNode...)\n\n\n\n\nCreate a \nSymbolicNode\n by grouping nodes together.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\nIdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)\n\n\n\n\nApply a sparse regularization to the output a sigmoid activation function.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nsparseness_target::float, optional, default=0.1\n: The sparseness target\n\n\npenalty::float, optional, default=0.001\n: The tradeoff parameter for the sparseness penalty\n\n\nmomentum::float, optional, default=0.9\n: The momentum for running average\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.InstanceNorm\n \n \nMethod\n.\n\n\nInstanceNorm(data, gamma, beta, eps)\n\n\n\n\nApplies instance normalization to the n-dimensional input array.\n\n\nThis operator takes an n-dimensional input array where (n\n2) and normalizes the input using the following formula:\n\n\n.. math::\n\n\nout = \\frac{x - mean[data]}{ \\sqrt{Var[data]} + \\epsilon} * gamma + beta\n\n\nThis layer is similar to batch normalization layer (\nBatchNorm\n) with two differences: first, the normalization is carried out per example (instance), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as \ncontrast normalization\n.\n\n\nIf the input data is of shape [batch, channel, spacial_dim1, spacial_dim2, ...], \ngamma\n and \nbeta\n parameters must be vectors of shape [channel].\n\n\nThis implementation is based on paper:\n\n\n.. [1] Instance Normalization: The Missing Ingredient for Fast Stylization,    D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2).\n\n\nExamples::\n\n\n// Input of shape (2,1,2)   x = [[[ 1.1,  2.2]],        [[ 3.3,  4.4]]]\n\n\n// gamma parameter of length 1   gamma = [1.5]\n\n\n// beta parameter of length 1   beta = [0.5]\n\n\n// Instance normalization is calculated with the above formula   InstanceNorm(x,gamma,beta) = [[[-0.997527  ,  1.99752665]],                                 [[-0.99752653,  1.99752724]]]\n\n\nDefined in src/operator/instance_norm.cc:L95\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: An n-dimensional input array (n \n 2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].\n\n\ngamma::NDArray-or-SymbolicNode\n: A vector of length 'channel', which multiplies the normalized input.\n\n\nbeta::NDArray-or-SymbolicNode\n: A vector of length 'channel', which is added to the product of the normalized input and the weight.\n\n\neps::float, optional, default=0.001\n: An \nepsilon\n parameter to prevent division by 0.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.L2Normalization\n \n \nMethod\n.\n\n\nL2Normalization(data, eps, mode)\n\n\n\n\nNormalize the input array using the L2 norm.\n\n\nFor 1-D NDArray, it computes::\n\n\nout = data / sqrt(sum(data ** 2) + eps)\n\n\nFor N-D NDArray, if the input array has shape (N, N, ..., N),\n\n\nwith $mode$ = $instance$, it normalizes each instance in the multidimensional array by its L2 norm.::\n\n\nfor i in 0...N     out[i,:,:,...,:] = data[i,:,:,...,:] / sqrt(sum(data[i,:,:,...,:] ** 2) + eps)\n\n\nwith $mode$ = $channel$, it normalizes each channel in the array by its L2 norm.::\n\n\nfor i in 0...N     out[:,i,:,...,:] = data[:,i,:,...,:] / sqrt(sum(data[:,i,:,...,:] ** 2) + eps)\n\n\nwith $mode$ = $spatial$, it normalizes the cross channel norm for each position in the array by its L2 norm.::\n\n\nfor dim in 2...N     for i in 0...N       out[.....,i,...] = take(out, indices=i, axis=dim) / sqrt(sum(take(out, indices=i, axis=dim) ** 2) + eps)           -dim-\n\n\nExample::\n\n\nx = [[[1,2],         [3,4]],        [[2,2],         [5,6]]]\n\n\nL2Normalization(x, mode='instance')   =[[[ 0.18257418  0.36514837]      [ 0.54772252  0.73029673]]     [[ 0.24077171  0.24077171]      [ 0.60192931  0.72231513]]]\n\n\nL2Normalization(x, mode='channel')   =[[[ 0.31622776  0.44721359]      [ 0.94868326  0.89442718]]     [[ 0.37139067  0.31622776]      [ 0.92847669  0.94868326]]]\n\n\nL2Normalization(x, mode='spatial')   =[[[ 0.44721359  0.89442718]      [ 0.60000002  0.80000001]]     [[ 0.70710677  0.70710677]      [ 0.6401844   0.76822126]]]\n\n\nDefined in src/operator/l2_normalization.cc:L93\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to normalize.\n\n\neps::float, optional, default=1e-10\n: A small constant for numerical stability.\n\n\nmode::{'channel', 'instance', 'spatial'},optional, default='instance'\n: Specify the dimension along which to compute L2 norm.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LRN\n \n \nMethod\n.\n\n\nLRN(data, alpha, beta, knorm, nsize)\n\n\n\n\nApplies local response normalization to the input.\n\n\nThe local response normalization layer performs \"lateral inhibition\" by normalizing over local input regions.\n\n\nIf :math:\na_{x,y}^{i}\n is the activity of a neuron computed by applying kernel :math:\ni\n at position :math:\n(x, y)\n and then applying the ReLU nonlinearity, the response-normalized activity :math:\nb_{x,y}^{i}\n is given by the expression:\n\n\n.. math::    b_{x,y}^{i} = \\frac{a_{x,y}^{i}}{\\Bigg({k + \\alpha \\sum_{j=max(0, i-\\frac{n}{2})}^{min(N-1, i+\\frac{n}{2})} (a_{x,y}^{j})^{2}}\\Bigg)^{\\beta}}\n\n\nwhere the sum runs over :math:\nn\n \"adjacent\" kernel maps at the same spatial position, and :math:\nN\n is the total number of kernels in the layer.\n\n\nDefined in src/operator/lrn.cc:L73\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nalpha::float, optional, default=0.0001\n: The variance scaling parameter :math:\n\u0007lpha\n in the LRN expression.\n\n\nbeta::float, optional, default=0.75\n: The power parameter :math:\n\beta\n in the LRN expression.\n\n\nknorm::float, optional, default=2\n: The parameter :math:\nk\n in the LRN expression.\n\n\nnsize::int (non-negative), required\n: normalization window width in elements.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LeakyReLU\n \n \nMethod\n.\n\n\nLeakyReLU(data, act_type, slope, lower_bound, upper_bound)\n\n\n\n\nApplies Leaky rectified linear unit activation element-wise to the input.\n\n\nLeaky ReLUs attempt to fix the \"dying ReLU\" problem by allowing a small \nslope\n when the input is negative and has a slope of one when input is positive.\n\n\nThe following modified ReLU Activation functions are supported:\n\n\n\n\nelu\n: Exponential Linear Unit. \ny = x \n 0 ? x : slope * (exp(x)-1)\n\n\nleaky\n: Leaky ReLU. \ny = x \n 0 ? x : slope * x\n\n\nprelu\n: Parametric ReLU. This is same as \nleaky\n except that \nslope\n is learnt during training.\n\n\nrrelu\n: Randomized ReLU. same as \nleaky\n but the \nslope\n is uniformly and randomly chosen from \n[lower_bound, upper_bound)\n for training, while fixed to be \n(lower_bound+upper_bound)/2\n for inference.\n\n\n\n\nDefined in src/operator/leaky_relu.cc:L58\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'\n: Activation function to be applied.\n\n\nslope::float, optional, default=0.25\n: Init slope for the activation. (For leaky and elu only)\n\n\nlower_bound::float, optional, default=0.125\n: Lower bound of random slope. (For rrelu only)\n\n\nupper_bound::float, optional, default=0.334\n: Upper bound of random slope. (For rrelu only)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LinearRegressionOutput\n \n \nMethod\n.\n\n\nLinearRegressionOutput(data, label, grad_scale)\n\n\n\n\nComputes and optimizes for squared loss during backward propagation. Just outputs $data$ during forward propagation.\n\n\nIf :math:\n\\hat{y}_i\n is the predicted value of the i-th sample, and :math:\ny_i\n is the corresponding target value, then the squared loss estimated over :math:\nn\n samples is defined as\n\n\n:math:\n\\text{SquaredLoss}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left( y_i - \\hat{y}_i \\right)^2\n\n\n.. note::    Use the LinearRegressionOutput as the final output layer of a net.\n\n\nBy default, gradients of this loss function are scaled by factor \n1/n\n, where n is the number of training examples. The parameter \ngrad_scale\n can be used to change this scale to \ngrad_scale/n\n.\n\n\nDefined in src/operator/regression_output.cc:L70\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label to the function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LogisticRegressionOutput\n \n \nMethod\n.\n\n\nLogisticRegressionOutput(data, label, grad_scale)\n\n\n\n\nApplies a logistic function to the input.\n\n\nThe logistic function, also known as the sigmoid function, is computed as :math:\n\\frac{1}{1+exp(-x)}\n.\n\n\nCommonly, the sigmoid is used to squash the real-valued output of a linear model :math:wTx+b into the [0,1] range so that it can be interpreted as a probability. It is suitable for binary classification or probability prediction tasks.\n\n\n.. note::    Use the LogisticRegressionOutput as the final output layer of a net.\n\n\nBy default, gradients of this loss function are scaled by factor \n1/n\n, where n is the number of training examples. The parameter \ngrad_scale\n can be used to change this scale to \ngrad_scale/n\n.\n\n\nDefined in src/operator/regression_output.cc:L112\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label to the function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MAERegressionOutput\n \n \nMethod\n.\n\n\nMAERegressionOutput(data, label, grad_scale)\n\n\n\n\nComputes mean absolute error of the input.\n\n\nMAE is a risk metric corresponding to the expected value of the absolute error.\n\n\nIf :math:\n\\hat{y}_i\n is the predicted value of the i-th sample, and :math:\ny_i\n is the corresponding target value, then the mean absolute error (MAE) estimated over :math:\nn\n samples is defined as\n\n\n:math:\n\\text{MAE}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left| y_i - \\hat{y}_i \\right|\n\n\n.. note::    Use the MAERegressionOutput as the final output layer of a net.\n\n\nBy default, gradients of this loss function are scaled by factor \n1/n\n, where n is the number of training examples. The parameter \ngrad_scale\n can be used to change this scale to \ngrad_scale/n\n.\n\n\nDefined in src/operator/regression_output.cc:L91\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the function.\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label to the function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MakeLoss\n \n \nMethod\n.\n\n\nMakeLoss(data, grad_scale, valid_thresh, normalization)\n\n\n\n\nMake your own loss function in network construction.\n\n\nThis operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.\n\n\nFor example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::\n\n\ncross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = MakeLoss(cross_entropy)\n\n\nWe will need to use $MakeLoss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.\n\n\nIn addition, we can give a scale to the loss by setting $grad_scale$, so that the gradient of the loss will be rescaled in the backpropagation.\n\n\n.. note:: This operator should be used as a Symbol instead of NDArray.\n\n\nDefined in src/operator/make_loss.cc:L71\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\ngrad_scale::float, optional, default=1\n: Gradient scale as a supplement to unary and binary operators\n\n\nvalid_thresh::float, optional, default=0\n: clip each element in the array to 0 when it is less than $valid_thresh$. This is used when $normalization$ is set to $'valid'$.\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If this is set to null, the output gradient will not be normalized. If this is set to batch, the output gradient will be divided by the batch size. If this is set to valid, the output gradient will be divided by the number of valid input elements.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pad\n \n \nMethod\n.\n\n\nPad(data, mode, pad_width, constant_value)\n\n\n\n\nPads an input array with a constant or edge values of the array.\n\n\n.. note:: \nPad\n is deprecated. Use \npad\n instead.\n\n\n.. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in \npad_width\n to be zero.\n\n\nThis operation pads an input array with either a \nconstant_value\n or edge values along each axis of the input array. The amount of padding is specified by \npad_width\n.\n\n\npad_width\n is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The \npad_width\n should be of length $2*N$ where $N$ is the number of dimensions of the array.\n\n\nFor dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.\n\n\nExample::\n\n\nx = [[[[  1.   2.   3.]           [  4.   5.   6.]]\n\n\n     [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]\n\n\n\n\npad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]\n\n\n\n\npad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]\n\n\n\n\nDefined in src/operator/pad.cc:L766\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: An n-dimensional input array.\n\n\nmode::{'constant', 'edge', 'reflect'}, required\n: Padding type to use. \"constant\" pads with \nconstant_value\n \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.\n\n\npad_width::Shape(tuple), required\n: Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.\n\n\nconstant_value::double, optional, default=0\n: The value used for padding when \nmode\n is \"constant\".\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling\n \n \nMethod\n.\n\n\nPooling(data, global_pool, cudnn_off, kernel, pool_type, pooling_convention, stride, pad)\n\n\n\n\nPerforms pooling on the input.\n\n\nThe shapes for 1-D pooling are\n\n\n\n\ndata\n: \n(batch_size, channel, width)\n,\n\n\nout\n: \n(batch_size, num_filter, out_width)\n.\n\n\n\n\nThe shapes for 2-D pooling are\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n, with::\n\n\nout_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])\n\n\n\n\n\n\nThe definition of \nf\n depends on $pooling_convention$, which has two options:\n\n\n\n\n\n\nvalid\n (default)::\n\n\nf(x, k, p, s) = floor((x+2*p-k)/s)+1\n  * \nfull\n, which is compatible with Caffe::\n\n\nf(x, k, p, s) = ceil((x+2*p-k)/s)+1\n\n\n\n\n\n\nBut $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.\n\n\nThree pooling options are supported by $pool_type$:\n\n\n\n\navg\n: average pooling\n\n\nmax\n: max pooling\n\n\nsum\n: sum pooling\n\n\n\n\nFor 3-D pooling, an additional \ndepth\n dimension is added before \nheight\n. Namely the input data will have shape \n(batch_size, channel, depth, height, width)\n.\n\n\nDefined in src/operator/nn/pooling.cc:L133\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=0\n: Ignore kernel size, do global pooling based on current input feature map.\n\n\ncudnn_off::boolean, optional, default=0\n: Turn off cudnn pooling and use MXNet pooling operator.\n\n\nkernel::Shape(tuple), required\n: Pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\npooling_convention::{'full', 'valid'},optional, default='valid'\n: Pooling convention to be applied.\n\n\nstride::Shape(tuple), optional, default=[]\n: Stride: for pooling (y, x) or (d, y, x). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: Pad for pooling: (y, x) or (d, y, x). Defaults to no padding.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling_v1\n \n \nMethod\n.\n\n\nPooling_v1(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)\n\n\n\n\nThis operator is DEPRECATED. Perform pooling on the input.\n\n\nThe shapes for 2-D pooling is\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n, with::\n\n\nout_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])\n\n\n\n\n\n\nThe definition of \nf\n depends on $pooling_convention$, which has two options:\n\n\n\n\n\n\nvalid\n (default)::\n\n\nf(x, k, p, s) = floor((x+2*p-k)/s)+1\n  * \nfull\n, which is compatible with Caffe::\n\n\nf(x, k, p, s) = ceil((x+2*p-k)/s)+1\n\n\n\n\n\n\nBut $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.\n\n\nThree pooling options are supported by $pool_type$:\n\n\n\n\navg\n: average pooling\n\n\nmax\n: max pooling\n\n\nsum\n: sum pooling\n\n\n\n\n1-D pooling is special case of 2-D pooling with \nweight=1\n and \nkernel[1]=1\n.\n\n\nFor 3-D pooling, an additional \ndepth\n dimension is added before \nheight\n. Namely the input data will have shape \n(batch_size, channel, depth, height, width)\n.\n\n\nDefined in src/operator/pooling_v1.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=0\n: Ignore kernel size, do global pooling based on current input feature map.\n\n\nkernel::Shape(tuple), required\n: pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\npooling_convention::{'full', 'valid'},optional, default='valid'\n: Pooling convention to be applied.\n\n\nstride::Shape(tuple), optional, default=[]\n: stride: for pooling (y, x) or (d, y, x)\n\n\npad::Shape(tuple), optional, default=[]\n: pad for pooling: (y, x) or (d, y, x)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.RNN\n \n \nMethod\n.\n\n\nRNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)\n\n\n\n\nApplies a recurrent layer to input.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to RNN\n\n\nparameters::NDArray-or-SymbolicNode\n: Vector of all RNN trainable parameters concatenated\n\n\nstate::NDArray-or-SymbolicNode\n: initial hidden state of the RNN\n\n\nstate_cell::NDArray-or-SymbolicNode\n: initial cell state for LSTM networks (only for LSTM)\n\n\nstate_size::int (non-negative), required\n: size of the state for each layer\n\n\nnum_layers::int (non-negative), required\n: number of stacked layers\n\n\nbidirectional::boolean, optional, default=0\n: whether to use bidirectional recurrent layers\n\n\nmode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required\n: the type of RNN to compute\n\n\np::float, optional, default=0\n: Dropout probability, fraction of the input that gets dropped out at training time\n\n\nstate_outputs::boolean, optional, default=0\n: Whether to have the states as symbol outputs.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ROIPooling\n \n \nMethod\n.\n\n\nROIPooling(data, rois, pooled_size, spatial_scale)\n\n\n\n\nPerforms region of interest(ROI) pooling on the input array.\n\n\nROI pooling is a variant of a max pooling layer, in which the output size is fixed and region of interest is a parameter. Its purpose is to perform max pooling on the inputs of non-uniform sizes to obtain fixed-size feature maps. ROI pooling is a neural-net layer mostly used in training a \nFast R-CNN\n network for object detection.\n\n\nThis operator takes a 4D feature map as an input array and region proposals as \nrois\n, then it pools over sub-regions of input and produces a fixed-sized output array regardless of the ROI size.\n\n\nTo crop the feature map accordingly, you can resize the bounding box coordinates by changing the parameters \nrois\n and \nspatial_scale\n.\n\n\nThe cropped feature maps are pooled by standard max pooling operation to a fixed size output indicated by a \npooled_size\n parameter. batch_size will change to the number of region bounding boxes after \nROIPooling\n.\n\n\nThe size of each region of interest doesn't have to be perfectly divisible by the number of pooling sections(\npooled_size\n).\n\n\nExample::\n\n\nx = [[[[  0.,   1.,   2.,   3.,   4.,   5.],          [  6.,   7.,   8.,   9.,  10.,  11.],          [ 12.,  13.,  14.,  15.,  16.,  17.],          [ 18.,  19.,  20.,  21.,  22.,  23.],          [ 24.,  25.,  26.,  27.,  28.,  29.],          [ 30.,  31.,  32.,  33.,  34.,  35.],          [ 36.,  37.,  38.,  39.,  40.,  41.],          [ 42.,  43.,  44.,  45.,  46.,  47.]]]]\n\n\n// region of interest i.e. bounding box coordinates.   y = [[0,0,0,4,4]]\n\n\n// returns array of shape (2,2) according to the given roi with max pooling.   ROIPooling(x, y, (2,2), 1.0) = [[[[ 14.,  16.],                                     [ 26.,  28.]]]]\n\n\n// region of interest is changed due to the change in \nspacial_scale\n parameter.   ROIPooling(x, y, (2,2), 0.7) = [[[[  7.,   9.],                                     [ 19.,  21.]]]]\n\n\nDefined in src/operator/roi_pooling.cc:L287\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array to the pooling operator,  a 4D Feature maps\n\n\nrois::NDArray-or-SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]], where (x1, y1) and (x2, y2) are top left and bottom right corners of designated region of interest. \nbatch_index\n indicates the index of corresponding image in the input array\n\n\npooled_size::Shape(tuple), required\n: ROI pooling output shape (h,w)\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SVMOutput\n \n \nMethod\n.\n\n\nSVMOutput(data, label, margin, regularization_coefficient, use_linear)\n\n\n\n\nComputes support vector machine based transformation of the input.\n\n\nThis tutorial demonstrates using SVM as output layer for classification instead of softmax: https://github.com/dmlc/mxnet/tree/master/example/svm_mnist.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data for SVM transformation.\n\n\nlabel::NDArray-or-SymbolicNode\n: Class label for the input data.\n\n\nmargin::float, optional, default=1\n: The loss function penalizes outputs that lie outside this margin. Default margin is 1.\n\n\nregularization_coefficient::float, optional, default=1\n: Regularization parameter for the SVM. This balances the tradeoff between coefficient size and error.\n\n\nuse_linear::boolean, optional, default=0\n: Whether to use L1-SVM objective. L2-SVM objective is used by default.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceLast\n \n \nMethod\n.\n\n\nSequenceLast(data, sequence_length, use_sequence_length)\n\n\n\n\nTakes the last element of a sequence.\n\n\nThis function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns a (n-1)-dimensional array of the form [batch_size, other_feature_dims].\n\n\nParameter \nsequence_length\n is used to handle variable-length sequences. \nsequence_length\n should be an input array of positive ints of dimension [batch_size]. To use this parameter, set \nuse_sequence_length\n to \nTrue\n, otherwise each example in the batch is assumed to have the max sequence length.\n\n\n.. note:: Alternatively, you can also use \ntake\n operator.\n\n\nExample::\n\n\nx = [[[  1.,   2.,   3.],          [  4.,   5.,   6.],          [  7.,   8.,   9.]],\n\n\n    [[ 10.,   11.,   12.],\n     [ 13.,   14.,   15.],\n     [ 16.,   17.,   18.]],\n\n    [[  19.,   20.,   21.],\n     [  22.,   23.,   24.],\n     [  25.,   26.,   27.]]]\n\n\n\n\n// returns last sequence when sequence_length parameter is not used    SequenceLast(x) = [[  19.,   20.,   21.],                       [  22.,   23.,   24.],                       [  25.,   26.,   27.]]\n\n\n// sequence_length y is used    SequenceLast(x, y=[1,1,1], use_sequence_length=True) =             [[  1.,   2.,   3.],              [  4.,   5.,   6.],              [  7.,   8.,   9.]]\n\n\n// sequence_length y is used    SequenceLast(x, y=[1,2,3], use_sequence_length=True) =             [[  1.,    2.,   3.],              [  13.,  14.,  15.],              [  25.,  26.,  27.]]\n\n\nDefined in src/operator/sequence_last.cc:L92\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n\n2\n\n\nsequence_length::NDArray-or-SymbolicNode\n: vector of sequence lengths of the form [batch_size]\n\n\nuse_sequence_length::boolean, optional, default=0\n: If set to true, this layer takes in an extra input parameter \nsequence_length\n to specify variable length sequence\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceMask\n \n \nMethod\n.\n\n\nSequenceMask(data, sequence_length, use_sequence_length, value)\n\n\n\n\nSets all elements outside the sequence to a constant value.\n\n\nThis function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.\n\n\nParameter \nsequence_length\n is used to handle variable-length sequences. \nsequence_length\n should be an input array of positive ints of dimension [batch_size]. To use this parameter, set \nuse_sequence_length\n to \nTrue\n, otherwise each example in the batch is assumed to have the max sequence length and this operator works as the \nidentity\n operator.\n\n\nExample::\n\n\nx = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],\n\n\n    [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]\n\n\n\n\n// Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]\n\n\n// Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]\n\n\n// works as identity operator when sequence_length parameter is not used    SequenceMask(x) = [[[  1.,   2.,   3.],                        [  4.,   5.,   6.]],\n\n\n                  [[  7.,   8.,   9.],\n                   [ 10.,  11.,  12.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]\n\n\n\n\n// sequence_length [1,1] means 1 of each batch will be kept    // and other rows are masked with default mask value = 0    SequenceMask(x, y=[1,1], use_sequence_length=True) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],\n\n\n             [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]],\n\n             [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]]]\n\n\n\n\n// sequence_length [2,3] means 2 of batch B1 and 3 of batch B2 will be kept    // and other rows are masked with value = 1    SequenceMask(x, y=[2,3], use_sequence_length=True, value=1) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],\n\n\n             [[  7.,   8.,   9.],\n              [  10.,  11.,  12.]],\n\n             [[   1.,   1.,   1.],\n              [  16.,  17.,  18.]]]\n\n\n\n\nDefined in src/operator/sequence_mask.cc:L114\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n\n2\n\n\nsequence_length::NDArray-or-SymbolicNode\n: vector of sequence lengths of the form [batch_size]\n\n\nuse_sequence_length::boolean, optional, default=0\n: If set to true, this layer takes in an extra input parameter \nsequence_length\n to specify variable length sequence\n\n\nvalue::float, optional, default=0\n: The value to be used as a mask.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SequenceReverse\n \n \nMethod\n.\n\n\nSequenceReverse(data, sequence_length, use_sequence_length)\n\n\n\n\nReverses the elements of each sequence.\n\n\nThis function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.\n\n\nParameter \nsequence_length\n is used to handle variable-length sequences. \nsequence_length\n should be an input array of positive ints of dimension [batch_size]. To use this parameter, set \nuse_sequence_length\n to \nTrue\n, otherwise each example in the batch is assumed to have the max sequence length.\n\n\nExample::\n\n\nx = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],\n\n\n    [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]\n\n\n\n\n// Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]\n\n\n// Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]\n\n\n// returns reverse sequence when sequence_length parameter is not used    SequenceReverse(x) = [[[ 13.,  14.,   15.],                           [ 16.,  17.,   18.]],\n\n\n                     [[  7.,   8.,   9.],\n                      [ 10.,  11.,  12.]],\n\n                     [[  1.,   2.,   3.],\n                      [  4.,   5.,   6.]]]\n\n\n\n\n// sequence_length [2,2] means 2 rows of    // both batch B1 and B2 will be reversed.    SequenceReverse(x, y=[2,2], use_sequence_length=True) =                      [[[  7.,   8.,   9.],                        [ 10.,  11.,  12.]],\n\n\n                  [[  1.,   2.,   3.],\n                   [  4.,   5.,   6.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]\n\n\n\n\n// sequence_length [2,3] means 2 of batch B2 and 3 of batch B3    // will be reversed.    SequenceReverse(x, y=[2,3], use_sequence_length=True) =                     [[[  7.,   8.,   9.],                       [ 16.,  17.,  18.]],\n\n\n                 [[  1.,   2.,   3.],\n                  [ 10.,  11.,  12.]],\n\n                 [[ 13.,  14,   15.],\n                  [  4.,   5.,   6.]]]\n\n\n\n\nDefined in src/operator/sequence_reverse.cc:L113\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: n-dimensional input array of the form [max_sequence_length, batch_size, other dims] where n\n2\n\n\nsequence_length::NDArray-or-SymbolicNode\n: vector of sequence lengths of the form [batch_size]\n\n\nuse_sequence_length::boolean, optional, default=0\n: If set to true, this layer takes in an extra input parameter \nsequence_length\n to specify variable length sequence\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SliceChannel\n \n \nMethod\n.\n\n\nSliceChannel(data, num_outputs, axis, squeeze_axis)\n\n\n\n\nSplits an array along a particular axis into multiple sub-arrays.\n\n\n.. note:: $SliceChannel$ is deprecated. Use $split$ instead.\n\n\nNote\n that \nnum_outputs\n should evenly divide the length of the axis along which to split the array.\n\n\nExample::\n\n\nx  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)\n\n\ny = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]\n\n\n   [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]\n\n\n\n\ny[0].shape = (3, 1, 1)\n\n\nz = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]\n\n\n   [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]\n\n\n\n\nz[0].shape = (1, 2, 1)\n\n\nsqueeze_axis=1\n removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $1$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to true only if $input.shape[axis] == num_outputs$.\n\n\nExample::\n\n\nz = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]\n\n\n   [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]\n\n\n\n\nz[0].shape = (2 ,1 )\n\n\nDefined in src/operator/slice_channel.cc:L107\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nnum_outputs::int, required\n: Number of splits. Note that this should evenly divide the length of the \naxis\n.\n\n\naxis::int, optional, default='1'\n: Axis along which to split.\n\n\nsqueeze_axis::boolean, optional, default=0\n: If true, Removes the axis with length 1 from the shapes of the output arrays. \nNote\n that setting \nsqueeze_axis\n to $true$ removes axis with length 1 only along the \naxis\n which it is split. Also \nsqueeze_axis\n can be set to $true$ only if $input.shape[axis] == num_outputs$.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Softmax\n \n \nMethod\n.\n\n\nSoftmax(data, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad, smooth_alpha)\n\n\n\n\nPlease use \nSoftmaxOutput\n.\n\n\n.. note::\n\n\nThis operator has been renamed to \nSoftmaxOutput\n, which   computes the gradient of cross-entropy loss w.r.t softmax output.   To just compute softmax output, use the \nsoftmax\n operator.\n\n\nDefined in src/operator/softmax_output.cc:L138\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\ngrad_scale::float, optional, default=1\n: Scales the gradient by a float factor.\n\n\nignore_label::float, optional, default=-1\n: The instances whose \nlabels\n == \nignore_label\n will be ignored during backward, if \nuse_ignore\n is set to $true$).\n\n\nmulti_output::boolean, optional, default=0\n: If set to $true$, the softmax function will be computed along axis $1$. This is applied when the shape of input array differs from the shape of label array.\n\n\nuse_ignore::boolean, optional, default=0\n: If set to $true$, the \nignore_label\n value will not contribute to the backward gradient.\n\n\npreserve_shape::boolean, optional, default=0\n: If set to $true$, the softmax function will be computed along the last axis ($-1$).\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: Normalizes the gradient.\n\n\nout_grad::boolean, optional, default=0\n: Multiplies gradient with output gradient element-wise.\n\n\nsmooth_alpha::float, optional, default=0\n: Constant for computing a label smoothed version of cross-entropyfor the backwards pass.  This constant gets subtracted from theone-hot encoding of the gold label and distributed uniformly toall other labels.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxActivation\n \n \nMethod\n.\n\n\nSoftmaxActivation(data, mode)\n\n\n\n\nApplies softmax activation to input. This is intended for internal layers.\n\n\n.. note::\n\n\nThis operator has been deprecated, please use \nsoftmax\n.\n\n\nIf \nmode\n = $instance$, this operator will compute a softmax for each instance in the batch. This is the default mode.\n\n\nIf \nmode\n = $channel$, this operator will compute a k-class softmax at each position of each instance, where \nk\n = $num_channel$. This mode can only be used when the input array has at least 3 dimensions. This can be used for \nfully convolutional network\n, \nimage segmentation\n, etc.\n\n\nExample::\n\n\n\n\n\n\n\n\ninput_array = mx.nd.array([[3., 0.5, -0.5, 2., 7.],                            [2., -.4, 7.,   3., 0.2]]) softmax_act = mx.nd.SoftmaxActivation(input_array) print softmax_act.asnumpy()\n\n\n\n\n\n\n\n\n[[  1.78322066e-02   1.46375655e-03   5.38485940e-04   6.56010211e-03   9.73605454e-01]    [  6.56221947e-03   5.95310994e-04   9.73919690e-01   1.78379621e-02   1.08472735e-03]]\n\n\nDefined in src/operator/nn/softmax_activation.cc:L67\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array to activation function.\n\n\nmode::{'channel', 'instance'},optional, default='instance'\n: Specifies how to compute the softmax. If set to $instance$, it computes softmax for each instance. If set to $channel$, It computes cross channel softmax for each position of each instance.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxOutput\n \n \nMethod\n.\n\n\nSoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad, smooth_alpha)\n\n\n\n\nComputes the gradient of cross entropy loss with respect to softmax output.\n\n\n\n\n\n\nThis operator computes the gradient in two steps. The cross entropy loss does not actually need to be computed.\n\n\n\n\nApplies softmax function on the input array.\n\n\nComputes and returns the gradient of cross entropy loss w.r.t. the softmax output.\n\n\n\n\nThe softmax function, cross entropy loss and gradient is given by:\n\n\n\n\n\n\nSoftmax Function:\n\n\n.. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n      * Cross Entropy Function:\n\n\n.. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)\n      * The gradient of cross entropy loss w.r.t softmax output:\n\n\n.. math:: \\text{gradient} = \\text{output} - \\text{label}\n  * During forward propagation, the softmax function is computed for each instance in the input array.\n\n\n\n\n\n\nFor general \nN\n-D input arrays with shape :math:\n(d_1, d_2, ..., d_n)\n. The size is :math:\ns=d_1 \\cdot d_2 \\cdot \\cdot \\cdot d_n\n. We can use the parameters \npreserve_shape\n and \nmulti_output\n to specify the way to compute softmax:\n\n\n\n\nBy default, \npreserve_shape\n is $false$. This operator will reshape the input array into a 2-D array with shape :math:\n(d_1, \\frac{s}{d_1})\n and then compute the softmax function for each row in the reshaped array, and afterwards reshape it back to the original shape :math:\n(d_1, d_2, ..., d_n)\n.\n\n\nIf \npreserve_shape\n is $true$, the softmax function will be computed along the last axis (\naxis\n = $-1$).\n\n\nIf \nmulti_output\n is $true$, the softmax function will be computed along the second axis (\naxis\n = $1$).\n\n\n\n\nDuring backward propagation, the gradient of cross-entropy loss w.r.t softmax output array is computed. The provided label can be a one-hot label array or a probability label array.\n\n\n\n\n\n\nIf the parameter \nuse_ignore\n is $true$, \nignore_label\n can specify input instances with a particular label to be ignored during backward propagation. \nThis has no effect when softmax \noutput\n has same shape as \nlabel\n.\n\n\nExample::\n\n\ndata = [[1,2,3,4],[2,2,2,2],[3,3,3,3],[4,4,4,4]]   label = [1,0,2,3]   ignore_label = 1   SoftmaxOutput(data=data, label = label,\n                multi_output=true, use_ignore=true,\n                ignore_label=ignore_label)\n\n\nforward softmax output\n\n\n[[ 0.0320586   0.08714432  0.23688284  0.64391428]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]]\n\n\nbackward gradient output\n\n\n[[ 0.    0.    0.    0.  ]    [-0.75  0.25  0.25  0.25]    [ 0.25  0.25 -0.75  0.25]    [ 0.25  0.25  0.25 -0.75]]\n\n\nnotice that the first row is all 0 because label[0] is 1, which is equal to ignore_label.\n\n\n  * The parameter `grad_scale` can be used to rescale the gradient, which is often used to give each loss function different weights.\n  * This operator also supports various ways to normalize the gradient by `normalization`, The `normalization` is applied if softmax output has different shape than the labels. The `normalization` mode can be set to the followings:\n\n\n\n\n\n$'null'$: do nothing.\n\n\n$'batch'$: divide the gradient by the batch size.\n\n\n$'valid'$: divide the gradient by the number of instances which are not ignored.\n\n\n\n\n\n\n\n\n\n\n\n\nDefined in src/operator/softmax_output.cc:L123\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\nlabel::NDArray-or-SymbolicNode\n: Ground truth label.\n\n\ngrad_scale::float, optional, default=1\n: Scales the gradient by a float factor.\n\n\nignore_label::float, optional, default=-1\n: The instances whose \nlabels\n == \nignore_label\n will be ignored during backward, if \nuse_ignore\n is set to $true$).\n\n\nmulti_output::boolean, optional, default=0\n: If set to $true$, the softmax function will be computed along axis $1$. This is applied when the shape of input array differs from the shape of label array.\n\n\nuse_ignore::boolean, optional, default=0\n: If set to $true$, the \nignore_label\n value will not contribute to the backward gradient.\n\n\npreserve_shape::boolean, optional, default=0\n: If set to $true$, the softmax function will be computed along the last axis ($-1$).\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: Normalizes the gradient.\n\n\nout_grad::boolean, optional, default=0\n: Multiplies gradient with output gradient element-wise.\n\n\nsmooth_alpha::float, optional, default=0\n: Constant for computing a label smoothed version of cross-entropyfor the backwards pass.  This constant gets subtracted from theone-hot encoding of the gold label and distributed uniformly toall other labels.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SpatialTransformer\n \n \nMethod\n.\n\n\nSpatialTransformer(data, loc, target_shape, transform_type, sampler_type)\n\n\n\n\nApplies a spatial transformer to input feature map.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the SpatialTransformerOp.\n\n\nloc::NDArray-or-SymbolicNode\n: localisation net, the output dim should be 6 when transform_type is affine. You shold initialize the weight and bias with identity tranform.\n\n\ntarget_shape::Shape(tuple), optional, default=[0,0]\n: output shape(h, w) of spatial transformer: (y, x)\n\n\ntransform_type::{'affine'}, required\n: transformation type\n\n\nsampler_type::{'bilinear'}, required\n: sampling type\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SwapAxis\n \n \nMethod\n.\n\n\nSwapAxis(data, dim1, dim2)\n\n\n\n\nInterchanges two axes of an array.\n\n\nExamples::\n\n\nx = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]\n\n\nx = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array\n\n\nswapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]\n\n\nDefined in src/operator/swapaxis.cc:L70\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UpSampling\n \n \nMethod\n.\n\n\nUpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)\n\n\n\n\nNote\n: UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.\n\n\nPerforms nearest neighbor/bilinear up sampling to inputs.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Array of tensors to upsample\n\n\nscale::int (non-negative), required\n: Up sampling scale\n\n\nnum_filter::int (non-negative), optional, default=0\n: Input filter. Only used by bilinear sample_type.\n\n\nsample_type::{'bilinear', 'nearest'}, required\n: upsampling method\n\n\nmulti_input_mode::{'concat', 'sum'},optional, default='concat'\n: How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.\n\n\nnum_args::int, required\n: Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale\nh_0,scale\nw_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CachedOp\n \n \nMethod\n.\n\n\n_CachedOp()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CrossDeviceCopy\n \n \nMethod\n.\n\n\n_CrossDeviceCopy()\n\n\n\n\nSpecial op to copy data cross device\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CustomFunction\n \n \nMethod\n.\n\n\n_CustomFunction()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Div\n \n \nMethod\n.\n\n\n_Div(lhs, rhs)\n\n\n\n\n_Div is an alias of elemwise_div.\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._DivScalar\n \n \nMethod\n.\n\n\n_DivScalar(data, scalar)\n\n\n\n\n_DivScalar is an alias of _div_scalar.\n\n\nDivide an array with a scalar.\n\n\n$_div_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Equal\n \n \nMethod\n.\n\n\n_Equal(lhs, rhs)\n\n\n\n\n_Equal is an alias of _equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._EqualScalar\n \n \nMethod\n.\n\n\n_EqualScalar(data, scalar)\n\n\n\n\n_EqualScalar is an alias of _equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater\n \n \nMethod\n.\n\n\n_Greater(lhs, rhs)\n\n\n\n\n_Greater is an alias of _greater.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterEqualScalar\n \n \nMethod\n.\n\n\n_GreaterEqualScalar(data, scalar)\n\n\n\n\n_GreaterEqualScalar is an alias of _greater_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._GreaterScalar\n \n \nMethod\n.\n\n\n_GreaterScalar(data, scalar)\n\n\n\n\n_GreaterScalar is an alias of _greater_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Greater_Equal\n \n \nMethod\n.\n\n\n_Greater_Equal(lhs, rhs)\n\n\n\n\n_Greater_Equal is an alias of _greater_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Hypot\n \n \nMethod\n.\n\n\n_Hypot(lhs, rhs)\n\n\n\n\n_Hypot is an alias of _hypot.\n\n\nGiven the \"legs\" of a right triangle, return its hypotenuse.\n\n\nDefined in src/operator/tensor/elemwise_binary_op_extended.cc:L79\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._HypotScalar\n \n \nMethod\n.\n\n\n_HypotScalar(data, scalar)\n\n\n\n\n_HypotScalar is an alias of _hypot_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser\n \n \nMethod\n.\n\n\n_Lesser(lhs, rhs)\n\n\n\n\n_Lesser is an alias of _lesser.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserEqualScalar\n \n \nMethod\n.\n\n\n_LesserEqualScalar(data, scalar)\n\n\n\n\n_LesserEqualScalar is an alias of _lesser_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._LesserScalar\n \n \nMethod\n.\n\n\n_LesserScalar(data, scalar)\n\n\n\n\n_LesserScalar is an alias of _lesser_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Lesser_Equal\n \n \nMethod\n.\n\n\n_Lesser_Equal(lhs, rhs)\n\n\n\n\n_Lesser_Equal is an alias of _lesser_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Maximum\n \n \nMethod\n.\n\n\n_Maximum(lhs, rhs)\n\n\n\n\n_Maximum is an alias of _maximum.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MaximumScalar\n \n \nMethod\n.\n\n\n_MaximumScalar(data, scalar)\n\n\n\n\n_MaximumScalar is an alias of _maximum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minimum\n \n \nMethod\n.\n\n\n_Minimum(lhs, rhs)\n\n\n\n\n_Minimum is an alias of _minimum.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinimumScalar\n \n \nMethod\n.\n\n\n_MinimumScalar(data, scalar)\n\n\n\n\n_MinimumScalar is an alias of _minimum_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minus\n \n \nMethod\n.\n\n\n_Minus(lhs, rhs)\n\n\n\n\n_Minus is an alias of elemwise_sub.\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinusScalar\n \n \nMethod\n.\n\n\n_MinusScalar(data, scalar)\n\n\n\n\n_MinusScalar is an alias of _minus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Mod\n \n \nMethod\n.\n\n\n_Mod(lhs, rhs)\n\n\n\n\n_Mod is an alias of _mod.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._ModScalar\n \n \nMethod\n.\n\n\n_ModScalar(data, scalar)\n\n\n\n\n_ModScalar is an alias of _mod_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Mul\n \n \nMethod\n.\n\n\n_Mul(lhs, rhs)\n\n\n\n\n_Mul is an alias of elemwise_mul.\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MulScalar\n \n \nMethod\n.\n\n\n_MulScalar(data, scalar)\n\n\n\n\n_MulScalar is an alias of _mul_scalar.\n\n\nMultiply an array with a scalar.\n\n\n$_mul_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NDArray\n \n \nMethod\n.\n\n\n_NDArray(data, info)\n\n\n\n\nStub for implementing an operator implemented in native frontend language with ndarray.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Input data for the custom operator.\n\n\ninfo::ptr, required\n:\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Native\n \n \nMethod\n.\n\n\n_Native(data, info, need_top_grad)\n\n\n\n\nStub for implementing an operator implemented in native frontend language.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: Input data for the custom operator.\n\n\ninfo::ptr, required\n:\n\n\nneed_top_grad::boolean, optional, default=1\n: Whether this layer needs out grad for backward. Should be false for loss layers.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NoGradient\n \n \nMethod\n.\n\n\n_NoGradient()\n\n\n\n\nPlace holder for variable who cannot perform gradient\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NotEqualScalar\n \n \nMethod\n.\n\n\n_NotEqualScalar(data, scalar)\n\n\n\n\n_NotEqualScalar is an alias of _not_equal_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Not_Equal\n \n \nMethod\n.\n\n\n_Not_Equal(lhs, rhs)\n\n\n\n\n_Not_Equal is an alias of _not_equal.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Plus\n \n \nMethod\n.\n\n\n_Plus(lhs, rhs)\n\n\n\n\n_Plus is an alias of elemwise_add.\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PlusScalar\n \n \nMethod\n.\n\n\n_PlusScalar(data, scalar)\n\n\n\n\n_PlusScalar is an alias of _plus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Power\n \n \nMethod\n.\n\n\n_Power(lhs, rhs)\n\n\n\n\n_Power is an alias of _power.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PowerScalar\n \n \nMethod\n.\n\n\n_PowerScalar(data, scalar)\n\n\n\n\n_PowerScalar is an alias of _power_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RDivScalar\n \n \nMethod\n.\n\n\n_RDivScalar(data, scalar)\n\n\n\n\n_RDivScalar is an alias of _rdiv_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RMinusScalar\n \n \nMethod\n.\n\n\n_RMinusScalar(data, scalar)\n\n\n\n\n_RMinusScalar is an alias of _rminus_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RModScalar\n \n \nMethod\n.\n\n\n_RModScalar(data, scalar)\n\n\n\n\n_RModScalar is an alias of _rmod_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RPowerScalar\n \n \nMethod\n.\n\n\n_RPowerScalar(data, scalar)\n\n\n\n\n_RPowerScalar is an alias of _rpower_scalar.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._add\n \n \nMethod\n.\n\n\n_add(lhs, rhs)\n\n\n\n\n_add is an alias of elemwise_add.\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._arange\n \n \nMethod\n.\n\n\n_arange(start, stop, step, repeat, ctx, dtype)\n\n\n\n\nReturn evenly spaced values within a given interval. Similar to Numpy\n\n\nArguments\n\n\n\n\nstart::double, required\n: Start of interval. The interval includes this value. The default start value is 0.\n\n\nstop::double or None, optional, default=None\n: End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.\n\n\nstep::double, optional, default=1\n: Spacing between values.\n\n\nrepeat::int, optional, default='1'\n: The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013\n a, a, a.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'int64', 'uint8'},optional, default='float32'\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Activation\n \n \nMethod\n.\n\n\n_backward_Activation()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BatchNorm\n \n \nMethod\n.\n\n\n_backward_BatchNorm()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BatchNorm_v1\n \n \nMethod\n.\n\n\n_backward_BatchNorm_v1()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_BilinearSampler\n \n \nMethod\n.\n\n\n_backward_BilinearSampler()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_CachedOp\n \n \nMethod\n.\n\n\n_backward_CachedOp()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Concat\n \n \nMethod\n.\n\n\n_backward_Concat()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Convolution\n \n \nMethod\n.\n\n\n_backward_Convolution()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Convolution_v1\n \n \nMethod\n.\n\n\n_backward_Convolution_v1()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Correlation\n \n \nMethod\n.\n\n\n_backward_Correlation()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Crop\n \n \nMethod\n.\n\n\n_backward_Crop()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Custom\n \n \nMethod\n.\n\n\n_backward_Custom()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_CustomFunction\n \n \nMethod\n.\n\n\n_backward_CustomFunction()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Deconvolution\n \n \nMethod\n.\n\n\n_backward_Deconvolution()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Dropout\n \n \nMethod\n.\n\n\n_backward_Dropout()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Embedding\n \n \nMethod\n.\n\n\n_backward_Embedding()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_FullyConnected\n \n \nMethod\n.\n\n\n_backward_FullyConnected()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_GridGenerator\n \n \nMethod\n.\n\n\n_backward_GridGenerator()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\n_backward_IdentityAttachKLSparseReg()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_InstanceNorm\n \n \nMethod\n.\n\n\n_backward_InstanceNorm()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_L2Normalization\n \n \nMethod\n.\n\n\n_backward_L2Normalization()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LRN\n \n \nMethod\n.\n\n\n_backward_LRN()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LeakyReLU\n \n \nMethod\n.\n\n\n_backward_LeakyReLU()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LinearRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LinearRegressionOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_LogisticRegressionOutput\n \n \nMethod\n.\n\n\n_backward_LogisticRegressionOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MAERegressionOutput\n \n \nMethod\n.\n\n\n_backward_MAERegressionOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_MakeLoss\n \n \nMethod\n.\n\n\n_backward_MakeLoss()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pad\n \n \nMethod\n.\n\n\n_backward_Pad()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pooling\n \n \nMethod\n.\n\n\n_backward_Pooling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Pooling_v1\n \n \nMethod\n.\n\n\n_backward_Pooling_v1()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_RNN\n \n \nMethod\n.\n\n\n_backward_RNN()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_ROIPooling\n \n \nMethod\n.\n\n\n_backward_ROIPooling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SVMOutput\n \n \nMethod\n.\n\n\n_backward_SVMOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceLast\n \n \nMethod\n.\n\n\n_backward_SequenceLast()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceMask\n \n \nMethod\n.\n\n\n_backward_SequenceMask()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SequenceReverse\n \n \nMethod\n.\n\n\n_backward_SequenceReverse()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SliceChannel\n \n \nMethod\n.\n\n\n_backward_SliceChannel()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_Softmax\n \n \nMethod\n.\n\n\n_backward_Softmax()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxActivation\n \n \nMethod\n.\n\n\n_backward_SoftmaxActivation()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SoftmaxOutput\n \n \nMethod\n.\n\n\n_backward_SoftmaxOutput()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SparseEmbedding\n \n \nMethod\n.\n\n\n_backward_SparseEmbedding()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SpatialTransformer\n \n \nMethod\n.\n\n\n_backward_SpatialTransformer()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_SwapAxis\n \n \nMethod\n.\n\n\n_backward_SwapAxis()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_UpSampling\n \n \nMethod\n.\n\n\n_backward_UpSampling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__CrossDeviceCopy\n \n \nMethod\n.\n\n\n_backward__CrossDeviceCopy()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__NDArray\n \n \nMethod\n.\n\n\n_backward__NDArray()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__Native\n \n \nMethod\n.\n\n\n_backward__Native()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_CTCLoss\n \n \nMethod\n.\n\n\n_backward__contrib_CTCLoss()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_DeformableConvolution\n \n \nMethod\n.\n\n\n_backward__contrib_DeformableConvolution()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_DeformablePSROIPooling\n \n \nMethod\n.\n\n\n_backward__contrib_DeformablePSROIPooling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiBoxDetection\n \n \nMethod\n.\n\n\n_backward__contrib_MultiBoxDetection()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiBoxPrior\n \n \nMethod\n.\n\n\n_backward__contrib_MultiBoxPrior()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiBoxTarget\n \n \nMethod\n.\n\n\n_backward__contrib_MultiBoxTarget()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_MultiProposal\n \n \nMethod\n.\n\n\n_backward__contrib_MultiProposal()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_PSROIPooling\n \n \nMethod\n.\n\n\n_backward__contrib_PSROIPooling()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_Proposal\n \n \nMethod\n.\n\n\n_backward__contrib_Proposal()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_count_sketch\n \n \nMethod\n.\n\n\n_backward__contrib_count_sketch()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_fft\n \n \nMethod\n.\n\n\n_backward__contrib_fft()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward__contrib_ifft\n \n \nMethod\n.\n\n\n_backward__contrib_ifft()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_abs\n \n \nMethod\n.\n\n\n_backward_abs(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_add\n \n \nMethod\n.\n\n\n_backward_add()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccos\n \n \nMethod\n.\n\n\n_backward_arccos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arccosh\n \n \nMethod\n.\n\n\n_backward_arccosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsin\n \n \nMethod\n.\n\n\n_backward_arcsin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arcsinh\n \n \nMethod\n.\n\n\n_backward_arcsinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctan\n \n \nMethod\n.\n\n\n_backward_arctan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_arctanh\n \n \nMethod\n.\n\n\n_backward_arctanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_batch_dot\n \n \nMethod\n.\n\n\n_backward_batch_dot()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_add\n \n \nMethod\n.\n\n\n_backward_broadcast_add()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_div\n \n \nMethod\n.\n\n\n_backward_broadcast_div()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_hypot\n \n \nMethod\n.\n\n\n_backward_broadcast_hypot()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_maximum\n \n \nMethod\n.\n\n\n_backward_broadcast_maximum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_minimum\n \n \nMethod\n.\n\n\n_backward_broadcast_minimum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_mod\n \n \nMethod\n.\n\n\n_backward_broadcast_mod()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_mul\n \n \nMethod\n.\n\n\n_backward_broadcast_mul()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_power\n \n \nMethod\n.\n\n\n_backward_broadcast_power()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_broadcast_sub\n \n \nMethod\n.\n\n\n_backward_broadcast_sub()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cast\n \n \nMethod\n.\n\n\n_backward_cast()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cbrt\n \n \nMethod\n.\n\n\n_backward_cbrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_clip\n \n \nMethod\n.\n\n\n_backward_clip()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_contrib_bipartite_matching\n \n \nMethod\n.\n\n\n_backward_contrib_bipartite_matching(is_ascend, threshold, topk)\n\n\n\n\nArguments\n\n\n\n\nis_ascend::boolean, optional, default=0\n: Use ascend order for scores instead of descending. Please set threshold accordingly.\n\n\nthreshold::float, required\n: Ignore matching when score \n thresh, if is_ascend=false, or ignore score \n thresh, if is_ascend=true.\n\n\ntopk::int, optional, default='-1'\n: Limit the number of matches to topk, set -1 for no limit\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_contrib_box_iou\n \n \nMethod\n.\n\n\n_backward_contrib_box_iou(format)\n\n\n\n\nArguments\n\n\n\n\nformat::{'center', 'corner'},optional, default='corner'\n: The box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_contrib_box_nms\n \n \nMethod\n.\n\n\n_backward_contrib_box_nms(overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)\n\n\n\n\nArguments\n\n\n\n\noverlap_thresh::float, optional, default=0.5\n: Overlapping(IoU) threshold to suppress object with smaller score.\n\n\ntopk::int, optional, default='-1'\n: Apply nms to topk boxes with descending scores, -1 to no restriction.\n\n\ncoord_start::int, optional, default='2'\n: Start index of the consecutive 4 coordinates.\n\n\nscore_index::int, optional, default='1'\n: Index of the scores/confidence of boxes.\n\n\nid_index::int, optional, default='-1'\n: Optional, index of the class categories, -1 to disable.\n\n\nforce_suppress::boolean, optional, default=0\n: Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category\n\n\nin_format::{'center', 'corner'},optional, default='corner'\n: The input box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nout_format::{'center', 'corner'},optional, default='corner'\n: The output box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_copy\n \n \nMethod\n.\n\n\n_backward_copy()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cos\n \n \nMethod\n.\n\n\n_backward_cos(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_cosh\n \n \nMethod\n.\n\n\n_backward_cosh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_degrees\n \n \nMethod\n.\n\n\n_backward_degrees(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_div\n \n \nMethod\n.\n\n\n_backward_div()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_div_scalar\n \n \nMethod\n.\n\n\n_backward_div_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_dot\n \n \nMethod\n.\n\n\n_backward_dot(transpose_a, transpose_b)\n\n\n\n\nArguments\n\n\n\n\ntranspose_a::boolean, optional, default=0\n: If true then transpose the first input before dot.\n\n\ntranspose_b::boolean, optional, default=0\n: If true then transpose the second input before dot.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_expm1\n \n \nMethod\n.\n\n\n_backward_expm1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gamma\n \n \nMethod\n.\n\n\n_backward_gamma(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_gammaln\n \n \nMethod\n.\n\n\n_backward_gammaln(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot\n \n \nMethod\n.\n\n\n_backward_hypot()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_hypot_scalar\n \n \nMethod\n.\n\n\n_backward_hypot_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_gelqf\n \n \nMethod\n.\n\n\n_backward_linalg_gelqf()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_gemm\n \n \nMethod\n.\n\n\n_backward_linalg_gemm()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_gemm2\n \n \nMethod\n.\n\n\n_backward_linalg_gemm2()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_potrf\n \n \nMethod\n.\n\n\n_backward_linalg_potrf()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_potri\n \n \nMethod\n.\n\n\n_backward_linalg_potri()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_sumlogdiag\n \n \nMethod\n.\n\n\n_backward_linalg_sumlogdiag()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_syevd\n \n \nMethod\n.\n\n\n_backward_linalg_syevd()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_syrk\n \n \nMethod\n.\n\n\n_backward_linalg_syrk()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_trmm\n \n \nMethod\n.\n\n\n_backward_linalg_trmm()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_linalg_trsm\n \n \nMethod\n.\n\n\n_backward_linalg_trsm()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log\n \n \nMethod\n.\n\n\n_backward_log(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log10\n \n \nMethod\n.\n\n\n_backward_log10(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log1p\n \n \nMethod\n.\n\n\n_backward_log1p(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log2\n \n \nMethod\n.\n\n\n_backward_log2(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_log_softmax\n \n \nMethod\n.\n\n\n_backward_log_softmax(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_max\n \n \nMethod\n.\n\n\n_backward_max()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum\n \n \nMethod\n.\n\n\n_backward_maximum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_maximum_scalar\n \n \nMethod\n.\n\n\n_backward_maximum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mean\n \n \nMethod\n.\n\n\n_backward_mean()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_min\n \n \nMethod\n.\n\n\n_backward_min()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum\n \n \nMethod\n.\n\n\n_backward_minimum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_minimum_scalar\n \n \nMethod\n.\n\n\n_backward_minimum_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mod\n \n \nMethod\n.\n\n\n_backward_mod()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mod_scalar\n \n \nMethod\n.\n\n\n_backward_mod_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mul\n \n \nMethod\n.\n\n\n_backward_mul()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_mul_scalar\n \n \nMethod\n.\n\n\n_backward_mul_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nanprod\n \n \nMethod\n.\n\n\n_backward_nanprod()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_nansum\n \n \nMethod\n.\n\n\n_backward_nansum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_pick\n \n \nMethod\n.\n\n\n_backward_pick()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power\n \n \nMethod\n.\n\n\n_backward_power()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_power_scalar\n \n \nMethod\n.\n\n\n_backward_power_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_prod\n \n \nMethod\n.\n\n\n_backward_prod()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_radians\n \n \nMethod\n.\n\n\n_backward_radians(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rcbrt\n \n \nMethod\n.\n\n\n_backward_rcbrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rdiv_scalar\n \n \nMethod\n.\n\n\n_backward_rdiv_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_reciprocal\n \n \nMethod\n.\n\n\n_backward_reciprocal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_relu\n \n \nMethod\n.\n\n\n_backward_relu(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_repeat\n \n \nMethod\n.\n\n\n_backward_repeat()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_reverse\n \n \nMethod\n.\n\n\n_backward_reverse()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rmod_scalar\n \n \nMethod\n.\n\n\n_backward_rmod_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rpower_scalar\n \n \nMethod\n.\n\n\n_backward_rpower_scalar(lhs, rhs, scalar)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nscalar::float\n: scalar value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_rsqrt\n \n \nMethod\n.\n\n\n_backward_rsqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sample_multinomial\n \n \nMethod\n.\n\n\n_backward_sample_multinomial()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sigmoid\n \n \nMethod\n.\n\n\n_backward_sigmoid(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sign\n \n \nMethod\n.\n\n\n_backward_sign(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sin\n \n \nMethod\n.\n\n\n_backward_sin(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sinh\n \n \nMethod\n.\n\n\n_backward_sinh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_slice\n \n \nMethod\n.\n\n\n_backward_slice()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_slice_axis\n \n \nMethod\n.\n\n\n_backward_slice_axis()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_smooth_l1\n \n \nMethod\n.\n\n\n_backward_smooth_l1(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_softmax\n \n \nMethod\n.\n\n\n_backward_softmax(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_softmax_cross_entropy\n \n \nMethod\n.\n\n\n_backward_softmax_cross_entropy()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sparse_retain\n \n \nMethod\n.\n\n\n_backward_sparse_retain()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sqrt\n \n \nMethod\n.\n\n\n_backward_sqrt(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_square\n \n \nMethod\n.\n\n\n_backward_square(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_square_sum\n \n \nMethod\n.\n\n\n_backward_square_sum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_stack\n \n \nMethod\n.\n\n\n_backward_stack()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sub\n \n \nMethod\n.\n\n\n_backward_sub()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_sum\n \n \nMethod\n.\n\n\n_backward_sum()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_take\n \n \nMethod\n.\n\n\n_backward_take()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tan\n \n \nMethod\n.\n\n\n_backward_tan(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tanh\n \n \nMethod\n.\n\n\n_backward_tanh(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_tile\n \n \nMethod\n.\n\n\n_backward_tile()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_topk\n \n \nMethod\n.\n\n\n_backward_topk()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._backward_where\n \n \nMethod\n.\n\n\n_backward_where()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast_backward\n \n \nMethod\n.\n\n\n_broadcast_backward()\n\n\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_CTCLoss\n \n \nMethod\n.\n\n\n_contrib_CTCLoss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)\n\n\n\n\nConnectionist Temporal Classification Loss.\n\n\nThe shapes of the inputs and outputs:\n\n\n\n\ndata\n: \n(sequence_length, batch_size, alphabet_size)\n\n\nlabel\n: \n(batch_size, label_sequence_length)\n\n\nout\n: \n(batch_size)\n\n\n\n\nThe \ndata\n tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When \nblank_label\n is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.\n\n\n$label$ is an index matrix of integers. When \nblank_label\n is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when \nblank_label\n is $\"last\"$, the value \n(alphabet_size-1)\n is reserved for blank label.\n\n\nIf a sequence of labels is shorter than \nlabel_sequence_length\n, use the special padding value at the end of the sequence to conform it to the correct length. The padding value is \n0\n when \nblank_label\n is $\"first\"$, and \n-1\n otherwise.\n\n\nFor example, suppose the vocabulary is \n[a, b, c]\n, and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When \nblank_label\n is $\"first\"$, we can index the labels as \n{'a': 1, 'b': 2, 'c': 3}\n, and we reserve the 0-th channel for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]\n\n\nWhen \nblank_label\n is $\"last\"$, we can index the labels as \n{'a': 0, 'b': 1, 'c': 2}\n, and we reserve the channel index 3 for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]\n\n\n$out$ is a list of CTC loss values, one per example in the batch.\n\n\nSee \nConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks\n, A. Graves \net al\n. for more information on the definition and the algorithm.\n\n\nDefined in src/operator/contrib/ctc_loss.cc:L115\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ctc_loss op.\n\n\nlabel::NDArray-or-SymbolicNode\n: Ground-truth labels for the loss.\n\n\ndata_lengths::NDArray-or-SymbolicNode\n: Lengths of data for each of the samples. Only required when use_data_lengths is true.\n\n\nlabel_lengths::NDArray-or-SymbolicNode\n: Lengths of labels for each of the samples. Only required when use_label_lengths is true.\n\n\nuse_data_lengths::boolean, optional, default=0\n: Whether the data lenghts are decided by \ndata_lengths\n. If false, the lengths are equal to the max sequence length.\n\n\nuse_label_lengths::boolean, optional, default=0\n: Whether the label lenghts are decided by \nlabel_lengths\n, or derived from \npadding_mask\n. If false, the lengths are derived from the first occurrence of the value of \npadding_mask\n. The value of \npadding_mask\n is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See \nblank_label\n.\n\n\nblank_label::{'first', 'last'},optional, default='first'\n: Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_DeformableConvolution\n \n \nMethod\n.\n\n\n_contrib_DeformableConvolution(data, offset, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, num_deformable_group, workspace, no_bias, layout)\n\n\n\n\nCompute 2-D deformable convolution on 4-D input.\n\n\nThe deformable convolution operation is described in https://arxiv.org/abs/1703.06211\n\n\nFor 2-D deformable convolution, the shapes are\n\n\n\n\ndata\n: \n(batch_size, channel, height, width)\n\n\noffset\n: \n(batch_size, num_deformable_group * kernel[0] * kernel[1], height, width)\n\n\nweight\n: \n(num_filter, channel, kernel[0], kernel[1])\n\n\nbias\n: \n(num_filter,)\n\n\nout\n: \n(batch_size, num_filter, out_height, out_width)\n.\n\n\n\n\nDefine::\n\n\nf(x,k,p,s,d) = floor((x+2\np-d\n(k-1)-1)/s)+1\n\n\nthen we have::\n\n\nout_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])\n\n\nIf $no_bias$ is set to be true, then the $bias$ term is ignored.\n\n\nThe default data $layout$ is \nNCHW\n, namely \n(batch_size, channle, height, width)\n.\n\n\nIf $num_group$ is larger than 1, denoted by \ng\n, then split the input $data$ evenly into \ng\n parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the \ni\n-th part of the data with the \ni\n-th weight part. The output is obtained by concating all the \ng\n results.\n\n\nIf $num_deformable_group$ is larger than 1, denoted by \ndg\n, then split the input $offset$ evenly into \ndg\n parts along the channel axis, and also evenly split $out$ evenly into \ndg\n parts along the channel axis. Next compute the deformable convolution, apply the \ni\n-th part of the offset part on the \ni\n-th out.\n\n\nBoth $weight$ and $bias$ are learnable parameters.\n\n\nDefined in src/operator/contrib/deformable_convolution.cc:L100\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the DeformableConvolutionOp.\n\n\noffset::NDArray-or-SymbolicNode\n: Input offset to the DeformableConvolutionOp.\n\n\nweight::NDArray-or-SymbolicNode\n: Weight matrix.\n\n\nbias::NDArray-or-SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: Convolution kernel size: (h, w) or (d, h, w)\n\n\nstride::Shape(tuple), optional, default=[]\n: Convolution stride: (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\ndilate::Shape(tuple), optional, default=[]\n: Convolution dilate: (h, w) or (d, h, w). Defaults to 1 for each dimension.\n\n\npad::Shape(tuple), optional, default=[]\n: Zero pad for convolution: (h, w) or (d, h, w). Defaults to no padding.\n\n\nnum_filter::int (non-negative), required\n: Convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of group partitions.\n\n\nnum_deformable_group::int (non-negative), optional, default=1\n: Number of deformable group partitions.\n\n\nworkspace::long (non-negative), optional, default=1024\n: Maximum temperal workspace allowed for convolution (MB).\n\n\nno_bias::boolean, optional, default=0\n: Whether to disable bias parameter.\n\n\nlayout::{None, 'NCDHW', 'NCHW', 'NCW'},optional, default='None'\n: Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_DeformablePSROIPooling\n \n \nMethod\n.\n\n\n_contrib_DeformablePSROIPooling(data, rois, trans, spatial_scale, output_dim, group_size, pooled_size, part_size, sample_per_part, trans_std, no_trans)\n\n\n\n\nPerforms deformable position-sensitive region-of-interest pooling on inputs. The DeformablePSROIPooling operation is described in https://arxiv.org/abs/1703.06211 .batch_size will change to the number of region bounding boxes after DeformablePSROIPooling\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\ntrans::SymbolicNode\n: transition parameter\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\noutput_dim::int, required\n: fix output dim\n\n\ngroup_size::int, required\n: fix group size\n\n\npooled_size::int, required\n: fix pooled size\n\n\npart_size::int, optional, default='0'\n: fix part size\n\n\nsample_per_part::int, optional, default='1'\n: fix samples per part\n\n\ntrans_std::float, optional, default=0\n: fix transition std\n\n\nno_trans::boolean, optional, default=0\n: Whether to disable trans parameter.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiBoxDetection\n \n \nMethod\n.\n\n\n_contrib_MultiBoxDetection(cls_prob, loc_pred, anchor, clip, threshold, background_id, nms_threshold, force_suppress, variances, nms_topk)\n\n\n\n\nConvert multibox detection predictions.\n\n\nArguments\n\n\n\n\ncls_prob::NDArray-or-SymbolicNode\n: Class probabilities.\n\n\nloc_pred::NDArray-or-SymbolicNode\n: Location regression predictions.\n\n\nanchor::NDArray-or-SymbolicNode\n: Multibox prior anchor boxes\n\n\nclip::boolean, optional, default=1\n: Clip out-of-boundary boxes.\n\n\nthreshold::float, optional, default=0.01\n: Threshold to be a positive prediction.\n\n\nbackground_id::int, optional, default='0'\n: Background id.\n\n\nnms_threshold::float, optional, default=0.5\n: Non-maximum suppression threshold.\n\n\nforce_suppress::boolean, optional, default=0\n: Suppress all detections regardless of class_id.\n\n\nvariances::tuple of \nfloat\n, optional, default=[0.1,0.1,0.2,0.2]\n: Variances to be decoded from box regression output.\n\n\nnms_topk::int, optional, default='-1'\n: Keep maximum top k detections before nms, -1 for no limit.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiBoxPrior\n \n \nMethod\n.\n\n\n_contrib_MultiBoxPrior(data, sizes, ratios, clip, steps, offsets)\n\n\n\n\nGenerate prior(anchor) boxes from data, sizes and ratios.\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data.\n\n\nsizes::tuple of \nfloat\n, optional, default=[1]\n: List of sizes of generated MultiBoxPriores.\n\n\nratios::tuple of \nfloat\n, optional, default=[1]\n: List of aspect ratios of generated MultiBoxPriores.\n\n\nclip::boolean, optional, default=0\n: Whether to clip out-of-boundary boxes.\n\n\nsteps::tuple of \nfloat\n, optional, default=[-1,-1]\n: Priorbox step across y and x, -1 for auto calculation.\n\n\noffsets::tuple of \nfloat\n, optional, default=[0.5,0.5]\n: Priorbox center offsets, y and x respectively\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiBoxTarget\n \n \nMethod\n.\n\n\n_contrib_MultiBoxTarget(anchor, label, cls_pred, overlap_threshold, ignore_label, negative_mining_ratio, negative_mining_thresh, minimum_negative_samples, variances)\n\n\n\n\nCompute Multibox training targets\n\n\nArguments\n\n\n\n\nanchor::NDArray-or-SymbolicNode\n: Generated anchor boxes.\n\n\nlabel::NDArray-or-SymbolicNode\n: Object detection labels.\n\n\ncls_pred::NDArray-or-SymbolicNode\n: Class predictions.\n\n\noverlap_threshold::float, optional, default=0.5\n: Anchor-GT overlap threshold to be regarded as a positive match.\n\n\nignore_label::float, optional, default=-1\n: Label for ignored anchors.\n\n\nnegative_mining_ratio::float, optional, default=-1\n: Max negative to positive samples ratio, use -1 to disable mining\n\n\nnegative_mining_thresh::float, optional, default=0.5\n: Threshold used for negative mining.\n\n\nminimum_negative_samples::int, optional, default='0'\n: Minimum number of negative samples.\n\n\nvariances::tuple of \nfloat\n, optional, default=[0.1,0.1,0.2,0.2]\n: Variances to be encoded in box regression target.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_MultiProposal\n \n \nMethod\n.\n\n\n_contrib_MultiProposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)\n\n\n\n\nGenerate region proposals via RPN\n\n\nArguments\n\n\n\n\ncls_score::NDArray-or-SymbolicNode\n: Score of how likely proposal is object.\n\n\nbbox_pred::NDArray-or-SymbolicNode\n: BBox Predicted deltas from anchors for proposals\n\n\nim_info::NDArray-or-SymbolicNode\n: Image size and scale.\n\n\nrpn_pre_nms_top_n::int, optional, default='6000'\n: Number of top scoring boxes to keep after applying NMS to RPN proposals\n\n\nrpn_post_nms_top_n::int, optional, default='300'\n: Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU \n= this threshold\n\n\nthreshold::float, optional, default=0.7\n: NMS value, below which to suppress.\n\n\nrpn_min_size::int, optional, default='16'\n: Minimum height or width in proposal\n\n\nscales::tuple of \nfloat\n, optional, default=[4,8,16,32]\n: Used to generate anchor windows by enumerating scales\n\n\nratios::tuple of \nfloat\n, optional, default=[0.5,1,2]\n: Used to generate anchor windows by enumerating ratios\n\n\nfeature_stride::int, optional, default='16'\n: The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.\n\n\noutput_score::boolean, optional, default=0\n: Add score to outputs\n\n\niou_loss::boolean, optional, default=0\n: Usage of IoU Loss\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_PSROIPooling\n \n \nMethod\n.\n\n\n_contrib_PSROIPooling(data, rois, spatial_scale, output_dim, pooled_size, group_size)\n\n\n\n\nPerforms region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after PSROIPooling\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\noutput_dim::int, required\n: fix output dim\n\n\npooled_size::int, required\n: fix pooled size\n\n\ngroup_size::int, optional, default='0'\n: fix group size\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_Proposal\n \n \nMethod\n.\n\n\n_contrib_Proposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)\n\n\n\n\nGenerate region proposals via RPN\n\n\nArguments\n\n\n\n\ncls_score::NDArray-or-SymbolicNode\n: Score of how likely proposal is object.\n\n\nbbox_pred::NDArray-or-SymbolicNode\n: BBox Predicted deltas from anchors for proposals\n\n\nim_info::NDArray-or-SymbolicNode\n: Image size and scale.\n\n\nrpn_pre_nms_top_n::int, optional, default='6000'\n: Number of top scoring boxes to keep after applying NMS to RPN proposals\n\n\nrpn_post_nms_top_n::int, optional, default='300'\n: Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU \n= this threshold\n\n\nthreshold::float, optional, default=0.7\n: NMS value, below which to suppress.\n\n\nrpn_min_size::int, optional, default='16'\n: Minimum height or width in proposal\n\n\nscales::tuple of \nfloat\n, optional, default=[4,8,16,32]\n: Used to generate anchor windows by enumerating scales\n\n\nratios::tuple of \nfloat\n, optional, default=[0.5,1,2]\n: Used to generate anchor windows by enumerating ratios\n\n\nfeature_stride::int, optional, default='16'\n: The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.\n\n\noutput_score::boolean, optional, default=0\n: Add score to outputs\n\n\niou_loss::boolean, optional, default=0\n: Usage of IoU Loss\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_SparseEmbedding\n \n \nMethod\n.\n\n\n_contrib_SparseEmbedding(data, weight, input_dim, output_dim, dtype)\n\n\n\n\nMaps integer indices to vector representations (embeddings).\n\n\nThis operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.\n\n\nFor an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).\n\n\nIf the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).\n\n\nThe storage type of weight must be \nrow_sparse\n, and the gradient of the weight will be of \nrow_sparse\n storage type, too.\n\n\n.. Note::\n\n\n`SparseEmbedding` is designed for the use case where `input_dim` is very large (e.g. 100k).\nThe operator is available on both CPU and GPU.\n\n\n\n\nExamples::\n\n\ninput_dim = 4   output_dim = 5\n\n\n// Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]\n\n\n// Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]\n\n\n// Mapped input x to its vector representation y.   SparseEmbedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                                  [ 15.,  16.,  17.,  18.,  19.]],\n\n\n                            [[  0.,   1.,   2.,   3.,   4.],\n                             [ 10.,  11.,  12.,  13.,  14.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L254\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array to the embedding operator.\n\n\nweight::NDArray-or-SymbolicNode\n: The embedding weight matrix.\n\n\ninput_dim::int, required\n: Vocabulary size of the input indices.\n\n\noutput_dim::int, required\n: Dimension of the embedding vectors.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Data type of weight.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_bipartite_matching\n \n \nMethod\n.\n\n\n_contrib_bipartite_matching(data, is_ascend, threshold, topk)\n\n\n\n\nCompute bipartite matching.   The matching is performed on score matrix with shape [B, N, M]\n\n\n\n\nB: batch_size\n\n\nN: number of rows to match\n\n\nM: number of columns as reference to be matched against.\n\n\n\n\nReturns:   x : matched column indices. -1 indicating non-matched elements in rows.   y : matched row indices.\n\n\nNote::\n\n\nZero gradients are back-propagated in this op for now.\n\n\n\n\nExample::\n\n\ns = [[0.5, 0.6], [0.1, 0.2], [0.3, 0.4]]\nx, y = bipartite_matching(x, threshold=1e-12, is_ascend=False)\nx = [1, -1, 0]\ny = [2\uff0c 0]\n\n\n\n\nDefined in src/operator/contrib/bounding_box.cc:L169\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nis_ascend::boolean, optional, default=0\n: Use ascend order for scores instead of descending. Please set threshold accordingly.\n\n\nthreshold::float, required\n: Ignore matching when score \n thresh, if is_ascend=false, or ignore score \n thresh, if is_ascend=true.\n\n\ntopk::int, optional, default='-1'\n: Limit the number of matches to topk, set -1 for no limit\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_box_iou\n \n \nMethod\n.\n\n\n_contrib_box_iou(lhs, rhs, format)\n\n\n\n\nBounding box overlap of two arrays.   The overlap is defined as Intersection-over-Union, aka, IOU.\n\n\n\n\nlhs: (a_1, a_2, ..., a_n, 4) array\n\n\nrhs: (b_1, b_2, ..., b_n, 4) array\n\n\noutput: (a_1, a_2, ..., a_n, b_1, b_2, ..., b_n) array\n\n\n\n\nNote::\n\n\nZero gradients are back-propagated in this op for now.\n\n\n\n\nExample::\n\n\nx = [[0.5, 0.5, 1.0, 1.0], [0.0, 0.0, 0.5, 0.5]]\ny = [0.25, 0.25, 0.75, 0.75]\nbox_iou(x, y, format='corner') = [[0.1428], [0.1428]]\n\n\n\n\nDefined in src/operator/contrib/bounding_box.cc:L123\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: The first input\n\n\nrhs::NDArray-or-SymbolicNode\n: The second input\n\n\nformat::{'center', 'corner'},optional, default='corner'\n: The box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_box_nms\n \n \nMethod\n.\n\n\n_contrib_box_nms(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)\n\n\n\n\nApply non-maximum suppression to input.\n\n\nThe output will be sorted in descending order according to \nscore\n. Boxes with overlaps larger than \noverlap_thresh\n and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.\n\n\nDuring back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.\n\n\nInput requirements:\n\n\n\n\nInput tensor have at least 2 dimensions, (n, k), any higher dims will be regarded\n\n\n\n\nas batch, e.g. (a, b, c, d, n, k) == (a\nb\nc*d, n, k)\n\n\n\n\nn is the number of boxes in each batch\n\n\nk is the width of each box item.\n\n\n\n\nBy default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.\n\n\n\n\nid_index\n: optional, use -1 to ignore, useful if \nforce_suppress=False\n, which means\n\n\n\n\nwe will skip highly overlapped boxes if one is \napple\n while the other is \ncar\n.\n\n\n\n\ncoord_start\n: required, default=2, the starting index of the 4 coordinates.\n\n\n\n\nTwo formats are supported:   \ncorner\n: [xmin, ymin, xmax, ymax]   \ncenter\n: [x, y, width, height]\n\n\n\n\nscore_index\n: required, default=1, box score/confidence.\n\n\n\n\nWhen two boxes overlap IOU \n \noverlap_thresh\n, the one with smaller score will be suppressed.\n\n\n\n\nin_format\n and \nout_format\n: default='corner', specify in/out box formats.\n\n\n\n\nExamples::\n\n\nx = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]\n\n\nexe.backward\n\n\nin_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]\n\n\nDefined in src/operator/contrib/bounding_box.cc:L82\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\noverlap_thresh::float, optional, default=0.5\n: Overlapping(IoU) threshold to suppress object with smaller score.\n\n\ntopk::int, optional, default='-1'\n: Apply nms to topk boxes with descending scores, -1 to no restriction.\n\n\ncoord_start::int, optional, default='2'\n: Start index of the consecutive 4 coordinates.\n\n\nscore_index::int, optional, default='1'\n: Index of the scores/confidence of boxes.\n\n\nid_index::int, optional, default='-1'\n: Optional, index of the class categories, -1 to disable.\n\n\nforce_suppress::boolean, optional, default=0\n: Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category\n\n\nin_format::{'center', 'corner'},optional, default='corner'\n: The input box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nout_format::{'center', 'corner'},optional, default='corner'\n: The output box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_box_non_maximum_suppression\n \n \nMethod\n.\n\n\n_contrib_box_non_maximum_suppression(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)\n\n\n\n\n_contrib_box_non_maximum_suppression is an alias of _contrib_box_nms.\n\n\nApply non-maximum suppression to input.\n\n\nThe output will be sorted in descending order according to \nscore\n. Boxes with overlaps larger than \noverlap_thresh\n and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.\n\n\nDuring back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.\n\n\nInput requirements:\n\n\n\n\nInput tensor have at least 2 dimensions, (n, k), any higher dims will be regarded\n\n\n\n\nas batch, e.g. (a, b, c, d, n, k) == (a\nb\nc*d, n, k)\n\n\n\n\nn is the number of boxes in each batch\n\n\nk is the width of each box item.\n\n\n\n\nBy default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.\n\n\n\n\nid_index\n: optional, use -1 to ignore, useful if \nforce_suppress=False\n, which means\n\n\n\n\nwe will skip highly overlapped boxes if one is \napple\n while the other is \ncar\n.\n\n\n\n\ncoord_start\n: required, default=2, the starting index of the 4 coordinates.\n\n\n\n\nTwo formats are supported:   \ncorner\n: [xmin, ymin, xmax, ymax]   \ncenter\n: [x, y, width, height]\n\n\n\n\nscore_index\n: required, default=1, box score/confidence.\n\n\n\n\nWhen two boxes overlap IOU \n \noverlap_thresh\n, the one with smaller score will be suppressed.\n\n\n\n\nin_format\n and \nout_format\n: default='corner', specify in/out box formats.\n\n\n\n\nExamples::\n\n\nx = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]\n\n\nexe.backward\n\n\nin_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]\n\n\nDefined in src/operator/contrib/bounding_box.cc:L82\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\noverlap_thresh::float, optional, default=0.5\n: Overlapping(IoU) threshold to suppress object with smaller score.\n\n\ntopk::int, optional, default='-1'\n: Apply nms to topk boxes with descending scores, -1 to no restriction.\n\n\ncoord_start::int, optional, default='2'\n: Start index of the consecutive 4 coordinates.\n\n\nscore_index::int, optional, default='1'\n: Index of the scores/confidence of boxes.\n\n\nid_index::int, optional, default='-1'\n: Optional, index of the class categories, -1 to disable.\n\n\nforce_suppress::boolean, optional, default=0\n: Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category\n\n\nin_format::{'center', 'corner'},optional, default='corner'\n: The input box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nout_format::{'center', 'corner'},optional, default='corner'\n: The output box encoding type.\n\n\n\n\n\"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_count_sketch\n \n \nMethod\n.\n\n\n_contrib_count_sketch(data, h, s, out_dim, processing_batch_size)\n\n\n\n\nApply CountSketch to input: map a d-dimension data to k-dimension data\"\n\n\n.. note:: \ncount_sketch\n is only available on GPU.\n\n\nAssume input data has shape (N, d), sign hash table s has shape (N, d), index hash table h has shape (N, d) and mapping dimension out_dim = k, each element in s is either +1 or -1, each element in h is random integer from 0 to k-1. Then the operator computs:\n\n\n.. math::    out[h[i]] += data[i] * s[i]\n\n\nExample::\n\n\nout_dim = 5    x = [[1.2, 2.5, 3.4],[3.2, 5.7, 6.6]]    h = [[0, 3, 4]]    s = [[1, -1, 1]]    mx.contrib.ndarray.count_sketch(data=x, h=h, s=s, out_dim = 5) = [[1.2, 0, 0, -2.5, 3.4],                                                                      [3.2, 0, 0, -5.7, 6.6]]\n\n\nDefined in src/operator/contrib/count_sketch.cc:L67\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the CountSketchOp.\n\n\nh::NDArray-or-SymbolicNode\n: The index vector\n\n\ns::NDArray-or-SymbolicNode\n: The sign vector\n\n\nout_dim::int, required\n: The output dimension.\n\n\nprocessing_batch_size::int, optional, default='32'\n: How many sketch vectors to process at one time.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_ctc_loss\n \n \nMethod\n.\n\n\n_contrib_ctc_loss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)\n\n\n\n\n_contrib_ctc_loss is an alias of _contrib_CTCLoss.\n\n\nConnectionist Temporal Classification Loss.\n\n\nThe shapes of the inputs and outputs:\n\n\n\n\ndata\n: \n(sequence_length, batch_size, alphabet_size)\n\n\nlabel\n: \n(batch_size, label_sequence_length)\n\n\nout\n: \n(batch_size)\n\n\n\n\nThe \ndata\n tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When \nblank_label\n is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.\n\n\n$label$ is an index matrix of integers. When \nblank_label\n is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when \nblank_label\n is $\"last\"$, the value \n(alphabet_size-1)\n is reserved for blank label.\n\n\nIf a sequence of labels is shorter than \nlabel_sequence_length\n, use the special padding value at the end of the sequence to conform it to the correct length. The padding value is \n0\n when \nblank_label\n is $\"first\"$, and \n-1\n otherwise.\n\n\nFor example, suppose the vocabulary is \n[a, b, c]\n, and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When \nblank_label\n is $\"first\"$, we can index the labels as \n{'a': 1, 'b': 2, 'c': 3}\n, and we reserve the 0-th channel for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]\n\n\nWhen \nblank_label\n is $\"last\"$, we can index the labels as \n{'a': 0, 'b': 1, 'c': 2}\n, and we reserve the channel index 3 for blank label in data tensor. The resulting \nlabel\n tensor should be padded to be::\n\n\n[[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]\n\n\n$out$ is a list of CTC loss values, one per example in the batch.\n\n\nSee \nConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks\n, A. Graves \net al\n. for more information on the definition and the algorithm.\n\n\nDefined in src/operator/contrib/ctc_loss.cc:L115\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the ctc_loss op.\n\n\nlabel::NDArray-or-SymbolicNode\n: Ground-truth labels for the loss.\n\n\ndata_lengths::NDArray-or-SymbolicNode\n: Lengths of data for each of the samples. Only required when use_data_lengths is true.\n\n\nlabel_lengths::NDArray-or-SymbolicNode\n: Lengths of labels for each of the samples. Only required when use_label_lengths is true.\n\n\nuse_data_lengths::boolean, optional, default=0\n: Whether the data lenghts are decided by \ndata_lengths\n. If false, the lengths are equal to the max sequence length.\n\n\nuse_label_lengths::boolean, optional, default=0\n: Whether the label lenghts are decided by \nlabel_lengths\n, or derived from \npadding_mask\n. If false, the lengths are derived from the first occurrence of the value of \npadding_mask\n. The value of \npadding_mask\n is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See \nblank_label\n.\n\n\nblank_label::{'first', 'last'},optional, default='first'\n: Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_dequantize\n \n \nMethod\n.\n\n\n_contrib_dequantize(input, min_range, max_range, out_type)\n\n\n\n\nDequantize the input tensor into a float tensor. [min_range, max_range] are scalar floats that spcify the range for the output data.\n\n\nEach value of the tensor will undergo the following:\n\n\nout[i] = min_range + (in[i] * (max_range - min_range) / range(INPUT_TYPE))\n\n\nhere \nrange(T) = numeric_limits\nT\n::max() - numeric_limits\nT\n::min()\n\n\nDefined in src/operator/contrib/dequantize.cc:L41\n\n\nArguments\n\n\n\n\ninput::NDArray-or-SymbolicNode\n: A ndarray/symbol of type \nuint8\n\n\nmin_range::NDArray-or-SymbolicNode\n: The minimum scalar value possibly produced for the input\n\n\nmax_range::NDArray-or-SymbolicNode\n: The maximum scalar value possibly produced for the input\n\n\nout_type::{'float32'}, required\n: Output data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_fft\n \n \nMethod\n.\n\n\n_contrib_fft(data, compute_size)\n\n\n\n\nApply 1D FFT to input\"\n\n\n.. note:: \nfft\n is only available on GPU.\n\n\nCurrently accept 2 input data shapes: (N, d) or (N1, N2, N3, d), data can only be real numbers. The output data has shape: (N, 2\nd) or (N1, N2, N3, 2\nd). The format is: [real0, imag0, real1, imag1, ...].\n\n\nExample::\n\n\ndata = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.fft(data = mx.nd.array(data,ctx = mx.gpu(0)))\n\n\nDefined in src/operator/contrib/fft.cc:L56\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the FFTOp.\n\n\ncompute_size::int, optional, default='128'\n: Maximum size of sub-batch to be forwarded at one time\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_ifft\n \n \nMethod\n.\n\n\n_contrib_ifft(data, compute_size)\n\n\n\n\nApply 1D ifft to input\"\n\n\n.. note:: \nifft\n is only available on GPU.\n\n\nCurrently accept 2 input data shapes: (N, d) or (N1, N2, N3, d). Data is in format: [real0, imag0, real1, imag1, ...]. Last dimension must be an even number. The output data has shape: (N, d/2) or (N1, N2, N3, d/2). It is only the real part of the result.\n\n\nExample::\n\n\ndata = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.ifft(data = mx.nd.array(data,ctx = mx.gpu(0)))\n\n\nDefined in src/operator/contrib/ifft.cc:L58\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data to the IFFTOp.\n\n\ncompute_size::int, optional, default='128'\n: Maximum size of sub-batch to be forwarded at one time\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._contrib_quantize\n \n \nMethod\n.\n\n\n_contrib_quantize(input, min_range, max_range, out_type)\n\n\n\n\nQuantize a input tensor from float to \nout_type\n, with user-specified \nmin_range\n and \nmax_range\n.\n\n\n[min_range, max_range] are scalar floats that spcify the range for the input data. Each value of the tensor will undergo the following:\n\n\nout[i] = (in[i] - min_range) * range(OUTPUT_TYPE) / (max_range - min_range)\n\n\nhere \nrange(T) = numeric_limits\nT\n::max() - numeric_limits\nT\n::min()\n\n\nDefined in src/operator/contrib/quantize.cc:L41\n\n\nArguments\n\n\n\n\ninput::NDArray-or-SymbolicNode\n: A ndarray/symbol of type \nfloat32\n\n\nmin_range::NDArray-or-SymbolicNode\n: The minimum scalar value possibly produced for the input\n\n\nmax_range::NDArray-or-SymbolicNode\n: The maximum scalar value possibly produced for the input\n\n\nout_type::{'uint8'},optional, default='uint8'\n: Output data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copy\n \n \nMethod\n.\n\n\n_copy(data)\n\n\n\n\nReturns a copy of the input.\n\n\nFrom:src/operator/tensor/elemwise_unary_op_basic.cc:112\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copyto\n \n \nMethod\n.\n\n\n_copyto(data)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray\n: input data\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign\n \n \nMethod\n.\n\n\n_crop_assign(lhs, rhs, begin, end, step)\n\n\n\n\n_crop_assign is an alias of _slice_assign.\n\n\nAssign the rhs to a cropped subset of lhs.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as lhs.\n\n\nlhs and rhs are of the same data type, and on the same device.\n\n\n\n\nFrom:src/operator/tensor/matrix_op.cc:381\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: Source input\n\n\nrhs::NDArray-or-SymbolicNode\n: value to assign\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._crop_assign_scalar\n \n \nMethod\n.\n\n\n_crop_assign_scalar(data, scalar, begin, end, step)\n\n\n\n\n_crop_assign_scalar is an alias of _slice_assign_scalar.\n\n\n(Assign the scalar to a cropped subset of the input.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as input\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:406\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nscalar::float, optional, default=0\n: The scalar value for assignment.\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvcopyMakeBorder\n \n \nMethod\n.\n\n\n_cvcopyMakeBorder(src, top, bot, left, right, type, value, values)\n\n\n\n\nPad image border with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\ntop::int, required\n: Top margin.\n\n\nbot::int, required\n: Bottom margin.\n\n\nleft::int, required\n: Left margin.\n\n\nright::int, required\n: Right margin.\n\n\ntype::int, optional, default='0'\n: Filling type (default=cv2.BORDER_CONSTANT).\n\n\nvalue::double, optional, default=0\n: (Deprecated! Use $values$ instead.) Fill with single value.\n\n\nvalues::tuple of \ndouble\n, optional, default=[]\n: Fill with value(RGB[A] or gray), up to 4 channels.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimdecode\n \n \nMethod\n.\n\n\n_cvimdecode(buf, flag, to_rgb)\n\n\n\n\nDecode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.\n\n\nArguments\n\n\n\n\nbuf::NDArray\n: Buffer containing binary encoded image\n\n\nflag::int, optional, default='1'\n: Convert decoded image to grayscale (0) or color (1).\n\n\nto_rgb::boolean, optional, default=1\n: Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimread\n \n \nMethod\n.\n\n\n_cvimread(filename, flag, to_rgb)\n\n\n\n\nRead and decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.\n\n\nArguments\n\n\n\n\nfilename::string, required\n: Name of the image file to be loaded.\n\n\nflag::int, optional, default='1'\n: Convert decoded image to grayscale (0) or color (1).\n\n\nto_rgb::boolean, optional, default=1\n: Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._cvimresize\n \n \nMethod\n.\n\n\n_cvimresize(src, w, h, interp)\n\n\n\n\nResize image with OpenCV. \n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source image\n\n\nw::int, required\n: Width of resized image.\n\n\nh::int, required\n: Height of resized image.\n\n\ninterp::int, optional, default='1'\n: Interpolation method (default=cv2.INTER_LINEAR).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._div_scalar\n \n \nMethod\n.\n\n\n_div_scalar(data, scalar)\n\n\n\n\nDivide an array with a scalar.\n\n\n$_div_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal\n \n \nMethod\n.\n\n\n_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._equal_scalar\n \n \nMethod\n.\n\n\n_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._full\n \n \nMethod\n.\n\n\n_full(shape, ctx, dtype, value)\n\n\n\n\nfill target with a scalar value\n\n\nArguments\n\n\n\n\nshape::Shape(tuple), optional, default=[]\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\nvalue::double, required\n: Value with which to fill newly created tensor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._grad_add\n \n \nMethod\n.\n\n\n_grad_add(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater\n \n \nMethod\n.\n\n\n_greater(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal\n \n \nMethod\n.\n\n\n_greater_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_equal_scalar\n \n \nMethod\n.\n\n\n_greater_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._greater_scalar\n \n \nMethod\n.\n\n\n_greater_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot\n \n \nMethod\n.\n\n\n_hypot(lhs, rhs)\n\n\n\n\nGiven the \"legs\" of a right triangle, return its hypotenuse.\n\n\nDefined in src/operator/tensor/elemwise_binary_op_extended.cc:L79\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._hypot_scalar\n \n \nMethod\n.\n\n\n_hypot_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._identity_with_attr_like_rhs\n \n \nMethod\n.\n\n\n_identity_with_attr_like_rhs(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input.\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._imdecode\n \n \nMethod\n.\n\n\n_imdecode(mean, index, x0, y0, x1, y1, c, size)\n\n\n\n\nDecode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer\n\n\nArguments\n\n\n\n\nmean::NDArray-or-SymbolicNode\n: image mean\n\n\nindex::int\n: buffer position for output\n\n\nx0::int\n: x0\n\n\ny0::int\n: y0\n\n\nx1::int\n: x1\n\n\ny1::int\n: y1\n\n\nc::int\n: channel\n\n\nsize::int\n: length of str_img\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser\n \n \nMethod\n.\n\n\n_lesser(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal\n \n \nMethod\n.\n\n\n_lesser_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_equal_scalar\n \n \nMethod\n.\n\n\n_lesser_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._lesser_scalar\n \n \nMethod\n.\n\n\n_lesser_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_gelqf\n \n \nMethod\n.\n\n\n_linalg_gelqf(A)\n\n\n\n\nLQ factorization for general matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, we compute the LQ factorization (LAPACK \ngelqf\n, followed by \norglq\n). \nA\n must have shape \n(x, y)\n with \nx \n= y\n, and must have full rank \n=x\n. The LQ factorization consists of \nL\n with shape \n(x, x)\n and \nQ\n with shape \n(x, y)\n, so that:\n\n\nA\n = \nL\n * \nQ\n\n\nHere, \nL\n is lower triangular (upper triangle equal to zero) with nonzero diagonal, and \nQ\n is row-orthonormal, meaning that\n\n\nQ\n * \nQ\n\\ :sup:\nT\n\n\nis equal to the identity matrix of shape \n(x, x)\n.\n\n\nIf \nn\n2\n, \ngelqf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]\n\n\n// Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L529\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be factorized\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_gemm\n \n \nMethod\n.\n\n\n_linalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)\n\n\n\n\nPerforms general matrix multiplication and accumulation. Input are tensors \nA\n, \nB\n, \nC\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n) + \nbeta\n * \nC\n\n\nHere, \nalpha\n and \nbeta\n are scalar parameters, and \nop()\n is either the identity or matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]\n\n\n// Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L69\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nC::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\nbeta::double, optional, default=1\n: Scalar factor multiplied with C.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_gemm2\n \n \nMethod\n.\n\n\n_linalg_gemm2(A, B, transpose_a, transpose_b, alpha)\n\n\n\n\nPerforms general matrix multiplication. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n)\n\n\nHere \nalpha\n is a scalar parameter and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]\n\n\n// Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L128\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_potrf\n \n \nMethod\n.\n\n\n_linalg_potrf(A)\n\n\n\n\nPerforms Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the Cholesky factor \nL\n of the symmetric, positive definite matrix \nA\n is computed. \nL\n is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:\n\n\nA\n = \nL\n * \nL\n\\ :sup:\nT\n\n\nIf \nn\n2\n, \npotrf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]\n\n\n// Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L178\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be decomposed\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_potri\n \n \nMethod\n.\n\n\n_linalg_potri(A)\n\n\n\n\nPerforms matrix inversion from a Cholesky factorization. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:\n\n\nout\n = \nA\n\\ :sup:\n-T\n * \nA\n\\ :sup:\n-1\n\n\nIn other words, if \nA\n is the Cholesky factor of a symmetric positive definite matrix \nB\n (obtained by \npotrf\n), then\n\n\nout\n = \nB\n\\ :sup:\n-1\n\n\nIf \nn\n2\n, \npotri\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\n.. note:: Use this operator only if you are certain you need the inverse of \nB\n, and           cannot use the Cholesky factor \nA\n (\npotrf\n), together with backsubstitution           (\ntrsm\n). The latter is numerically much safer, and also cheaper.\n\n\nExamples::\n\n\n// Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]\n\n\n// Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L236\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_sumlogdiag\n \n \nMethod\n.\n\n\n_linalg_sumlogdiag(A)\n\n\n\n\nComputes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).\n\n\nIf \nn\n2\n, \nsumlogdiag\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]\n\n\n// Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]\n\n\nDefined in src/operator/tensor/la_op.cc:L405\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of square matrices\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_syevd\n \n \nMethod\n.\n\n\n_linalg_syevd(A)\n\n\n\n\nEigendecomposition for symmetric matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n must be symmetric, of shape \n(x, x)\n. We compute the eigendecomposition, resulting in the orthonormal matrix \nU\n of eigenvectors, shape \n(x, x)\n, and the vector \nL\n of eigenvalues, shape \n(x,)\n, so that:\n\n\nU\n * \nA\n = \ndiag(L)\n * \nU\n\n\nHere:\n\n\nU\n * \nU\n\\ :sup:\nT\n = \nU\n\\ :sup:\nT\n * \nU\n = \nI\n\n\nwhere \nI\n is the identity matrix. Also, \nL(0) \n= L(1) \n= L(2) \n= ...\n (ascending order).\n\n\nIf \nn\n2\n, \nsyevd\n is performed separately on the trailing two dimensions of \nA\n (batch mode). In this case, \nU\n has \nn\n dimensions like \nA\n, and \nL\n has \nn-1\n dimensions.\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\n.. note:: Derivatives for this operator are defined only if \nA\n is such that all its           eigenvalues are distinct, and the eigengaps are not too small. If you need           gradients, do not apply this operator to matrices with multiple eigenvalues.\n\n\nExamples::\n\n\n// Single symmetric eigendecomposition    A = [[1., 2.], [2., 4.]]    U, L = syevd(A)    U = [[0.89442719, -0.4472136],         [0.4472136, 0.89442719]]    L = [0., 5.]\n\n\n// Batch symmetric eigendecomposition    A = [[[1., 2.], [2., 4.]],         [[1., 2.], [2., 5.]]]    U, L = syevd(A)    U = [[[0.89442719, -0.4472136],          [0.4472136, 0.89442719]],         [[0.92387953, -0.38268343],          [0.38268343, 0.92387953]]]    L = [[0., 5.],         [0.17157288, 5.82842712]]\n\n\nDefined in src/operator/tensor/la_op.cc:L598\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be factorized\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_syrk\n \n \nMethod\n.\n\n\n_linalg_syrk(A, transpose, alpha)\n\n\n\n\nMultiplication of matrix with its transpose. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the operator performs the BLAS3 function \nsyrk\n:\n\n\nout\n = \nalpha\n * \nA\n * \nA\n\\ :sup:\nT\n\n\nif \ntranspose=False\n, or\n\n\nout\n = \nalpha\n * \nA\n\\ :sup:\nT\n \\ * \nA\n\n\nif \ntranspose=True\n.\n\n\nIf \nn\n2\n, \nsyrk\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]\n\n\n// Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L461\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transpose of input matrix.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_trmm\n \n \nMethod\n.\n\n\n_linalg_trmm(A, B, transpose, rightside, alpha)\n\n\n\n\nPerforms multiplication with a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrmm\n:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n = \nalpha\n * \nB\n * \nop\n\\ (\nA\n)\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrmm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]\n\n\n// Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L293\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._linalg_trsm\n \n \nMethod\n.\n\n\n_linalg_trsm(A, B, transpose, rightside, alpha)\n\n\n\n\nSolves matrix equation involving a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrsm\n, solving for \nout\n in:\n\n\nop\n\\ (\nA\n) * \nout\n = \nalpha\n * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n * \nop\n\\ (\nA\n) = \nalpha\n * \nB\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrsm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n\n\n// Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L356\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum\n \n \nMethod\n.\n\n\n_maximum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum_scalar\n \n \nMethod\n.\n\n\n_maximum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum\n \n \nMethod\n.\n\n\n_minimum(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum_scalar\n \n \nMethod\n.\n\n\n_minimum_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus\n \n \nMethod\n.\n\n\n_minus(lhs, rhs)\n\n\n\n\n_minus is an alias of elemwise_sub.\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus_scalar\n \n \nMethod\n.\n\n\n_minus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mod\n \n \nMethod\n.\n\n\n_mod(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mod_scalar\n \n \nMethod\n.\n\n\n_mod_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul\n \n \nMethod\n.\n\n\n_mul(lhs, rhs)\n\n\n\n\n_mul is an alias of elemwise_mul.\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul_scalar\n \n \nMethod\n.\n\n\n_mul_scalar(data, scalar)\n\n\n\n\nMultiply an array with a scalar.\n\n\n$_mul_scalar$ only operates on data array of input if input is sparse.\n\n\nFor example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal\n \n \nMethod\n.\n\n\n_not_equal(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._not_equal_scalar\n \n \nMethod\n.\n\n\n_not_equal_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._onehot_encode\n \n \nMethod\n.\n\n\n_onehot_encode(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._ones\n \n \nMethod\n.\n\n\n_ones(shape, ctx, dtype)\n\n\n\n\nfill target with ones\n\n\nArguments\n\n\n\n\nshape::Shape(tuple), optional, default=[]\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus\n \n \nMethod\n.\n\n\n_plus(lhs, rhs)\n\n\n\n\n_plus is an alias of elemwise_add.\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus_scalar\n \n \nMethod\n.\n\n\n_plus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power\n \n \nMethod\n.\n\n\n_power(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power_scalar\n \n \nMethod\n.\n\n\n_power_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_exponential\n \n \nMethod\n.\n\n\n_random_exponential(lam, shape, ctx, dtype)\n\n\n\n\nDraw random samples from an exponential distribution.\n\n\nSamples are distributed according to an exponential distribution parametrized by \nlambda\n (rate).\n\n\nExample::\n\n\nexponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]\n\n\nDefined in src/operator/random/sample_op.cc:L115\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the exponential distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_gamma\n \n \nMethod\n.\n\n\n_random_gamma(alpha, beta, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a gamma distribution.\n\n\nSamples are distributed according to a gamma distribution parametrized by \nalpha\n (shape) and \nbeta\n (scale).\n\n\nExample::\n\n\ngamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]\n\n\nDefined in src/operator/random/sample_op.cc:L100\n\n\nArguments\n\n\n\n\nalpha::float, optional, default=1\n: Alpha parameter (shape) of the gamma distribution.\n\n\nbeta::float, optional, default=1\n: Beta parameter (scale) of the gamma distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_generalized_negative_binomial\n \n \nMethod\n.\n\n\n_random_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a generalized negative binomial distribution.\n\n\nSamples are distributed according to a generalized negative binomial distribution parametrized by \nmu\n (mean) and \nalpha\n (dispersion). \nalpha\n is defined as \n1/k\n where \nk\n is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\ngeneralized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]\n\n\nDefined in src/operator/random/sample_op.cc:L168\n\n\nArguments\n\n\n\n\nmu::float, optional, default=1\n: Mean of the negative binomial distribution.\n\n\nalpha::float, optional, default=1\n: Alpha (dispersion) parameter of the negative binomial distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_negative_binomial\n \n \nMethod\n.\n\n\n_random_negative_binomial(k, p, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a negative binomial distribution.\n\n\nSamples are distributed according to a negative binomial distribution parametrized by \nk\n (limit of unsuccessful experiments) and \np\n (failure probability in each experiment). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\nnegative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]\n\n\nDefined in src/operator/random/sample_op.cc:L149\n\n\nArguments\n\n\n\n\nk::int, optional, default='1'\n: Limit of unsuccessful experiments.\n\n\np::float, optional, default=1\n: Failure probability in each experiment.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_normal\n \n \nMethod\n.\n\n\n_random_normal(loc, scale, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a normal (Gaussian) distribution.\n\n\n.. note:: The existing alias $normal$ is deprecated.\n\n\nSamples are distributed according to a normal distribution parametrized by \nloc\n (mean) and \nscale\n (standard deviation).\n\n\nExample::\n\n\nnormal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]\n\n\nDefined in src/operator/random/sample_op.cc:L85\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_poisson\n \n \nMethod\n.\n\n\n_random_poisson(lam, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a Poisson distribution.\n\n\nSamples are distributed according to a Poisson distribution parametrized by \nlambda\n (rate). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\npoisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]\n\n\nDefined in src/operator/random/sample_op.cc:L132\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the Poisson distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_uniform\n \n \nMethod\n.\n\n\n_random_uniform(low, high, shape, ctx, dtype)\n\n\n\n\nDraw random samples from a uniform distribution.\n\n\n.. note:: The existing alias $uniform$ is deprecated.\n\n\nSamples are uniformly distributed over the half-open interval \n[low, high)\n (includes \nlow\n, but excludes \nhigh\n).\n\n\nExample::\n\n\nuniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]\n\n\nDefined in src/operator/random/sample_op.cc:L66\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: Lower bound of the distribution.\n\n\nhigh::float, optional, default=1\n: Upper bound of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rdiv_scalar\n \n \nMethod\n.\n\n\n_rdiv_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rminus_scalar\n \n \nMethod\n.\n\n\n_rminus_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rmod_scalar\n \n \nMethod\n.\n\n\n_rmod_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rpower_scalar\n \n \nMethod\n.\n\n\n_rpower_scalar(data, scalar)\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_exponential\n \n \nMethod\n.\n\n\n_sample_exponential(lam, shape, dtype)\n\n\n\n\nConcurrent sampling from multiple exponential distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]\n\n\n// Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]\n\n\nDefined in src/operator/random/multisample_op.cc:L284\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_gamma\n \n \nMethod\n.\n\n\n_sample_gamma(alpha, shape, dtype, beta)\n\n\n\n\nConcurrent sampling from multiple gamma distributions with parameters \nalpha\n (shape) and \nbeta\n (scale).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nalpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]\n\n\n// Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]\n\n\n// Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]\n\n\nDefined in src/operator/random/multisample_op.cc:L282\n\n\nArguments\n\n\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (shape) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nbeta::NDArray-or-SymbolicNode\n: Beta (scale) parameters of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_generalized_negative_binomial\n \n \nMethod\n.\n\n\n_sample_generalized_negative_binomial(mu, shape, dtype, alpha)\n\n\n\n\nConcurrent sampling from multiple generalized negative binomial distributions with parameters \nmu\n (mean) and \nalpha\n (dispersion).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nmu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]\n\n\n// Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]\n\n\n// Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L293\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (dispersion) parameters of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_multinomial\n \n \nMethod\n.\n\n\n_sample_multinomial(data, shape, get_prob, dtype)\n\n\n\n\nConcurrent sampling from multiple multinomial distributions.\n\n\ndata\n is an \nn\n dimensional array whose last dimension has length \nk\n, where \nk\n is the number of possible outcomes of each multinomial distribution. This operator will draw \nshape\n samples from each distribution. If shape is empty one sample will be drawn from each distribution.\n\n\nIf \nget_prob\n is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.\n\n\nNote that the input distribution must be normalized, i.e. \ndata\n must sum to 1 along its last axis.\n\n\nExamples::\n\n\nprobs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]\n\n\n// Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]\n\n\n// Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]\n\n\n// requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Distribution probabilities. Must sum to one on the last axis.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\nget_prob::boolean, optional, default=0\n: Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.\n\n\ndtype::{'int32'},optional, default='int32'\n: DType of the output in case this can't be inferred. Only support int32 for now.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_negative_binomial\n \n \nMethod\n.\n\n\n_sample_negative_binomial(k, shape, dtype, p)\n\n\n\n\nConcurrent sampling from multiple negative binomial distributions with parameters \nk\n (failure limit) and \np\n (failure probability).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nk = [ 20, 49 ]    p = [ 0.4 , 0.77 ]\n\n\n// Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]\n\n\n// Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L289\n\n\nArguments\n\n\n\n\nk::NDArray-or-SymbolicNode\n: Limits of unsuccessful experiments.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\np::NDArray-or-SymbolicNode\n: Failure probabilities in each experiment.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_normal\n \n \nMethod\n.\n\n\n_sample_normal(mu, shape, dtype, sigma)\n\n\n\n\nConcurrent sampling from multiple normal distributions with parameters \nmu\n (mean) and \nsigma\n (standard deviation).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nmu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]\n\n\n// Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]\n\n\nDefined in src/operator/random/multisample_op.cc:L279\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nsigma::NDArray-or-SymbolicNode\n: Standard deviations of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_poisson\n \n \nMethod\n.\n\n\n_sample_poisson(lam, shape, dtype)\n\n\n\n\nConcurrent sampling from multiple Poisson distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]\n\n\n// Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L286\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_uniform\n \n \nMethod\n.\n\n\n_sample_uniform(low, shape, dtype, high)\n\n\n\n\nConcurrent sampling from multiple uniform distributions on the intervals given by \n[low,high)\n.\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nlow = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]\n\n\n// Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]\n\n\nDefined in src/operator/random/multisample_op.cc:L277\n\n\nArguments\n\n\n\n\nlow::NDArray-or-SymbolicNode\n: Lower bounds of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nhigh::NDArray-or-SymbolicNode\n: Upper bounds of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_elemwise_div\n \n \nMethod\n.\n\n\n_scatter_elemwise_div(lhs, rhs)\n\n\n\n\nDivides arguments element-wise.  If the left-hand-side input is 'row_sparse', then only the values which exist in the left-hand sparse array are computed.  The 'missing' values are ignored.\n\n\nThe storage type of $_scatter_elemwise_div$ output depends on storage types of inputs\n\n\n\n\n_scatter_elemwise_div(row_sparse, row_sparse) = row_sparse\n\n\n_scatter_elemwise_div(row_sparse, dense) = row_sparse\n\n\n_scatter_elemwise_div(row_sparse, csr) = row_sparse\n\n\notherwise, $_scatter_elemwise_div$ behaves exactly like elemwise_div and generates output\n\n\n\n\nwith default storage\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_minus_scalar\n \n \nMethod\n.\n\n\n_scatter_minus_scalar(data, scalar)\n\n\n\n\nSubtracts a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.\n\n\nThe storage type of $_scatter_minus_scalar$ output depends on storage types of inputs\n\n\n\n\n_scatter_minus_scalar(row_sparse, scalar) = row_sparse\n\n\n_scatter_minus_scalar(csr, scalar) = csr\n\n\notherwise, $_scatter_minus_scalar$ behaves exactly like _minus_scalar and generates output\n\n\n\n\nwith default storage\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_plus_scalar\n \n \nMethod\n.\n\n\n_scatter_plus_scalar(data, scalar)\n\n\n\n\nAdds a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.\n\n\nThe storage type of $_scatter_plus_scalar$ output depends on storage types of inputs\n\n\n\n\n_scatter_plus_scalar(row_sparse, scalar) = row_sparse\n\n\n_scatter_plus_scalar(csr, scalar) = csr\n\n\notherwise, $_scatter_plus_scalar$ behaves exactly like _plus_scalar and generates output\n\n\n\n\nwith default storage\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._scatter_set_nd\n \n \nMethod\n.\n\n\n_scatter_set_nd(data, indices, shape)\n\n\n\n\nThis operator has the same functionality as scatter_nd except that it does not reset the elements not indexed by the input index \nNDArray\n in the input data \nNDArray\n.\n\n\n.. note:: This operator is for internal use only.\n\n\nExamples::\n\n\ndata = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   out = [[1, 1], [1, 1]]   scatter_nd(data=data, indices=indices, out=out)   out = [[0, 1], [2, 3]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: data\n\n\nindices::NDArray-or-SymbolicNode\n: indices\n\n\nshape::Shape(tuple), required\n: Shape of output.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._slice_assign\n \n \nMethod\n.\n\n\n_slice_assign(lhs, rhs, begin, end, step)\n\n\n\n\nAssign the rhs to a cropped subset of lhs.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as lhs.\n\n\nlhs and rhs are of the same data type, and on the same device.\n\n\n\n\nFrom:src/operator/tensor/matrix_op.cc:381\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: Source input\n\n\nrhs::NDArray-or-SymbolicNode\n: value to assign\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._slice_assign_scalar\n \n \nMethod\n.\n\n\n_slice_assign_scalar(data, scalar, begin, end, step)\n\n\n\n\n(Assign the scalar to a cropped subset of the input.\n\n\nRequirements\n\n\n\n\noutput should be explicitly given and be the same as input\n\n\n\n\n)\n\n\nFrom:src/operator/tensor/matrix_op.cc:406\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nscalar::float, optional, default=0\n: The scalar value for assignment.\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_ElementWiseSum\n \n \nMethod\n.\n\n\n_sparse_ElementWiseSum(args)\n\n\n\n\n_sparse_ElementWiseSum is an alias of add_n.\n\n\nNote\n: _sparse_ElementWiseSum takes variable number of positional inputs. So instead of calling as _sparse_ElementWiseSum([x, y, z], num_args=3), one should call via _sparse_ElementWiseSum(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_abs\n \n \nMethod\n.\n\n\n_sparse_abs(data)\n\n\n\n\n_sparse_abs is an alias of abs.\n\n\nReturns element-wise absolute value of the input.\n\n\nExample::\n\n\nabs([-2, 0, 3]) = [2, 0, 3]\n\n\nThe storage type of $abs$ output depends upon the input storage type:\n\n\n\n\nabs(default) = default\n\n\nabs(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L386\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_adam_update\n \n \nMethod\n.\n\n\n_sparse_adam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_adam_update is an alias of adam_update.\n\n\nUpdate function for Adam optimizer. Adam is seen as a generalization of AdaGrad.\n\n\nAdam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).\n\n\n.. math::\n\n\ng_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }\n\n\nIt updates the weights using::\n\n\nm = beta1\nm + (1-beta1)\ngrad  v = beta2\nv + (1-beta2)\n(grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)\n\n\nIf w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::\n\n\nfor row in grad.indices:      m[row] = beta1\nm[row] + (1-beta1)\ngrad[row]      v[row] = beta2\nv[row] + (1-beta2)\n(grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)\n\n\nDefined in src/operator/optimizer_op.cc:L208\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmean::NDArray-or-SymbolicNode\n: Moving mean\n\n\nvar::NDArray-or-SymbolicNode\n: Moving variance\n\n\nlr::float, required\n: Learning rate\n\n\nbeta1::float, optional, default=0.9\n: The decay rate for the 1st moment estimates.\n\n\nbeta2::float, optional, default=0.999\n: The decay rate for the 2nd moment estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_add_n\n \n \nMethod\n.\n\n\n_sparse_add_n(args)\n\n\n\n\n_sparse_add_n is an alias of add_n.\n\n\nNote\n: _sparse_add_n takes variable number of positional inputs. So instead of calling as _sparse_add_n([x, y, z], num_args=3), one should call via _sparse_add_n(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arccos\n \n \nMethod\n.\n\n\n_sparse_arccos(data)\n\n\n\n\n_sparse_arccos is an alias of arccos.\n\n\nReturns element-wise inverse cosine of the input array.\n\n\nThe input should be in range \n[-1, 1]\n. The output is in the closed interval :math:\n[0, \\pi]\n\n\n.. math::    arccos([-1, -.707, 0, .707, 1]) = [\\pi, 3\\pi/4, \\pi/2, \\pi/4, 0]\n\n\nThe storage type of $arccos$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L123\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arccosh\n \n \nMethod\n.\n\n\n_sparse_arccosh(data)\n\n\n\n\n_sparse_arccosh is an alias of arccosh.\n\n\nReturns the element-wise inverse hyperbolic cosine of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arccosh$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L264\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arcsin\n \n \nMethod\n.\n\n\n_sparse_arcsin(data)\n\n\n\n\n_sparse_arcsin is an alias of arcsin.\n\n\nReturns element-wise inverse sine of the input array.\n\n\nThe input should be in the range \n[-1, 1]\n. The output is in the closed interval of [:math:\n-\\pi/2\n, :math:\n\\pi/2\n].\n\n\n.. math::    arcsin([-1, -.707, 0, .707, 1]) = [-\\pi/2, -\\pi/4, 0, \\pi/4, \\pi/2]\n\n\nThe storage type of $arcsin$ output depends upon the input storage type:\n\n\n\n\narcsin(default) = default\n\n\narcsin(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arcsinh\n \n \nMethod\n.\n\n\n_sparse_arcsinh(data)\n\n\n\n\n_sparse_arcsinh is an alias of arcsinh.\n\n\nReturns the element-wise inverse hyperbolic sine of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arcsinh$ output depends upon the input storage type:\n\n\n\n\narcsinh(default) = default\n\n\narcsinh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L250\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arctan\n \n \nMethod\n.\n\n\n_sparse_arctan(data)\n\n\n\n\n_sparse_arctan is an alias of arctan.\n\n\nReturns element-wise inverse tangent of the input array.\n\n\nThe output is in the closed interval :math:\n[-\\pi/2, \\pi/2]\n\n\n.. math::    arctan([-1, 0, 1]) = [-\\pi/4, 0, \\pi/4]\n\n\nThe storage type of $arctan$ output depends upon the input storage type:\n\n\n\n\narctan(default) = default\n\n\narctan(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L144\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_arctanh\n \n \nMethod\n.\n\n\n_sparse_arctanh(data)\n\n\n\n\n_sparse_arctanh is an alias of arctanh.\n\n\nReturns the element-wise inverse hyperbolic tangent of the input array, \ncomputed element-wise.\n\n\nThe storage type of $arctanh$ output depends upon the input storage type:\n\n\n\n\narctanh(default) = default\n\n\narctanh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L281\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_cast_storage\n \n \nMethod\n.\n\n\n_sparse_cast_storage(data, stype)\n\n\n\n\n_sparse_cast_storage is an alias of cast_storage.\n\n\nCasts tensor storage type to the new type.\n\n\nWhen an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:\n\n\n\n\nfor csr, zero values will not be retained\n\n\nfor row_sparse, row slices of all zeros will not be retained\n\n\n\n\nThe storage type of $cast_storage$ output depends on stype parameter:\n\n\n\n\ncast_storage(csr, 'default') = default\n\n\ncast_storage(row_sparse, 'default') = default\n\n\ncast_storage(default, 'csr') = csr\n\n\ncast_storage(default, 'row_sparse') = row_sparse\n\n\n\n\nExample::\n\n\ndense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]\n\n\n\n\nDefined in src/operator/tensor/cast_storage.cc:L69\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\nstype::{'csr', 'default', 'row_sparse'}, required\n: Output storage type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_ceil\n \n \nMethod\n.\n\n\n_sparse_ceil(data)\n\n\n\n\n_sparse_ceil is an alias of ceil.\n\n\nReturns element-wise ceiling of the input.\n\n\nThe ceil of the scalar x is the smallest integer i, such that i \n= x.\n\n\nExample::\n\n\nceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]\n\n\nThe storage type of $ceil$ output depends upon the input storage type:\n\n\n\n\nceil(default) = default\n\n\nceil(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L464\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_clip\n \n \nMethod\n.\n\n\n_sparse_clip(data, a_min, a_max)\n\n\n\n\n_sparse_clip is an alias of clip.\n\n\nClips (limits) the values in an array.\n\n\nGiven an interval, values outside the interval are clipped to the interval edges. Clipping $x$ between \na_min\n and \na_x\n would be::\n\n\nclip(x, a_min, a_max) = max(min(x, a_max), a_min))\n\n\nExample::\n\n\nx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nclip(x,1,8) = [ 1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  8.]\n\n\n\n\nThe storage type of $clip$ output depends on storage types of inputs and the a_min, a_max \nparameter values:\n\n\n\n\nclip(default) = default\n\n\nclip(row_sparse, a_min \n= 0, a_max \n= 0) = row_sparse\n\n\nclip(csr, a_min \n= 0, a_max \n= 0) = csr\n\n\nclip(row_sparse, a_min \n 0, a_max \n 0) = default\n\n\nclip(row_sparse, a_min \n 0, a_max \n 0) = default\n\n\nclip(csr, a_min \n 0, a_max \n 0) = csr\n\n\nclip(csr, a_min \n 0, a_max \n 0) = csr\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L486\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\na_min::float, required\n: Minimum value\n\n\na_max::float, required\n: Maximum value\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_cos\n \n \nMethod\n.\n\n\n_sparse_cos(data)\n\n\n\n\n_sparse_cos is an alias of cos.\n\n\nComputes the element-wise cosine of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    cos([0, \\pi/4, \\pi/2]) = [1, 0.707, 0]\n\n\nThe storage type of $cos$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L63\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_cosh\n \n \nMethod\n.\n\n\n_sparse_cosh(data)\n\n\n\n\n_sparse_cosh is an alias of cosh.\n\n\nReturns the hyperbolic cosine  of the input array, computed element-wise.\n\n\n.. math::    cosh(x) = 0.5\\times(exp(x) + exp(-x))\n\n\nThe storage type of $cosh$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L216\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_degrees\n \n \nMethod\n.\n\n\n_sparse_degrees(data)\n\n\n\n\n_sparse_degrees is an alias of degrees.\n\n\nConverts each element of the input array from radians to degrees.\n\n\n.. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]\n\n\nThe storage type of $degrees$ output depends upon the input storage type:\n\n\n\n\ndegrees(default) = default\n\n\ndegrees(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L163\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_dot\n \n \nMethod\n.\n\n\n_sparse_dot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\n_sparse_dot is an alias of dot.\n\n\nDot product of two arrays.\n\n\n$dot$'s behavior depends on the input array dimensions:\n\n\n\n\n1-D arrays: inner product of vectors\n\n\n2-D arrays: matrix multiplication\n\n\n\n\nN-D arrays: a sum product over the last axis of the first input and the first axis of the second input\n\n\nFor example, given 3-D $x$ with shape \n(n,m,k)\n and $y$ with shape \n(k,r,s)\n, the result array will have shape \n(n,m,r,s)\n. It is computed by::\n\n\ndot(x,y)[i,j,a,b] = sum(x[i,j,:]*y[:,a,b])\n\n\nExample::\n\n\nx = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))   y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))   dot(x,y)[0,0,1,1] = 0   sum(x[0,0,:]*y[:,1,1]) = 0\n\n\n\n\n\n\nThe storage type of $dot$ output depends on storage types of inputs and transpose options:\n\n\n\n\ndot(csr, default) = default\n\n\ndot(csr.T, default) = row_sparse\n\n\ndot(csr, row_sparse) = default\n\n\ndot(default, csr) = csr\n\n\notherwise, $dot$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/dot.cc:L62\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: The first input\n\n\nrhs::NDArray-or-SymbolicNode\n: The second input\n\n\ntranspose_a::boolean, optional, default=0\n: If true then transpose the first input before dot.\n\n\ntranspose_b::boolean, optional, default=0\n: If true then transpose the second input before dot.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_add\n \n \nMethod\n.\n\n\n_sparse_elemwise_add(lhs, rhs)\n\n\n\n\n_sparse_elemwise_add is an alias of elemwise_add.\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_div\n \n \nMethod\n.\n\n\n_sparse_elemwise_div(lhs, rhs)\n\n\n\n\n_sparse_elemwise_div is an alias of elemwise_div.\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_mul\n \n \nMethod\n.\n\n\n_sparse_elemwise_mul(lhs, rhs)\n\n\n\n\n_sparse_elemwise_mul is an alias of elemwise_mul.\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_elemwise_sub\n \n \nMethod\n.\n\n\n_sparse_elemwise_sub(lhs, rhs)\n\n\n\n\n_sparse_elemwise_sub is an alias of elemwise_sub.\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_exp\n \n \nMethod\n.\n\n\n_sparse_exp(data)\n\n\n\n\n_sparse_exp is an alias of exp.\n\n\nReturns element-wise exponential value of the input.\n\n\n.. math::    exp(x) = e^x \\approx 2.718^x\n\n\nExample::\n\n\nexp([0, 1, 2]) = [1., 2.71828175, 7.38905621]\n\n\nThe storage type of $exp$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L642\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_expm1\n \n \nMethod\n.\n\n\n_sparse_expm1(data)\n\n\n\n\n_sparse_expm1 is an alias of expm1.\n\n\nReturns $exp(x) - 1$ computed element-wise on the input.\n\n\nThis function provides greater precision than $exp(x) - 1$ for small values of $x$.\n\n\nThe storage type of $expm1$ output depends upon the input storage type:\n\n\n\n\nexpm1(default) = default\n\n\nexpm1(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L721\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_fix\n \n \nMethod\n.\n\n\n_sparse_fix(data)\n\n\n\n\n_sparse_fix is an alias of fix.\n\n\nReturns element-wise rounded value to the nearest \ninteger towards zero of the input.\n\n\nExample::\n\n\nfix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]\n\n\nThe storage type of $fix$ output depends upon the input storage type:\n\n\n\n\nfix(default) = default\n\n\nfix(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L521\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_floor\n \n \nMethod\n.\n\n\n_sparse_floor(data)\n\n\n\n\n_sparse_floor is an alias of floor.\n\n\nReturns element-wise floor of the input.\n\n\nThe floor of the scalar x is the largest integer i, such that i \n= x.\n\n\nExample::\n\n\nfloor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]\n\n\nThe storage type of $floor$ output depends upon the input storage type:\n\n\n\n\nfloor(default) = default\n\n\nfloor(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L483\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_ftrl_update\n \n \nMethod\n.\n\n\n_sparse_ftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_ftrl_update is an alias of ftrl_update.\n\n\nUpdate function for Ftrl optimizer. Referenced from \nAd Click Prediction: a View from the Trenches\n, available at http://dl.acm.org/citation.cfm?id=2488200.\n\n\nIt updates the weights using::\n\n\nrescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad\n2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad\n2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z) \n lamda1)\n\n\nIf w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::\n\n\nfor row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row]\n2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row]\n2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row]) \n lamda1)\n\n\nDefined in src/operator/optimizer_op.cc:L341\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nz::NDArray-or-SymbolicNode\n: z\n\n\nn::NDArray-or-SymbolicNode\n: Square of grad\n\n\nlr::float, required\n: Learning rate\n\n\nlamda1::float, optional, default=0.01\n: The L1 regularization coefficient.\n\n\nbeta::float, optional, default=1\n: Per-Coordinate Learning Rate beta.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_gamma\n \n \nMethod\n.\n\n\n_sparse_gamma(data)\n\n\n\n\n_sparse_gamma is an alias of gamma.\n\n\nReturns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.\n\n\nThe storage type of $gamma$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_gammaln\n \n \nMethod\n.\n\n\n_sparse_gammaln(data)\n\n\n\n\n_sparse_gammaln is an alias of gammaln.\n\n\nReturns element-wise log of the absolute value of the gamma function \nof the input.\n\n\nThe storage type of $gammaln$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log\n \n \nMethod\n.\n\n\n_sparse_log(data)\n\n\n\n\n_sparse_log is an alias of log.\n\n\nReturns element-wise Natural logarithmic value of the input.\n\n\nThe natural logarithm is logarithm in base \ne\n, so that $log(exp(x)) = x$\n\n\nThe storage type of $log$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L654\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log10\n \n \nMethod\n.\n\n\n_sparse_log10(data)\n\n\n\n\n_sparse_log10 is an alias of log10.\n\n\nReturns element-wise Base-10 logarithmic value of the input.\n\n\n$10**log10(x) = x$\n\n\nThe storage type of $log10$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L666\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log1p\n \n \nMethod\n.\n\n\n_sparse_log1p(data)\n\n\n\n\n_sparse_log1p is an alias of log1p.\n\n\nReturns element-wise $log(1 + x)$ value of the input.\n\n\nThis function is more accurate than $log(1 + x)$  for small $x$ so that :math:\n1+x\\approx 1\n\n\nThe storage type of $log1p$ output depends upon the input storage type:\n\n\n\n\nlog1p(default) = default\n\n\nlog1p(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L703\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_log2\n \n \nMethod\n.\n\n\n_sparse_log2(data)\n\n\n\n\n_sparse_log2 is an alias of log2.\n\n\nReturns element-wise Base-2 logarithmic value of the input.\n\n\n$2**log2(x) = x$\n\n\nThe storage type of $log2$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L678\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_make_loss\n \n \nMethod\n.\n\n\n_sparse_make_loss(data)\n\n\n\n\n_sparse_make_loss is an alias of make_loss.\n\n\nMake your own loss function in network construction.\n\n\nThis operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.\n\n\nFor example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::\n\n\ncross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)\n\n\nWe will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.\n\n\nThe storage type of $make_loss$ output depends upon the input storage type:\n\n\n\n\nmake_loss(default) = default\n\n\nmake_loss(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L200\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_mean\n \n \nMethod\n.\n\n\n_sparse_mean(data, axis, keepdims, exclude)\n\n\n\n\n_sparse_mean is an alias of mean.\n\n\nComputes the mean of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L101\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_negative\n \n \nMethod\n.\n\n\n_sparse_negative(data)\n\n\n\n\n_sparse_negative is an alias of negative.\n\n\nNumerical negative of the argument, element-wise.\n\n\nThe storage type of $negative$ output depends upon the input storage type:\n\n\n\n\nnegative(default) = default\n\n\nnegative(row_sparse) = row_sparse\n\n\nnegative(csr) = csr\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_radians\n \n \nMethod\n.\n\n\n_sparse_radians(data)\n\n\n\n\n_sparse_radians is an alias of radians.\n\n\nConverts each element of the input array from degrees to radians.\n\n\n.. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]\n\n\nThe storage type of $radians$ output depends upon the input storage type:\n\n\n\n\nradians(default) = default\n\n\nradians(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L182\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_relu\n \n \nMethod\n.\n\n\n_sparse_relu(data)\n\n\n\n\n_sparse_relu is an alias of relu.\n\n\nComputes rectified linear.\n\n\n.. math::    max(features, 0)\n\n\nThe storage type of $relu$ output depends upon the input storage type:\n\n\n\n\nrelu(default) = default\n\n\nrelu(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L84\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_retain\n \n \nMethod\n.\n\n\n_sparse_retain(data, indices)\n\n\n\n\npick rows specified by user input index array from a row sparse matrix and save them in the output sparse matrix.\n\n\nExample::\n\n\ndata = [[1, 2], [3, 4], [5, 6]]   indices = [0, 1, 3]   shape = (4, 2)   rsp_in = row_sparse(data, indices)   to_retain = [0, 3]   rsp_out = retain(rsp_in, to_retain)   rsp_out.values = [[1, 2], [5, 6]]   rsp_out.indices = [0, 3]\n\n\nThe storage type of $retain$ output depends on storage types of inputs\n\n\n\n\nretain(row_sparse, default) = row_sparse\n\n\notherwise, $retain$ is not supported\n\n\n\n\nDefined in src/operator/tensor/sparse_retain.cc:L53\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array for sparse_retain operator.\n\n\nindices::NDArray-or-SymbolicNode\n: The index array of rows ids that will be retained.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_rint\n \n \nMethod\n.\n\n\n_sparse_rint(data)\n\n\n\n\n_sparse_rint is an alias of rint.\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\n.. note::\n\n\n\n\nFor input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.\n\n\nFor input $-n.5$ both $rint$ and $round$ returns $-n-1$.\n\n\n\n\nExample::\n\n\nrint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]\n\n\nThe storage type of $rint$ output depends upon the input storage type:\n\n\n\n\nrint(default) = default\n\n\nrint(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L445\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_round\n \n \nMethod\n.\n\n\n_sparse_round(data)\n\n\n\n\n_sparse_round is an alias of round.\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\nExample::\n\n\nround([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]\n\n\nThe storage type of $round$ output depends upon the input storage type:\n\n\n\n\nround(default) = default\n\n\nround(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L424\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_rsqrt\n \n \nMethod\n.\n\n\n_sparse_rsqrt(data)\n\n\n\n\n_sparse_rsqrt is an alias of rsqrt.\n\n\nReturns element-wise inverse square-root value of the input.\n\n\n.. math::    rsqrt(x) = 1/\\sqrt{x}\n\n\nExample::\n\n\nrsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]\n\n\nThe storage type of $rsqrt$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L585\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sgd_mom_update\n \n \nMethod\n.\n\n\n_sparse_sgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_sgd_mom_update is an alias of sgd_mom_update.\n\n\nMomentum update function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nMomentum update has better convergence rates on neural networks. Mathematically it looks like below:\n\n\n.. math::\n\n\nv_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t\n\n\nIt updates the weights using::\n\n\nv = momentum * v - learning_rate * gradient   weight += v\n\n\nWhere the parameter $momentum$ is the decay rate of momentum estimates at each epoch.\n\n\nIf weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::\n\n\nfor row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]\n\n\nDefined in src/operator/optimizer_op.cc:L94\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmom::NDArray-or-SymbolicNode\n: Momentum\n\n\nlr::float, required\n: Learning rate\n\n\nmomentum::float, optional, default=0\n: The decay rate of momentum estimates at each epoch.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sgd_update\n \n \nMethod\n.\n\n\n_sparse_sgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)\n\n\n\n\n_sparse_sgd_update is an alias of sgd_update.\n\n\nUpdate function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nIt updates the weights using::\n\n\nweight = weight - learning_rate * gradient\n\n\nIf weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::\n\n\nfor row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]\n\n\nDefined in src/operator/optimizer_op.cc:L54\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nlr::float, required\n: Learning rate\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sigmoid\n \n \nMethod\n.\n\n\n_sparse_sigmoid(data)\n\n\n\n\n_sparse_sigmoid is an alias of sigmoid.\n\n\nComputes sigmoid of x element-wise.\n\n\n.. math::    y = 1 / (1 + exp(-x))\n\n\nThe storage type of $sigmoid$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L103\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sign\n \n \nMethod\n.\n\n\n_sparse_sign(data)\n\n\n\n\n_sparse_sign is an alias of sign.\n\n\nReturns element-wise sign of the input.\n\n\nExample::\n\n\nsign([-2, 0, 3]) = [-1, 0, 1]\n\n\nThe storage type of $sign$ output depends upon the input storage type:\n\n\n\n\nsign(default) = default\n\n\nsign(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L405\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sin\n \n \nMethod\n.\n\n\n_sparse_sin(data)\n\n\n\n\n_sparse_sin is an alias of sin.\n\n\nComputes the element-wise sine of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    sin([0, \\pi/4, \\pi/2]) = [0, 0.707, 1]\n\n\nThe storage type of $sin$ output depends upon the input storage type:\n\n\n\n\nsin(default) = default\n\n\nsin(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L46\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sinh\n \n \nMethod\n.\n\n\n_sparse_sinh(data)\n\n\n\n\n_sparse_sinh is an alias of sinh.\n\n\nReturns the hyperbolic sine of the input array, computed element-wise.\n\n\n.. math::    sinh(x) = 0.5\\times(exp(x) - exp(-x))\n\n\nThe storage type of $sinh$ output depends upon the input storage type:\n\n\n\n\nsinh(default) = default\n\n\nsinh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L201\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_slice\n \n \nMethod\n.\n\n\n_sparse_slice(data, begin, end, step)\n\n\n\n\n_sparse_slice is an alias of slice.\n\n\nSlices a region of the array.\n\n\n.. note:: $crop$ is deprecated. Use $slice$ instead.\n\n\nThis function returns a sliced array between the indices given by \nbegin\n and \nend\n with the corresponding \nstep\n.\n\n\nFor an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m \n= n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.\n\n\nThe resulting array's \nk\n-th dimension contains elements from the \nk\n-th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).\n\n\nIf the \nk\n-th elements are \nNone\n in the sequence of \nbegin\n, \nend\n, and \nstep\n, the following rule will be used to set default values. If \ns_k\n is \nNone\n, set \ns_k=1\n. If \ns_k \n 0\n, set \nb_k=0\n, \ne_k=d_k\n; else, set \nb_k=d_k-1\n, \ne_k=-1\n.\n\n\nThe storage type of $slice$ output depends on storage types of inputs\n\n\n\n\nslice(csr) = csr\n\n\notherwise, $slice$ generates output with default storage\n\n\n\n\n.. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.\n\n\nExample::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L355\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sqrt\n \n \nMethod\n.\n\n\n_sparse_sqrt(data)\n\n\n\n\n_sparse_sqrt is an alias of sqrt.\n\n\nReturns element-wise square-root value of the input.\n\n\n.. math::    \\textrm{sqrt}(x) = \\sqrt{x}\n\n\nExample::\n\n\nsqrt([4, 9, 16]) = [2, 3, 4]\n\n\nThe storage type of $sqrt$ output depends upon the input storage type:\n\n\n\n\nsqrt(default) = default\n\n\nsqrt(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L565\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_square\n \n \nMethod\n.\n\n\n_sparse_square(data)\n\n\n\n\n_sparse_square is an alias of square.\n\n\nReturns element-wise squared value of the input.\n\n\n.. math::    square(x) = x^2\n\n\nExample::\n\n\nsquare([2, 3, 4]) = [4, 9, 16]\n\n\nThe storage type of $square$ output depends upon the input storage type:\n\n\n\n\nsquare(default) = default\n\n\nsquare(row_sparse) = row_sparse\n\n\nsquare(csr) = csr\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L542\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_stop_gradient\n \n \nMethod\n.\n\n\n_sparse_stop_gradient(data)\n\n\n\n\n_sparse_stop_gradient is an alias of BlockGrad.\n\n\nStops gradient computation.\n\n\nStops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.\n\n\nExample::\n\n\nv1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)\n\n\nexecutor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]\n\n\nexecutor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L167\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_sum\n \n \nMethod\n.\n\n\n_sparse_sum(data, axis, keepdims, exclude)\n\n\n\n\n_sparse_sum is an alias of sum.\n\n\nComputes the sum of array elements over given axes.\n\n\n.. Note::\n\n\nsum\n and \nsum_axis\n are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.\n\n\nExample::\n\n\ndata = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]\n\n\nsum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]\n\n\nsum(data, axis=[1,2])   [ 12.  19.  27.]\n\n\ndata = [[1,2,0],           [3,0,1],           [4,1,0]]\n\n\ncsr = cast_storage(data, 'csr')\n\n\nsum(csr, axis=0)   [ 8.  3.  1.]\n\n\nsum(csr, axis=1)   [ 3.  4.  5.]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L85\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_tan\n \n \nMethod\n.\n\n\n_sparse_tan(data)\n\n\n\n\n_sparse_tan is an alias of tan.\n\n\nComputes the element-wise tangent of the input array.\n\n\nThe input should be in radians (:math:\n2\\pi\n rad equals 360 degrees).\n\n\n.. math::    tan([0, \\pi/4, \\pi/2]) = [0, 1, -inf]\n\n\nThe storage type of $tan$ output depends upon the input storage type:\n\n\n\n\ntan(default) = default\n\n\ntan(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L83\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_tanh\n \n \nMethod\n.\n\n\n_sparse_tanh(data)\n\n\n\n\n_sparse_tanh is an alias of tanh.\n\n\nReturns the hyperbolic tangent of the input array, computed element-wise.\n\n\n.. math::    tanh(x) = sinh(x) / cosh(x)\n\n\nThe storage type of $tanh$ output depends upon the input storage type:\n\n\n\n\ntanh(default) = default\n\n\ntanh(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L234\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_trunc\n \n \nMethod\n.\n\n\n_sparse_trunc(data)\n\n\n\n\n_sparse_trunc is an alias of trunc.\n\n\nReturn the element-wise truncated value of the input.\n\n\nThe truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.\n\n\nExample::\n\n\ntrunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]\n\n\nThe storage type of $trunc$ output depends upon the input storage type:\n\n\n\n\ntrunc(default) = default\n\n\ntrunc(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L503\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sparse_zeros_like\n \n \nMethod\n.\n\n\n_sparse_zeros_like(data)\n\n\n\n\n_sparse_zeros_like is an alias of zeros_like.\n\n\nReturn an array of zeros with the same shape and type as the input array.\n\n\nThe storage type of $zeros_like$ output depends on the storage type of the input\n\n\n\n\nzeros_like(row_sparse) = row_sparse\n\n\nzeros_like(csr) = csr\n\n\nzeros_like(default) = default\n\n\n\n\nExamples::\n\n\nx = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]\n\n\nzeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._square_sum\n \n \nMethod\n.\n\n\n_square_sum(data, axis, keepdims, exclude)\n\n\n\n\nComputes the square sum of array elements over a given axis for row-sparse matrix. This is a temporary solution for fusing ops square and sum together for row-sparse matrix to save memory for storing gradients. It will become deprecated once the functionality of fusing operators is finished in the future.\n\n\nExample::\n\n\ndns = mx.nd.array([[0, 0], [1, 2], [0, 0], [3, 4], [0, 0]])   rsp = dns.tostype('row_sparse')   sum = mx.nd._internal._square_sum(rsp, axis=1)   sum = [0, 5, 0, 25, 0]\n\n\nDefined in src/operator/tensor/square_sum.cc:L63\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._zeros\n \n \nMethod\n.\n\n\n_zeros(shape, ctx, dtype)\n\n\n\n\nfill target with zeros\n\n\nArguments\n\n\n\n\nshape::Shape(tuple), optional, default=[]\n: The shape of the output\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n.Only used for imperative calls.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.adam_update\n \n \nMethod\n.\n\n\nadam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdate function for Adam optimizer. Adam is seen as a generalization of AdaGrad.\n\n\nAdam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).\n\n\n.. math::\n\n\ng_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }\n\n\nIt updates the weights using::\n\n\nm = beta1\nm + (1-beta1)\ngrad  v = beta2\nv + (1-beta2)\n(grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)\n\n\nIf w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::\n\n\nfor row in grad.indices:      m[row] = beta1\nm[row] + (1-beta1)\ngrad[row]      v[row] = beta2\nv[row] + (1-beta2)\n(grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)\n\n\nDefined in src/operator/optimizer_op.cc:L208\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmean::NDArray-or-SymbolicNode\n: Moving mean\n\n\nvar::NDArray-or-SymbolicNode\n: Moving variance\n\n\nlr::float, required\n: Learning rate\n\n\nbeta1::float, optional, default=0.9\n: The decay rate for the 1st moment estimates.\n\n\nbeta2::float, optional, default=0.999\n: The decay rate for the 2nd moment estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.add_n\n \n \nMethod\n.\n\n\nadd_n(args)\n\n\n\n\nNote\n: add_n takes variable number of positional inputs. So instead of calling as add_n([x, y, z], num_args=3), one should call via add_n(x, y, z), and num_args will be determined automatically.\n\n\nAdds all input arguments element-wise.\n\n\n.. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n\n\n$add_n$ is potentially more efficient than calling $add$ by \nn\n times.\n\n\nThe storage type of $add_n$ output depends on storage types of inputs\n\n\n\n\nadd_n(row_sparse, row_sparse, ..) = row_sparse\n\n\notherwise, $add_n$ generates output with default storage\n\n\n\n\nDefined in src/operator/tensor/elemwise_sum.cc:L123\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input arguments\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax\n \n \nMethod\n.\n\n\nargmax(data, axis, keepdims)\n\n\n\n\nReturns indices of the maximum values along an axis.\n\n\nIn the case of multiple occurrences of maximum values, the indices corresponding to the first occurrence are returned.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]\n\n\n// argmax along axis 0   argmax(x, axis=0) = [ 1.,  1.,  1.]\n\n\n// argmax along axis 1   argmax(x, axis=1) = [ 2.,  2.]\n\n\n// argmax along axis 1 keeping same dims as an input array   argmax(x, axis=1, keepdims=True) = [[ 2.],                                       [ 2.]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L52\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::int or None, optional, default='None'\n: The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$\n\n\nkeepdims::boolean, optional, default=0\n: If this is set to \nTrue\n, the reduced axis is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax_channel\n \n \nMethod\n.\n\n\nargmax_channel(data)\n\n\n\n\nReturns argmax indices of each channel from the input array.\n\n\nThe result will be an NDArray of shape (num_channel,).\n\n\nIn case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]\n\n\nargmax_channel(x) = [ 2.,  2.]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L97\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmin\n \n \nMethod\n.\n\n\nargmin(data, axis, keepdims)\n\n\n\n\nReturns indices of the minimum values along an axis.\n\n\nIn the case of multiple occurrences of minimum values, the indices corresponding to the first occurrence are returned.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]\n\n\n// argmin along axis 0   argmin(x, axis=0) = [ 0.,  0.,  0.]\n\n\n// argmin along axis 1   argmin(x, axis=1) = [ 0.,  0.]\n\n\n// argmin along axis 1 keeping same dims as an input array   argmin(x, axis=1, keepdims=True) = [[ 0.],                                       [ 0.]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L77\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::int or None, optional, default='None'\n: The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$\n\n\nkeepdims::boolean, optional, default=0\n: If this is set to \nTrue\n, the reduced axis is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argsort\n \n \nMethod\n.\n\n\nargsort(data, axis, is_ascend)\n\n\n\n\nReturns the indices that would sort an input array along the given axis.\n\n\nThis function performs sorting along the given axis and returns an array of indices having same shape as an input array that index data in sorted order.\n\n\nExamples::\n\n\nx = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]\n\n\n// sort along axis -1   argsort(x) = [[ 1.,  0.,  2.],                 [ 0.,  2.,  1.]]\n\n\n// sort along axis 0   argsort(x, axis=0) = [[ 1.,  0.,  1.]                         [ 0.,  1.,  0.]]\n\n\n// flatten and then sort   argsort(x) = [ 3.,  1.,  5.,  0.,  4.,  2.]\n\n\nDefined in src/operator/tensor/ordering_op.cc:L176\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.\n\n\nis_ascend::boolean, optional, default=1\n: Whether to sort in ascending or descending order.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_dot\n \n \nMethod\n.\n\n\nbatch_dot(lhs, rhs, transpose_a, transpose_b)\n\n\n\n\nBatchwise dot product.\n\n\n$batch_dot$ is used to compute dot product of $x$ and $y$ when $x$ and $y$ are data in batch, namely 3D arrays in shape of \n(batch_size, :, :)\n.\n\n\nFor example, given $x$ with shape \n(batch_size, n, m)\n and $y$ with shape \n(batch_size, m, k)\n, the result array will have shape \n(batch_size, n, k)\n, which is computed by::\n\n\nbatch_dot(x,y)[i,:,:] = dot(x[i,:,:], y[i,:,:])\n\n\nDefined in src/operator/tensor/dot.cc:L110\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: The first input\n\n\nrhs::NDArray-or-SymbolicNode\n: The second input\n\n\ntranspose_a::boolean, optional, default=0\n: If true then transpose the first input before dot.\n\n\ntranspose_b::boolean, optional, default=0\n: If true then transpose the second input before dot.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_take\n \n \nMethod\n.\n\n\nbatch_take(a, indices)\n\n\n\n\nTakes elements from a data batch.\n\n\n.. note::   \nbatch_take\n is deprecated. Use \npick\n instead.\n\n\nGiven an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::\n\n\noutput[i] = input[i, indices[i]]\n\n\nExamples::\n\n\nx = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]\n\n\n// takes elements with specified indices   batch_take(x, [0,1,0]) = [ 1.  4.  5.]\n\n\nDefined in src/operator/tensor/indexing_op.cc:L382\n\n\nArguments\n\n\n\n\na::NDArray-or-SymbolicNode\n: The input array\n\n\nindices::NDArray-or-SymbolicNode\n: The index array\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_add\n \n \nMethod\n.\n\n\nbroadcast_add(lhs, rhs)\n\n\n\n\nReturns element-wise sum of the input arrays with broadcasting.\n\n\nbroadcast_plus\n is an alias to the function \nbroadcast_add\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]\n\n\nbroadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axes\n \n \nMethod\n.\n\n\nbroadcast_axes(data, axis, size)\n\n\n\n\nbroadcast_axes is an alias of broadcast_axis.\n\n\nBroadcasts the input array over particular axes.\n\n\nBroadcasting is allowed on axes with size 1, such as from \n(2,1,3,1)\n to \n(2,8,3,9)\n. Elements will be duplicated on the broadcasted axes.\n\n\nExample::\n\n\n// given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]\n\n\n// broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L207\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::Shape(tuple), optional, default=[]\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=[]\n: Target sizes of the broadcasting axes.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axis\n \n \nMethod\n.\n\n\nbroadcast_axis(data, axis, size)\n\n\n\n\nBroadcasts the input array over particular axes.\n\n\nBroadcasting is allowed on axes with size 1, such as from \n(2,1,3,1)\n to \n(2,8,3,9)\n. Elements will be duplicated on the broadcasted axes.\n\n\nExample::\n\n\n// given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]\n\n\n// broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L207\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\naxis::Shape(tuple), optional, default=[]\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=[]\n: Target sizes of the broadcasting axes.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_div\n \n \nMethod\n.\n\n\nbroadcast_div(lhs, rhs)\n\n\n\n\nReturns element-wise division of the input arrays with broadcasting.\n\n\nExample::\n\n\nx = [[ 6.,  6.,  6.],         [ 6.,  6.,  6.]]\n\n\ny = [[ 2.],         [ 3.]]\n\n\nbroadcast_div(x, y) = [[ 3.,  3.,  3.],                           [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L157\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_equal\n \n \nMethod\n.\n\n\nbroadcast_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nequal to\n (==) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_equal(x, y) = [[ 0.,  0.,  0.],                             [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L46\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater\n \n \nMethod\n.\n\n\nbroadcast_greater(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \ngreater than\n (\n) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_greater(x, y) = [[ 1.,  1.,  1.],                               [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L82\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_greater_equal\n \n \nMethod\n.\n\n\nbroadcast_greater_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \ngreater than or equal to\n (\n=) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_greater_equal(x, y) = [[ 1.,  1.,  1.],                                     [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L100\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_hypot\n \n \nMethod\n.\n\n\nbroadcast_hypot(lhs, rhs)\n\n\n\n\nReturns the hypotenuse of a right angled triangle, given its \"legs\" with broadcasting.\n\n\nIt is equivalent to doing :math:\nsqrt(x_1^2 + x_2^2)\n.\n\n\nExample::\n\n\nx = [[ 3.,  3.,  3.]]\n\n\ny = [[ 4.],         [ 4.]]\n\n\nbroadcast_hypot(x, y) = [[ 5.,  5.,  5.],                             [ 5.,  5.,  5.]]\n\n\nz = [[ 0.],         [ 4.]]\n\n\nbroadcast_hypot(x, z) = [[ 3.,  3.,  3.],                             [ 5.,  5.,  5.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L156\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser\n \n \nMethod\n.\n\n\nbroadcast_lesser(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nlesser than\n (\n) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_lesser(x, y) = [[ 0.,  0.,  0.],                              [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L118\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_lesser_equal\n \n \nMethod\n.\n\n\nbroadcast_lesser_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nlesser than or equal to\n (\n=) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_lesser_equal(x, y) = [[ 0.,  0.,  0.],                                    [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L136\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_maximum\n \n \nMethod\n.\n\n\nbroadcast_maximum(lhs, rhs)\n\n\n\n\nReturns element-wise maximum of the input arrays with broadcasting.\n\n\nThis function compares two input arrays and returns a new array having the element-wise maxima.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_maximum(x, y) = [[ 1.,  1.,  1.],                               [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L80\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minimum\n \n \nMethod\n.\n\n\nbroadcast_minimum(lhs, rhs)\n\n\n\n\nReturns element-wise minimum of the input arrays with broadcasting.\n\n\nThis function compares two input arrays and returns a new array having the element-wise minima.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_maximum(x, y) = [[ 0.,  0.,  0.],                               [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L115\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minus\n \n \nMethod\n.\n\n\nbroadcast_minus(lhs, rhs)\n\n\n\n\nbroadcast_minus is an alias of broadcast_sub.\n\n\nReturns element-wise difference of the input arrays with broadcasting.\n\n\nbroadcast_minus\n is an alias to the function \nbroadcast_sub\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]\n\n\nbroadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mod\n \n \nMethod\n.\n\n\nbroadcast_mod(lhs, rhs)\n\n\n\n\nReturns element-wise modulo of the input arrays with broadcasting.\n\n\nExample::\n\n\nx = [[ 8.,  8.,  8.],         [ 8.,  8.,  8.]]\n\n\ny = [[ 2.],         [ 3.]]\n\n\nbroadcast_mod(x, y) = [[ 0.,  0.,  0.],                           [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L190\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mul\n \n \nMethod\n.\n\n\nbroadcast_mul(lhs, rhs)\n\n\n\n\nReturns element-wise product of the input arrays with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_mul(x, y) = [[ 0.,  0.,  0.],                           [ 1.,  1.,  1.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L123\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_not_equal\n \n \nMethod\n.\n\n\nbroadcast_not_equal(lhs, rhs)\n\n\n\n\nReturns the result of element-wise \nnot equal to\n (!=) comparison operation with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_not_equal(x, y) = [[ 1.,  1.,  1.],                                 [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L64\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_plus\n \n \nMethod\n.\n\n\nbroadcast_plus(lhs, rhs)\n\n\n\n\nbroadcast_plus is an alias of broadcast_add.\n\n\nReturns element-wise sum of the input arrays with broadcasting.\n\n\nbroadcast_plus\n is an alias to the function \nbroadcast_add\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]\n\n\nbroadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_power\n \n \nMethod\n.\n\n\nbroadcast_power(lhs, rhs)\n\n\n\n\nReturns result of first array elements raised to powers from second array, element-wise with broadcasting.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_power(x, y) = [[ 2.,  2.,  2.],                             [ 4.,  4.,  4.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L45\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_sub\n \n \nMethod\n.\n\n\nbroadcast_sub(lhs, rhs)\n\n\n\n\nReturns element-wise difference of the input arrays with broadcasting.\n\n\nbroadcast_minus\n is an alias to the function \nbroadcast_sub\n.\n\n\nExample::\n\n\nx = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]\n\n\ny = [[ 0.],         [ 1.]]\n\n\nbroadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]\n\n\nbroadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]\n\n\nDefined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input to the function\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_to\n \n \nMethod\n.\n\n\nbroadcast_to(data, shape)\n\n\n\n\nBroadcasts the input array to a new shape.\n\n\nBroadcasting is a mechanism that allows NDArrays to perform arithmetic operations with arrays of different shapes efficiently without creating multiple copies of arrays. Also see, \nBroadcasting \nhttps://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n_ for more explanation.\n\n\nBroadcasting is allowed on axes with size 1, such as from \n(2,1,3,1)\n to \n(2,8,3,9)\n. Elements will be duplicated on the broadcasted axes.\n\n\nFor example::\n\n\nbroadcast_to([[1,2,3]], shape=(2,3)) = [[ 1.,  2.,  3.],                                            [ 1.,  2.,  3.]])\n\n\nThe dimension which you do not want to change can also be kept as \n0\n which means copy the original value. So with \nshape=(2,0)\n, we will obtain the same result as in the above example.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L231\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nshape::Shape(tuple), optional, default=[]\n: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g \nA = broadcast_to(B, shape=(10, 0, 0))\n has the same meaning as \nA = broadcast_axis(B, axis=0, size=10)\n.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cast\n \n \nMethod\n.\n\n\ncast(data, dtype)\n\n\n\n\ncast is an alias of Cast.\n\n\nCasts all elements of the input to a new type.\n\n\n.. note:: $Cast$ is deprecated. Use $cast$ instead.\n\n\nExample::\n\n\ncast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L311\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Output data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cast_storage\n \n \nMethod\n.\n\n\ncast_storage(data, stype)\n\n\n\n\nCasts tensor storage type to the new type.\n\n\nWhen an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:\n\n\n\n\nfor csr, zero values will not be retained\n\n\nfor row_sparse, row slices of all zeros will not be retained\n\n\n\n\nThe storage type of $cast_storage$ output depends on stype parameter:\n\n\n\n\ncast_storage(csr, 'default') = default\n\n\ncast_storage(row_sparse, 'default') = default\n\n\ncast_storage(default, 'csr') = csr\n\n\ncast_storage(default, 'row_sparse') = row_sparse\n\n\n\n\nExample::\n\n\ndense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]\n\n\n\n\nDefined in src/operator/tensor/cast_storage.cc:L69\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input.\n\n\nstype::{'csr', 'default', 'row_sparse'}, required\n: Output storage type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.choose_element_0index\n \n \nMethod\n.\n\n\nchoose_element_0index(lhs, rhs)\n\n\n\n\nChoose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.concat\n \n \nMethod\n.\n\n\nconcat(data, num_args, dim)\n\n\n\n\nconcat is an alias of Concat.\n\n\nNote\n: concat takes variable number of positional inputs. So instead of calling as concat([x, y, z], num_args=3), one should call via concat(x, y, z), and num_args will be determined automatically.\n\n\nJoins input arrays along a given axis.\n\n\n.. note:: \nConcat\n is deprecated. Use \nconcat\n instead.\n\n\nThe dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.\n\n\nExample::\n\n\nx = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]\n\n\nconcat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]\n\n\nNote that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.\n\n\nconcat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]\n\n\nDefined in src/operator/concat.cc:L104\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: List of arrays to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.crop\n \n \nMethod\n.\n\n\ncrop(data, begin, end, step)\n\n\n\n\ncrop is an alias of slice.\n\n\nSlices a region of the array.\n\n\n.. note:: $crop$ is deprecated. Use $slice$ instead.\n\n\nThis function returns a sliced array between the indices given by \nbegin\n and \nend\n with the corresponding \nstep\n.\n\n\nFor an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m \n= n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.\n\n\nThe resulting array's \nk\n-th dimension contains elements from the \nk\n-th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).\n\n\nIf the \nk\n-th elements are \nNone\n in the sequence of \nbegin\n, \nend\n, and \nstep\n, the following rule will be used to set default values. If \ns_k\n is \nNone\n, set \ns_k=1\n. If \ns_k \n 0\n, set \nb_k=0\n, \ne_k=d_k\n; else, set \nb_k=d_k-1\n, \ne_k=-1\n.\n\n\nThe storage type of $slice$ output depends on storage types of inputs\n\n\n\n\nslice(csr) = csr\n\n\notherwise, $slice$ generates output with default storage\n\n\n\n\n.. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.\n\n\nExample::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L355\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.degrees\n \n \nMethod\n.\n\n\ndegrees(data)\n\n\n\n\nConverts each element of the input array from radians to degrees.\n\n\n.. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]\n\n\nThe storage type of $degrees$ output depends upon the input storage type:\n\n\n\n\ndegrees(default) = default\n\n\ndegrees(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L163\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_add\n \n \nMethod\n.\n\n\nelemwise_add(lhs, rhs)\n\n\n\n\nAdds arguments element-wise.\n\n\nThe storage type of $elemwise_add$ output depends on storage types of inputs\n\n\n\n\nelemwise_add(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_add(csr, csr) = csr\n\n\notherwise, $elemwise_add$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_div\n \n \nMethod\n.\n\n\nelemwise_div(lhs, rhs)\n\n\n\n\nDivides arguments element-wise.\n\n\nThe storage type of $elemwise_div$ output is always dense\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_mul\n \n \nMethod\n.\n\n\nelemwise_mul(lhs, rhs)\n\n\n\n\nMultiplies arguments element-wise.\n\n\nThe storage type of $elemwise_mul$ output depends on storage types of inputs\n\n\n\n\nelemwise_mul(default, default) = default\n\n\nelemwise_mul(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_mul(default, row_sparse) = default\n\n\nelemwise_mul(row_sparse, default) = default\n\n\nelemwise_mul(csr, csr) = csr\n\n\notherwise, $elemwise_mul$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.elemwise_sub\n \n \nMethod\n.\n\n\nelemwise_sub(lhs, rhs)\n\n\n\n\nSubtracts arguments element-wise.\n\n\nThe storage type of $elemwise_sub$ output depends on storage types of inputs\n\n\n\n\nelemwise_sub(row_sparse, row_sparse) = row_sparse\n\n\nelemwise_sub(csr, csr) = csr\n\n\notherwise, $elemwise_sub$ generates output with default storage\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: first input\n\n\nrhs::NDArray-or-SymbolicNode\n: second input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fill_element_0index\n \n \nMethod\n.\n\n\nfill_element_0index(lhs, mhs, rhs)\n\n\n\n\nFill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nmhs::NDArray\n: Middle operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fix\n \n \nMethod\n.\n\n\nfix(data)\n\n\n\n\nReturns element-wise rounded value to the nearest \ninteger towards zero of the input.\n\n\nExample::\n\n\nfix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]\n\n\nThe storage type of $fix$ output depends upon the input storage type:\n\n\n\n\nfix(default) = default\n\n\nfix(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L521\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flatten\n \n \nMethod\n.\n\n\nflatten(data)\n\n\n\n\nflatten is an alias of Flatten.\n\n\nFlattens the input array into a 2-D array by collapsing the higher dimensions.\n\n\n.. note:: \nFlatten\n is deprecated. Use \nflatten\n instead.\n\n\nFor an input array with shape $(d1, d2, ..., dk)$, \nflatten\n operation reshapes the input array into an output array of shape $(d1, d2\n...\ndk)$.\n\n\nExample::\n\n\nx = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L208\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flip\n \n \nMethod\n.\n\n\nflip(data, axis)\n\n\n\n\nflip is an alias of reverse.\n\n\nReverses the order of elements along given axis while preserving array shape.\n\n\nNote: reverse and flip are equivalent. We use reverse in the following examples.\n\n\nExamples::\n\n\nx = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]\n\n\nreverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]\n\n\nreverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L662\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\naxis::Shape(tuple), required\n: The axis which to reverse elements.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.from_json\n \n \nMethod\n.\n\n\nfrom_json(repr :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a \nSymbolicNode\n from a JSON string representation.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ftml_update\n \n \nMethod\n.\n\n\nftml_update(weight, grad, d, v, z, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)\n\n\n\n\nThe FTML optimizer described in \nFTML - Follow the Moving Leader in Deep Learning\n, available at http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf.\n\n\n.. math::\n\n\ng_t = \\nabla J(W_{t-1})\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n d_t = \\frac{ (1 - \\beta_1^t) }{ \\eta_t } (\\sqrt{ \\frac{ v_t }{ 1 - \\beta_2^t } } + \\epsilon)  \\sigma_t = d_t - \\beta_1 d_{t-1}  z_t = \\beta_1 z_{ t-1 } + (1 - \\beta_1^t) g_t - \\sigma_t W_{t-1}  W_t = - \\frac{ z_t }{ d_t }\n\n\nDefined in src/operator/optimizer_op.cc:L161\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nd::NDArray-or-SymbolicNode\n: Internal state $d_t$\n\n\nv::NDArray-or-SymbolicNode\n: Internal state $v_t$\n\n\nz::NDArray-or-SymbolicNode\n: Internal state $z_t$\n\n\nlr::float, required\n: Learning rate\n\n\nbeta1::float, optional, default=0.9\n: The decay rate for the 1st moment estimates.\n\n\nbeta2::float, optional, default=0.999\n: The decay rate for the 2nd moment estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ftrl_update\n \n \nMethod\n.\n\n\nftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdate function for Ftrl optimizer. Referenced from \nAd Click Prediction: a View from the Trenches\n, available at http://dl.acm.org/citation.cfm?id=2488200.\n\n\nIt updates the weights using::\n\n\nrescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad\n2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad\n2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z) \n lamda1)\n\n\nIf w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::\n\n\nfor row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row]\n2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row]\n2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row]) \n lamda1)\n\n\nDefined in src/operator/optimizer_op.cc:L341\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nz::NDArray-or-SymbolicNode\n: z\n\n\nn::NDArray-or-SymbolicNode\n: Square of grad\n\n\nlr::float, required\n: Learning rate\n\n\nlamda1::float, optional, default=0.01\n: The L1 regularization coefficient.\n\n\nbeta::float, optional, default=1\n: Per-Coordinate Learning Rate beta.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gammaln\n \n \nMethod\n.\n\n\ngammaln(data)\n\n\n\n\nReturns element-wise log of the absolute value of the gamma function \nof the input.\n\n\nThe storage type of $gammaln$ output is always dense\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gather_nd\n \n \nMethod\n.\n\n\ngather_nd(data, indices)\n\n\n\n\nGather elements or slices from \ndata\n and store to a tensor whose shape is defined by \nindices\n. \ngather_nd\n and \nscatter_nd\n are inverse functions to each other.\n\n\nGiven \ndata\n with shape \n(X_0, X_1, ..., X_{N-1})\n and indices with shape \n(M, Y_0, ..., Y_{K-1})\n, the output will have shape \n(Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1})\n, where \nM \n= N\n. If \nM == N\n, output shape will simply be \n(Y_0, ..., Y_{K-1})\n.\n\n\nThe elements in output is defined as follows::\n\n\noutput[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}] = data[indices[0, y_0, ..., y_{K-1}],                                                       ...,                                                       indices[M-1, y_0, ..., y_{K-1}],                                                       x_M, ..., x_{N-1}]\n\n\nExamples::\n\n\ndata = [[0, 1], [2, 3]]   indices = [[1, 1, 0], [0, 1, 0]]   gather_nd(data, indices) = [2, 3, 0]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: data\n\n\nindices::NDArray-or-SymbolicNode\n: indices\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_attr\n \n \nMethod\n.\n\n\nget_attr(self :: SymbolicNode, key :: Symbol)\n\n\n\n\nGet attribute attached to this \nSymbolicNode\n belonging to key.\n\n\nReturns the value belonging to key as a \nNullable\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_children\n \n \nMethod\n.\n\n\nget_children(x::SymbolicNode)\n\n\n\n\nGets a new grouped \nSymbolicNode\n whose output contains inputs to output nodes of the original symbol.\n\n\njulia\n x = mx.Variable(:x)\nMXNet.mx.SymbolicNode x\n\njulia\n y = mx.Variable(:y)\nMXNet.mx.SymbolicNode y\n\njulia\n z = x + y\nMXNet.mx.SymbolicNode _plus1\n\njulia\n a |\n mx.get_children |\n mx.list_outputs\n2-element Array{Symbol,1}:\n :x\n :y\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_internals\n \n \nMethod\n.\n\n\nget_internals(self :: SymbolicNode)\n\n\n\n\nGet a new grouped \nSymbolicNode\n whose output contains all the internal outputs of this \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_name\n \n \nMethod\n.\n\n\nget_name(self :: SymbolicNode)\n\n\n\n\nGet the name of the symbol.\n\n\njulia\n x = mx.Variable(:data)\njulia\n mx.get_name(x)\n:data\n\njulia\n y = mx.FullyConnected(x, num_hidden = 128)\njulia\n mx.get_name(y)\n:fullyconnected0\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.grad\n \n \nMethod\n.\n\n\ngrad(self :: SymbolicNode, wrt :: Vector{SymbolicNode})\n\n\n\n\nGet the autodiff gradient of the current \nSymbolicNode\n. This function can only be used if the current symbol is a loss function.\n\n\nArguments:\n\n\n\n\nself::SymbolicNode\n: current node.\n\n\nwrt::Vector{Symbol}\n: the names of the arguments to the gradient.\n\n\n\n\nReturns a gradient symbol of the corresponding gradient.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_shape\n \n \nMethod\n.\n\n\ninfer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)\n\n\n\n\nDo shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by \nlist_arguments\n. Alternatively, the shape information could be specified via keyword arguments.\n\n\nReturns a 3-tuple containing shapes of all the arguments, shapes of all the outputs and shapes of all the auxiliary variables. If shape inference failed due to incomplete or incompatible inputs, the return value will be \n(nothing, nothing, nothing)\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_type\n \n \nMethod\n.\n\n\ninfer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)\n\n\n\n\nDo type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by \nlist_arguments\n. Alternatively, the type information could be specified via keyword arguments.\n\n\nReturns a 3-tuple containing types of all the arguments, types of all the outputs and types of all the auxiliary variables. If type inference failed due to incomplete or incompatible inputs, the return value will be \n(nothing, nothing, nothing)\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.khatri_rao\n \n \nMethod\n.\n\n\nkhatri_rao(args)\n\n\n\n\nNote\n: khatri_rao takes variable number of positional inputs. So instead of calling as khatri_rao([x, y, z], num_args=3), one should call via khatri_rao(x, y, z), and num_args will be determined automatically.\n\n\nComputes the Khatri-Rao product of the input matrices.\n\n\nGiven a collection of :math:\nn\n input matrices,\n\n\n.. math::    A_1 \\in \\mathbb{R}^{M_1 \\times M}, \\ldots, A_n \\in \\mathbb{R}^{M_n \\times N},\n\n\nthe (column-wise) Khatri-Rao product is defined as the matrix,\n\n\n.. math::    X = A_1 \\otimes \\cdots \\otimes A_n \\in \\mathbb{R}^{(M_1 \\cdots M_n) \\times N},\n\n\nwhere the :math:\nk\nth column is equal to the column-wise outer product :math:\n{A_1}_k \\otimes \\cdots \\otimes {A_n}_k\n where :math:\n{A_i}_k\n is the kth column of the ith matrix.\n\n\nExample::\n\n\n\n\n\n\n\n\nA = mx.nd.array([[1, -1],                  [2, -3]]) B = mx.nd.array([[1, 4],                  [2, 5],                  [3, 6]]) C = mx.nd.khatri_rao(A, B) print(C.asnumpy())\n\n\n\n\n\n\n\n\n[[  1.  -4.]    [  2.  -5.]    [  3.  -6.]    [  2. -12.]    [  4. -15.]    [  6. -18.]]\n\n\nDefined in src/operator/contrib/krprod.cc:L108\n\n\nArguments\n\n\n\n\nargs::NDArray-or-SymbolicNode[]\n: Positional input matrices\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_gelqf\n \n \nMethod\n.\n\n\nlinalg_gelqf(A)\n\n\n\n\nlinalg_gelqf is an alias of _linalg_gelqf.\n\n\nLQ factorization for general matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, we compute the LQ factorization (LAPACK \ngelqf\n, followed by \norglq\n). \nA\n must have shape \n(x, y)\n with \nx \n= y\n, and must have full rank \n=x\n. The LQ factorization consists of \nL\n with shape \n(x, x)\n and \nQ\n with shape \n(x, y)\n, so that:\n\n\nA\n = \nL\n * \nQ\n\n\nHere, \nL\n is lower triangular (upper triangle equal to zero) with nonzero diagonal, and \nQ\n is row-orthonormal, meaning that\n\n\nQ\n * \nQ\n\\ :sup:\nT\n\n\nis equal to the identity matrix of shape \n(x, x)\n.\n\n\nIf \nn\n2\n, \ngelqf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]\n\n\n// Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L529\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be factorized\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_gemm\n \n \nMethod\n.\n\n\nlinalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)\n\n\n\n\nlinalg_gemm is an alias of _linalg_gemm.\n\n\nPerforms general matrix multiplication and accumulation. Input are tensors \nA\n, \nB\n, \nC\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n) + \nbeta\n * \nC\n\n\nHere, \nalpha\n and \nbeta\n are scalar parameters, and \nop()\n is either the identity or matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]\n\n\n// Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L69\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nC::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\nbeta::double, optional, default=1\n: Scalar factor multiplied with C.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_gemm2\n \n \nMethod\n.\n\n\nlinalg_gemm2(A, B, transpose_a, transpose_b, alpha)\n\n\n\n\nlinalg_gemm2 is an alias of _linalg_gemm2.\n\n\nPerforms general matrix multiplication. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, the BLAS3 function \ngemm\n is performed:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nop\n\\ (\nB\n)\n\n\nHere \nalpha\n is a scalar parameter and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose_a\n, \ntranspose_b\n).\n\n\nIf \nn\n2\n, \ngemm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]\n\n\n// Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L128\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose_a::boolean, optional, default=0\n: Multiply with transposed of first input (A).\n\n\ntranspose_b::boolean, optional, default=0\n: Multiply with transposed of second input (B).\n\n\nalpha::double, optional, default=1\n: Scalar factor multiplied with A*B.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_potrf\n \n \nMethod\n.\n\n\nlinalg_potrf(A)\n\n\n\n\nlinalg_potrf is an alias of _linalg_potrf.\n\n\nPerforms Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the Cholesky factor \nL\n of the symmetric, positive definite matrix \nA\n is computed. \nL\n is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:\n\n\nA\n = \nL\n * \nL\n\\ :sup:\nT\n\n\nIf \nn\n2\n, \npotrf\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]\n\n\n// Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L178\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices to be decomposed\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_potri\n \n \nMethod\n.\n\n\nlinalg_potri(A)\n\n\n\n\nlinalg_potri is an alias of _linalg_potri.\n\n\nPerforms matrix inversion from a Cholesky factorization. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:\n\n\nout\n = \nA\n\\ :sup:\n-T\n * \nA\n\\ :sup:\n-1\n\n\nIn other words, if \nA\n is the Cholesky factor of a symmetric positive definite matrix \nB\n (obtained by \npotrf\n), then\n\n\nout\n = \nB\n\\ :sup:\n-1\n\n\nIf \nn\n2\n, \npotri\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\n.. note:: Use this operator only if you are certain you need the inverse of \nB\n, and           cannot use the Cholesky factor \nA\n (\npotrf\n), together with backsubstitution           (\ntrsm\n). The latter is numerically much safer, and also cheaper.\n\n\nExamples::\n\n\n// Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]\n\n\n// Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L236\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_sumlogdiag\n \n \nMethod\n.\n\n\nlinalg_sumlogdiag(A)\n\n\n\n\nlinalg_sumlogdiag is an alias of _linalg_sumlogdiag.\n\n\nComputes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, \nA\n must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).\n\n\nIf \nn\n2\n, \nsumlogdiag\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]\n\n\n// Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]\n\n\nDefined in src/operator/tensor/la_op.cc:L405\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of square matrices\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_syrk\n \n \nMethod\n.\n\n\nlinalg_syrk(A, transpose, alpha)\n\n\n\n\nlinalg_syrk is an alias of _linalg_syrk.\n\n\nMultiplication of matrix with its transpose. Input is a tensor \nA\n of dimension \nn \n= 2\n.\n\n\nIf \nn=2\n, the operator performs the BLAS3 function \nsyrk\n:\n\n\nout\n = \nalpha\n * \nA\n * \nA\n\\ :sup:\nT\n\n\nif \ntranspose=False\n, or\n\n\nout\n = \nalpha\n * \nA\n\\ :sup:\nT\n \\ * \nA\n\n\nif \ntranspose=True\n.\n\n\nIf \nn\n2\n, \nsyrk\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]\n\n\n// Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L461\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of input matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transpose of input matrix.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_trmm\n \n \nMethod\n.\n\n\nlinalg_trmm(A, B, transpose, rightside, alpha)\n\n\n\n\nlinalg_trmm is an alias of _linalg_trmm.\n\n\nPerforms multiplication with a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrmm\n:\n\n\nout\n = \nalpha\n * \nop\n\\ (\nA\n) * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n = \nalpha\n * \nB\n * \nop\n\\ (\nA\n)\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrmm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]\n\n\n// Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L293\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.linalg_trsm\n \n \nMethod\n.\n\n\nlinalg_trsm(A, B, transpose, rightside, alpha)\n\n\n\n\nlinalg_trsm is an alias of _linalg_trsm.\n\n\nSolves matrix equation involving a lower triangular matrix. Input are tensors \nA\n, \nB\n, each of dimension \nn \n= 2\n and having the same shape on the leading \nn-2\n dimensions.\n\n\nIf \nn=2\n, \nA\n must be lower triangular. The operator performs the BLAS3 function \ntrsm\n, solving for \nout\n in:\n\n\nop\n\\ (\nA\n) * \nout\n = \nalpha\n * \nB\n\n\nif \nrightside=False\n, or\n\n\nout\n * \nop\n\\ (\nA\n) = \nalpha\n * \nB\n\n\nif \nrightside=True\n. Here, \nalpha\n is a scalar parameter, and \nop()\n is either the identity or the matrix transposition (depending on \ntranspose\n).\n\n\nIf \nn\n2\n, \ntrsm\n is performed separately on the trailing two dimensions for all inputs (batch mode).\n\n\n.. note:: The operator supports float32 and float64 data types only.\n\n\nExamples::\n\n\n// Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n\n\n// Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]\n\n\nDefined in src/operator/tensor/la_op.cc:L356\n\n\nArguments\n\n\n\n\nA::NDArray-or-SymbolicNode\n: Tensor of lower triangular matrices\n\n\nB::NDArray-or-SymbolicNode\n: Tensor of matrices\n\n\ntranspose::boolean, optional, default=0\n: Use transposed of the triangular matrix\n\n\nrightside::boolean, optional, default=0\n: Multiply triangular matrix from the right to non-triangular one.\n\n\nalpha::double, optional, default=1\n: Scalar factor to be applied to the result.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_all_attr\n \n \nMethod\n.\n\n\nlist_all_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from the symbol graph.\n\n\nReturns a dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_arguments\n \n \nMethod\n.\n\n\nlist_arguments(self :: SymbolicNode)\n\n\n\n\nList all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a \nFullyConnected\n node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.\n\n\nReturns a list of symbols indicating the names of the arguments.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_attr\n \n \nMethod\n.\n\n\nlist_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from a symbol.\n\n\nReturns a dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_auxiliary_states\n \n \nMethod\n.\n\n\nlist_auxiliary_states(self :: SymbolicNode)\n\n\n\n\nList all auxiliary states in the symbool.\n\n\nAuxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.\n\n\nReturns a list of symbols indicating the names of the auxiliary states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_outputs\n \n \nMethod\n.\n\n\nlist_outputs(self :: SymbolicNode)\n\n\n\n\nList all the outputs of this node.\n\n\nReturns a list of symbols indicating the names of the outputs.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a \nSymbolicNode\n from a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.make_loss\n \n \nMethod\n.\n\n\nmake_loss(data)\n\n\n\n\nMake your own loss function in network construction.\n\n\nThis operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.\n\n\nFor example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::\n\n\ncross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)\n\n\nWe will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.\n\n\nThe storage type of $make_loss$ output depends upon the input storage type:\n\n\n\n\nmake_loss(default) = default\n\n\nmake_loss(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L200\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.max\n \n \nMethod\n.\n\n\nmax(data, axis, keepdims, exclude)\n\n\n\n\nComputes the max of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L160\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.max_axis\n \n \nMethod\n.\n\n\nmax_axis(data, axis, keepdims, exclude)\n\n\n\n\nmax_axis is an alias of max.\n\n\nComputes the max of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L160\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.min\n \n \nMethod\n.\n\n\nmin(data, axis, keepdims, exclude)\n\n\n\n\nComputes the min of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L174\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.min_axis\n \n \nMethod\n.\n\n\nmin_axis(data, axis, keepdims, exclude)\n\n\n\n\nmin_axis is an alias of min.\n\n\nComputes the min of array elements over given axes.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L174\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mp_sgd_mom_update\n \n \nMethod\n.\n\n\nmp_sgd_mom_update(weight, grad, mom, weight32, lr, momentum, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdater function for multi-precision sgd optimizer\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmom::NDArray-or-SymbolicNode\n: Momentum\n\n\nweight32::NDArray-or-SymbolicNode\n: Weight32\n\n\nlr::float, required\n: Learning rate\n\n\nmomentum::float, optional, default=0\n: The decay rate of momentum estimates at each epoch.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mp_sgd_update\n \n \nMethod\n.\n\n\nmp_sgd_update(weight, grad, weight32, lr, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdater function for multi-precision sgd optimizer\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: gradient\n\n\nweight32::NDArray-or-SymbolicNode\n: Weight32\n\n\nlr::float, required\n: Learning rate\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nanprod\n \n \nMethod\n.\n\n\nnanprod(data, axis, keepdims, exclude)\n\n\n\n\nComputes the product of array elements over given axes treating Not a Numbers ($NaN$) as one.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L146\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.nansum\n \n \nMethod\n.\n\n\nnansum(data, axis, keepdims, exclude)\n\n\n\n\nComputes the sum of array elements over given axes treating Not a Numbers ($NaN$) as zero.\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L131\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.negative\n \n \nMethod\n.\n\n\nnegative(data)\n\n\n\n\nNumerical negative of the argument, element-wise.\n\n\nThe storage type of $negative$ output depends upon the input storage type:\n\n\n\n\nnegative(default) = default\n\n\nnegative(row_sparse) = row_sparse\n\n\nnegative(csr) = csr\n\n\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normal\n \n \nMethod\n.\n\n\nnormal(loc, scale, shape, ctx, dtype)\n\n\n\n\nnormal is an alias of _random_normal.\n\n\nDraw random samples from a normal (Gaussian) distribution.\n\n\n.. note:: The existing alias $normal$ is deprecated.\n\n\nSamples are distributed according to a normal distribution parametrized by \nloc\n (mean) and \nscale\n (standard deviation).\n\n\nExample::\n\n\nnormal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]\n\n\nDefined in src/operator/random/sample_op.cc:L85\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.one_hot\n \n \nMethod\n.\n\n\none_hot(indices, depth, on_value, off_value, dtype)\n\n\n\n\nReturns a one-hot array.\n\n\nThe locations represented by \nindices\n take value \non_value\n, while all other locations take value \noff_value\n.\n\n\none_hot\n operation with \nindices\n of shape $(i0, i1)$ and \ndepth\n  of $d$ would result in an output array of shape $(i0, i1, d)$ with::\n\n\noutput[i,j,:] = off_value   output[i,j,indices[i,j]] = on_value\n\n\nExamples::\n\n\none_hot([1,0,2,0], 3) = [[ 0.  1.  0.]                            [ 1.  0.  0.]                            [ 0.  0.  1.]                            [ 1.  0.  0.]]\n\n\none_hot([1,0,2,0], 3, on_value=8, off_value=1,           dtype='int32') = [[1 8 1]                             [8 1 1]                             [1 1 8]                             [8 1 1]]\n\n\none_hot([[1,0],[1,0],[2,0]], 3) = [[[ 0.  1.  0.]                                       [ 1.  0.  0.]]\n\n\n                                 [[ 0.  1.  0.]\n                                  [ 1.  0.  0.]]\n\n                                 [[ 0.  0.  1.]\n                                  [ 1.  0.  0.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L428\n\n\nArguments\n\n\n\n\nindices::NDArray-or-SymbolicNode\n: array of locations where to set on_value\n\n\ndepth::int, required\n: Depth of the one hot dimension.\n\n\non_value::double, optional, default=1\n: The value assigned to the locations represented by indices.\n\n\noff_value::double, optional, default=0\n: The value assigned to the locations not represented by indices.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32'\n: DType of the output\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones_like\n \n \nMethod\n.\n\n\nones_like(data)\n\n\n\n\nReturn an array of ones with the same shape and type as the input array.\n\n\nExamples::\n\n\nx = [[ 0.,  0.,  0.],        [ 0.,  0.,  0.]]\n\n\nones_like(x) = [[ 1.,  1.,  1.],                   [ 1.,  1.,  1.]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.pad\n \n \nMethod\n.\n\n\npad(data, mode, pad_width, constant_value)\n\n\n\n\npad is an alias of Pad.\n\n\nPads an input array with a constant or edge values of the array.\n\n\n.. note:: \nPad\n is deprecated. Use \npad\n instead.\n\n\n.. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in \npad_width\n to be zero.\n\n\nThis operation pads an input array with either a \nconstant_value\n or edge values along each axis of the input array. The amount of padding is specified by \npad_width\n.\n\n\npad_width\n is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The \npad_width\n should be of length $2*N$ where $N$ is the number of dimensions of the array.\n\n\nFor dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.\n\n\nExample::\n\n\nx = [[[[  1.   2.   3.]           [  4.   5.   6.]]\n\n\n     [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]\n\n\n\n\npad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]\n\n\n\n\npad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =\n\n\n     [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]\n\n\n\n\nDefined in src/operator/pad.cc:L766\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: An n-dimensional input array.\n\n\nmode::{'constant', 'edge', 'reflect'}, required\n: Padding type to use. \"constant\" pads with \nconstant_value\n \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.\n\n\npad_width::Shape(tuple), required\n: Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.\n\n\nconstant_value::double, optional, default=0\n: The value used for padding when \nmode\n is \"constant\".\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.pick\n \n \nMethod\n.\n\n\npick(data, index, axis, keepdims)\n\n\n\n\nPicks elements from an input array according to the input indices along the given axis.\n\n\nGiven an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::\n\n\noutput[i] = input[i, indices[i]]\n\n\nBy default, if any index mentioned is too large, it is replaced by the index that addresses the last element along an axis (the \nclip\n mode).\n\n\nThis function supports n-dimensional input and (n-1)-dimensional indices arrays.\n\n\nExamples::\n\n\nx = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]\n\n\n// picks elements with specified indices along axis 0   pick(x, y=[0,1], 0) = [ 1.,  4.]\n\n\n// picks elements with specified indices along axis 1   pick(x, y=[0,1,0], 1) = [ 1.,  4.,  5.]\n\n\ny = [[ 1.],        [ 0.],        [ 2.]]\n\n\n// picks elements with specified indices along axis 1 and dims are maintained   pick(x,y, 1, keepdims=True) = [[ 2.],                                  [ 3.],                                  [ 6.]]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_index.cc:L145\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\nindex::NDArray-or-SymbolicNode\n: The index array\n\n\naxis::int or None, optional, default='None'\n: The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$\n\n\nkeepdims::boolean, optional, default=0\n: If this is set to \nTrue\n, the reduced axis is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.radians\n \n \nMethod\n.\n\n\nradians(data)\n\n\n\n\nConverts each element of the input array from degrees to radians.\n\n\n.. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]\n\n\nThe storage type of $radians$ output depends upon the input storage type:\n\n\n\n\nradians(default) = default\n\n\nradians(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_trig.cc:L182\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_exponential\n \n \nMethod\n.\n\n\nrandom_exponential(lam, shape, ctx, dtype)\n\n\n\n\nrandom_exponential is an alias of _random_exponential.\n\n\nDraw random samples from an exponential distribution.\n\n\nSamples are distributed according to an exponential distribution parametrized by \nlambda\n (rate).\n\n\nExample::\n\n\nexponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]\n\n\nDefined in src/operator/random/sample_op.cc:L115\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the exponential distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_gamma\n \n \nMethod\n.\n\n\nrandom_gamma(alpha, beta, shape, ctx, dtype)\n\n\n\n\nrandom_gamma is an alias of _random_gamma.\n\n\nDraw random samples from a gamma distribution.\n\n\nSamples are distributed according to a gamma distribution parametrized by \nalpha\n (shape) and \nbeta\n (scale).\n\n\nExample::\n\n\ngamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]\n\n\nDefined in src/operator/random/sample_op.cc:L100\n\n\nArguments\n\n\n\n\nalpha::float, optional, default=1\n: Alpha parameter (shape) of the gamma distribution.\n\n\nbeta::float, optional, default=1\n: Beta parameter (scale) of the gamma distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_generalized_negative_binomial\n \n \nMethod\n.\n\n\nrandom_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)\n\n\n\n\nrandom_generalized_negative_binomial is an alias of _random_generalized_negative_binomial.\n\n\nDraw random samples from a generalized negative binomial distribution.\n\n\nSamples are distributed according to a generalized negative binomial distribution parametrized by \nmu\n (mean) and \nalpha\n (dispersion). \nalpha\n is defined as \n1/k\n where \nk\n is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\ngeneralized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]\n\n\nDefined in src/operator/random/sample_op.cc:L168\n\n\nArguments\n\n\n\n\nmu::float, optional, default=1\n: Mean of the negative binomial distribution.\n\n\nalpha::float, optional, default=1\n: Alpha (dispersion) parameter of the negative binomial distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_negative_binomial\n \n \nMethod\n.\n\n\nrandom_negative_binomial(k, p, shape, ctx, dtype)\n\n\n\n\nrandom_negative_binomial is an alias of _random_negative_binomial.\n\n\nDraw random samples from a negative binomial distribution.\n\n\nSamples are distributed according to a negative binomial distribution parametrized by \nk\n (limit of unsuccessful experiments) and \np\n (failure probability in each experiment). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\nnegative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]\n\n\nDefined in src/operator/random/sample_op.cc:L149\n\n\nArguments\n\n\n\n\nk::int, optional, default='1'\n: Limit of unsuccessful experiments.\n\n\np::float, optional, default=1\n: Failure probability in each experiment.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_normal\n \n \nMethod\n.\n\n\nrandom_normal(loc, scale, shape, ctx, dtype)\n\n\n\n\nrandom_normal is an alias of _random_normal.\n\n\nDraw random samples from a normal (Gaussian) distribution.\n\n\n.. note:: The existing alias $normal$ is deprecated.\n\n\nSamples are distributed according to a normal distribution parametrized by \nloc\n (mean) and \nscale\n (standard deviation).\n\n\nExample::\n\n\nnormal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]\n\n\nDefined in src/operator/random/sample_op.cc:L85\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_poisson\n \n \nMethod\n.\n\n\nrandom_poisson(lam, shape, ctx, dtype)\n\n\n\n\nrandom_poisson is an alias of _random_poisson.\n\n\nDraw random samples from a Poisson distribution.\n\n\nSamples are distributed according to a Poisson distribution parametrized by \nlambda\n (rate). Samples will always be returned as a floating point data type.\n\n\nExample::\n\n\npoisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]\n\n\nDefined in src/operator/random/sample_op.cc:L132\n\n\nArguments\n\n\n\n\nlam::float, optional, default=1\n: Lambda parameter (rate) of the Poisson distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.random_uniform\n \n \nMethod\n.\n\n\nrandom_uniform(low, high, shape, ctx, dtype)\n\n\n\n\nrandom_uniform is an alias of _random_uniform.\n\n\nDraw random samples from a uniform distribution.\n\n\n.. note:: The existing alias $uniform$ is deprecated.\n\n\nSamples are uniformly distributed over the half-open interval \n[low, high)\n (includes \nlow\n, but excludes \nhigh\n).\n\n\nExample::\n\n\nuniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]\n\n\nDefined in src/operator/random/sample_op.cc:L66\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: Lower bound of the distribution.\n\n\nhigh::float, optional, default=1\n: Upper bound of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rcbrt\n \n \nMethod\n.\n\n\nrcbrt(data)\n\n\n\n\nReturns element-wise inverse cube-root value of the input.\n\n\n.. math::    rcbrt(x) = 1/\\sqrt[3]{x}\n\n\nExample::\n\n\nrcbrt([1,8,-125]) = [1.0, 0.5, -0.2]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L619\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.reciprocal\n \n \nMethod\n.\n\n\nreciprocal(data)\n\n\n\n\nReturns the reciprocal of the argument, element-wise.\n\n\nCalculates 1/x.\n\n\nExample::\n\n\nreciprocal([-2, 1, 3, 1.6, 0.2]) = [-0.5, 1.0, 0.33333334, 0.625, 5.0]\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L364\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.reshape_like\n \n \nMethod\n.\n\n\nreshape_like(lhs, rhs)\n\n\n\n\nReshape lhs to have the same shape as rhs.\n\n\nArguments\n\n\n\n\nlhs::NDArray-or-SymbolicNode\n: First input.\n\n\nrhs::NDArray-or-SymbolicNode\n: Second input.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rint\n \n \nMethod\n.\n\n\nrint(data)\n\n\n\n\nReturns element-wise rounded value to the nearest integer of the input.\n\n\n.. note::\n\n\n\n\nFor input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.\n\n\nFor input $-n.5$ both $rint$ and $round$ returns $-n-1$.\n\n\n\n\nExample::\n\n\nrint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]\n\n\nThe storage type of $rint$ output depends upon the input storage type:\n\n\n\n\nrint(default) = default\n\n\nrint(row_sparse) = row_sparse\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L445\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rmsprop_update\n \n \nMethod\n.\n\n\nrmsprop_update(weight, grad, n, lr, gamma1, epsilon, wd, rescale_grad, clip_gradient, clip_weights)\n\n\n\n\nUpdate function for \nRMSProp\n optimizer.\n\n\nRMSprop\n is a variant of stochastic gradient descent where the gradients are divided by a cache which grows with the sum of squares of recent gradients?\n\n\nRMSProp\n is similar to \nAdaGrad\n, a popular variant of \nSGD\n which adaptively tunes the learning rate of each parameter. \nAdaGrad\n lowers the learning rate for each parameter monotonically over the course of training. While this is analytically motivated for convex optimizations, it may not be ideal for non-convex problems. \nRMSProp\n deals with this heuristically by allowing the learning rates to rebound as the denominator decays over time.\n\n\nDefine the Root Mean Square (RMS) error criterion of the gradient as :math:\nRMS[g]_t = \\sqrt{E[g^2]_t + \\epsilon}\n, where :math:\ng\n represents gradient and :math:\nE[g^2]_t\n is the decaying average over past squared gradient.\n\n\nThe :math:\nE[g^2]_t\n is given by:\n\n\n.. math::   E[g^2]\nt = \\gamma * E[g^2]\n + (1-\\gamma) * g_t^2\n\n\nThe update step is\n\n\n.. math::   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{RMS[g]_t} g_t\n\n\nThe RMSProp code follows the version in http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf Tieleman \n Hinton, 2012.\n\n\nHinton suggests the momentum term :math:\n\\gamma\n to be 0.9 and the learning rate :math:\n\\eta\n to be 0.001.\n\n\nDefined in src/operator/optimizer_op.cc:L262\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nn::NDArray-or-SymbolicNode\n: n\n\n\nlr::float, required\n: Learning rate\n\n\ngamma1::float, optional, default=0.95\n: The decay rate of momentum estimates.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nclip_weights::float, optional, default=-1\n: Clip weights to the range of [-clip_weights, clip_weights] If clip_weights \n= 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rmspropalex_update\n \n \nMethod\n.\n\n\nrmspropalex_update(weight, grad, n, g, delta, lr, gamma1, gamma2, epsilon, wd, rescale_grad, clip_gradient, clip_weights)\n\n\n\n\nUpdate function for RMSPropAlex optimizer.\n\n\nRMSPropAlex\n is non-centered version of \nRMSProp\n.\n\n\nDefine :math:\nE[g^2]_t\n is the decaying average over past squared gradient and :math:\nE[g]_t\n is the decaying average over past gradient.\n\n\n.. math::   E[g^2]\nt = \\gamma_1 * E[g^2]\n + (1 - \\gamma_1) * g_t^2\\\n  E[g]\nt = \\gamma_1 * E[g]\n + (1 - \\gamma_1) * g_t\\\n  \\Delta_t = \\gamma_2 * \\Delta_{t-1} - \\frac{\\eta}{\\sqrt{E[g^2]_t - E[g]_t^2 + \\epsilon}} g_t\\\n The update step is\n\n\n.. math::   \\theta_{t+1} = \\theta_t + \\Delta_t\n\n\nThe RMSPropAlex code follows the version in http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.\n\n\nGraves suggests the momentum term :math:\n\\gamma_1\n to be 0.95, :math:\n\\gamma_2\n to be 0.9 and the learning rate :math:\n\\eta\n to be 0.0001.\n\n\nDefined in src/operator/optimizer_op.cc:L301\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nn::NDArray-or-SymbolicNode\n: n\n\n\ng::NDArray-or-SymbolicNode\n: g\n\n\ndelta::NDArray-or-SymbolicNode\n: delta\n\n\nlr::float, required\n: Learning rate\n\n\ngamma1::float, optional, default=0.95\n: Decay rate.\n\n\ngamma2::float, optional, default=0.9\n: Decay rate.\n\n\nepsilon::float, optional, default=1e-08\n: A small constant for numerical stability.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nclip_weights::float, optional, default=-1\n: Clip weights to the range of [-clip_weights, clip_weights] If clip_weights \n= 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rsqrt\n \n \nMethod\n.\n\n\nrsqrt(data)\n\n\n\n\nReturns element-wise inverse square-root value of the input.\n\n\n.. math::    rsqrt(x) = 1/\\sqrt{x}\n\n\nExample::\n\n\nrsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]\n\n\nThe storage type of $rsqrt$ output is always dense\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L585\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_exponential\n \n \nMethod\n.\n\n\nsample_exponential(lam, shape, dtype)\n\n\n\n\nsample_exponential is an alias of _sample_exponential.\n\n\nConcurrent sampling from multiple exponential distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]\n\n\n// Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]\n\n\nDefined in src/operator/random/multisample_op.cc:L284\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_gamma\n \n \nMethod\n.\n\n\nsample_gamma(alpha, shape, dtype, beta)\n\n\n\n\nsample_gamma is an alias of _sample_gamma.\n\n\nConcurrent sampling from multiple gamma distributions with parameters \nalpha\n (shape) and \nbeta\n (scale).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nalpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]\n\n\n// Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]\n\n\n// Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]\n\n\nDefined in src/operator/random/multisample_op.cc:L282\n\n\nArguments\n\n\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (shape) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nbeta::NDArray-or-SymbolicNode\n: Beta (scale) parameters of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_generalized_negative_binomial\n \n \nMethod\n.\n\n\nsample_generalized_negative_binomial(mu, shape, dtype, alpha)\n\n\n\n\nsample_generalized_negative_binomial is an alias of _sample_generalized_negative_binomial.\n\n\nConcurrent sampling from multiple generalized negative binomial distributions with parameters \nmu\n (mean) and \nalpha\n (dispersion).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nmu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]\n\n\n// Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]\n\n\n// Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L293\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nalpha::NDArray-or-SymbolicNode\n: Alpha (dispersion) parameters of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_multinomial\n \n \nMethod\n.\n\n\nsample_multinomial(data, shape, get_prob, dtype)\n\n\n\n\nsample_multinomial is an alias of _sample_multinomial.\n\n\nConcurrent sampling from multiple multinomial distributions.\n\n\ndata\n is an \nn\n dimensional array whose last dimension has length \nk\n, where \nk\n is the number of possible outcomes of each multinomial distribution. This operator will draw \nshape\n samples from each distribution. If shape is empty one sample will be drawn from each distribution.\n\n\nIf \nget_prob\n is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.\n\n\nNote that the input distribution must be normalized, i.e. \ndata\n must sum to 1 along its last axis.\n\n\nExamples::\n\n\nprobs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]\n\n\n// Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]\n\n\n// Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]\n\n\n// requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Distribution probabilities. Must sum to one on the last axis.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\nget_prob::boolean, optional, default=0\n: Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.\n\n\ndtype::{'int32'},optional, default='int32'\n: DType of the output in case this can't be inferred. Only support int32 for now.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_negative_binomial\n \n \nMethod\n.\n\n\nsample_negative_binomial(k, shape, dtype, p)\n\n\n\n\nsample_negative_binomial is an alias of _sample_negative_binomial.\n\n\nConcurrent sampling from multiple negative binomial distributions with parameters \nk\n (failure limit) and \np\n (failure probability).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nk = [ 20, 49 ]    p = [ 0.4 , 0.77 ]\n\n\n// Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]\n\n\n// Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L289\n\n\nArguments\n\n\n\n\nk::NDArray-or-SymbolicNode\n: Limits of unsuccessful experiments.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\np::NDArray-or-SymbolicNode\n: Failure probabilities in each experiment.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_normal\n \n \nMethod\n.\n\n\nsample_normal(mu, shape, dtype, sigma)\n\n\n\n\nsample_normal is an alias of _sample_normal.\n\n\nConcurrent sampling from multiple normal distributions with parameters \nmu\n (mean) and \nsigma\n (standard deviation).\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nmu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]\n\n\n// Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]\n\n\nDefined in src/operator/random/multisample_op.cc:L279\n\n\nArguments\n\n\n\n\nmu::NDArray-or-SymbolicNode\n: Means of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nsigma::NDArray-or-SymbolicNode\n: Standard deviations of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_poisson\n \n \nMethod\n.\n\n\nsample_poisson(lam, shape, dtype)\n\n\n\n\nsample_poisson is an alias of _sample_poisson.\n\n\nConcurrent sampling from multiple Poisson distributions with parameters lambda (rate).\n\n\nThe parameters of the distributions are provided as an input array. Let \n[s]\n be the shape of the input array, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input array, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.\n\n\nSamples will always be returned as a floating point data type.\n\n\nExamples::\n\n\nlam = [ 1.0, 8.5 ]\n\n\n// Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]\n\n\n// Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]\n\n\nDefined in src/operator/random/multisample_op.cc:L286\n\n\nArguments\n\n\n\n\nlam::NDArray-or-SymbolicNode\n: Lambda (rate) parameters of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sample_uniform\n \n \nMethod\n.\n\n\nsample_uniform(low, shape, dtype, high)\n\n\n\n\nsample_uniform is an alias of _sample_uniform.\n\n\nConcurrent sampling from multiple uniform distributions on the intervals given by \n[low,high)\n.\n\n\nThe parameters of the distributions are provided as input arrays. Let \n[s]\n be the shape of the input arrays, \nn\n be the dimension of \n[s]\n, \n[t]\n be the shape specified as the parameter of the operator, and \nm\n be the dimension of \n[t]\n. Then the output will be a \n(n+m)\n-dimensional array with shape \n[s]x[t]\n.\n\n\nFor any valid \nn\n-dimensional index \ni\n with respect to the input arrays, \noutput[i]\n will be an \nm\n-dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index \ni\n. If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.\n\n\nExamples::\n\n\nlow = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]\n\n\n// Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]\n\n\n// Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]\n\n\nDefined in src/operator/random/multisample_op.cc:L277\n\n\nArguments\n\n\n\n\nlow::NDArray-or-SymbolicNode\n: Lower bounds of the distributions.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape to be sampled from each random distribution.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nhigh::NDArray-or-SymbolicNode\n: Upper bounds of the distributions.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename :: AbstractString, node :: SymbolicNode)\n\n\n\n\nSave a \nSymbolicNode\n to a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.scatter_nd\n \n \nMethod\n.\n\n\nscatter_nd(data, indices, shape)\n\n\n\n\nScatters data into a new tensor according to indices. \ngather_nd\n and \nscatter_nd\n are inverse functions to each other.\n\n\nGiven \ndata\n with shape \n(Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1})\n and indices with shape \n(M, Y_0, ..., Y_{K-1})\n, the output will have shape \n(X_0, X_1, ..., X_{N-1})\n, where \nM \n= N\n. If \nM == N\n, data shape should simply be \n(Y_0, ..., Y_{K-1})\n.\n\n\nThe elements in output is defined as follows::\n\n\noutput[indices[0, y_0, ..., y_{K-1}],          ...,          indices[M-1, y_0, ..., y_{K-1}],          x_M, ..., x_{N-1}] = data[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}]\n\n\nall other entries in output are 0.\n\n\nExamples::\n\n\ndata = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   shape = (2, 2)   scatter_nd(data, indices, shape) = [[0, 0], [2, 3]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: data\n\n\nindices::NDArray-or-SymbolicNode\n: indices\n\n\nshape::Shape(tuple), required\n: Shape of output.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.set_attr\n \n \nMethod\n.\n\n\nset_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)\n\n\n\n\nSet the attribute key to value for this \nSymbolicNode\n.\n\n\n\n\nNote\n\n\nIt is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the \nSymbolicNode\n. Changing the attributes of a \nSymbolicNode\n that is already been used somewhere else might cause unexpected behavior and inconsistency.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_mom_update\n \n \nMethod\n.\n\n\nsgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)\n\n\n\n\nMomentum update function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nMomentum update has better convergence rates on neural networks. Mathematically it looks like below:\n\n\n.. math::\n\n\nv_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t\n\n\nIt updates the weights using::\n\n\nv = momentum * v - learning_rate * gradient   weight += v\n\n\nWhere the parameter $momentum$ is the decay rate of momentum estimates at each epoch.\n\n\nIf weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::\n\n\nfor row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]\n\n\nDefined in src/operator/optimizer_op.cc:L94\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nmom::NDArray-or-SymbolicNode\n: Momentum\n\n\nlr::float, required\n: Learning rate\n\n\nmomentum::float, optional, default=0\n: The decay rate of momentum estimates at each epoch.\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sgd_update\n \n \nMethod\n.\n\n\nsgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)\n\n\n\n\nUpdate function for Stochastic Gradient Descent (SDG) optimizer.\n\n\nIt updates the weights using::\n\n\nweight = weight - learning_rate * gradient\n\n\nIf weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::\n\n\nfor row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]\n\n\nDefined in src/operator/optimizer_op.cc:L54\n\n\nArguments\n\n\n\n\nweight::NDArray-or-SymbolicNode\n: Weight\n\n\ngrad::NDArray-or-SymbolicNode\n: Gradient\n\n\nlr::float, required\n: Learning rate\n\n\nwd::float, optional, default=0\n: Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.\n\n\nrescale_grad::float, optional, default=1\n: Rescale gradient to grad = rescale_grad*grad.\n\n\nclip_gradient::float, optional, default=-1\n: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient \n= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice\n \n \nMethod\n.\n\n\nslice(data, begin, end, step)\n\n\n\n\nSlices a region of the array.\n\n\n.. note:: $crop$ is deprecated. Use $slice$ instead.\n\n\nThis function returns a sliced array between the indices given by \nbegin\n and \nend\n with the corresponding \nstep\n.\n\n\nFor an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m \n= n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.\n\n\nThe resulting array's \nk\n-th dimension contains elements from the \nk\n-th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).\n\n\nIf the \nk\n-th elements are \nNone\n in the sequence of \nbegin\n, \nend\n, and \nstep\n, the following rule will be used to set default values. If \ns_k\n is \nNone\n, set \ns_k=1\n. If \ns_k \n 0\n, set \nb_k=0\n, \ne_k=d_k\n; else, set \nb_k=d_k-1\n, \ne_k=-1\n.\n\n\nThe storage type of $slice$ output depends on storage types of inputs\n\n\n\n\nslice(csr) = csr\n\n\notherwise, $slice$ generates output with default storage\n\n\n\n\n.. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.\n\n\nExample::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L355\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\nbegin::Shape(tuple), required\n: starting indices for the slice operation, supports negative indices.\n\n\nend::Shape(tuple), required\n: ending indices for the slice operation, supports negative indices.\n\n\nstep::Shape(tuple), optional, default=[]\n: step for the slice operation, supports negative values.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice_axis\n \n \nMethod\n.\n\n\nslice_axis(data, axis, begin, end)\n\n\n\n\nSlices along a given axis.\n\n\nReturns an array slice along a given \naxis\n starting from the \nbegin\n index to the \nend\n index.\n\n\nExamples::\n\n\nx = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]\n\n\nslice_axis(x, axis=0, begin=1, end=3) = [[  5.,   6.,   7.,   8.],                                            [  9.,  10.,  11.,  12.]]\n\n\nslice_axis(x, axis=1, begin=0, end=2) = [[  1.,   2.],                                            [  5.,   6.],                                            [  9.,  10.]]\n\n\nslice_axis(x, axis=1, begin=-3, end=-1) = [[  2.,   3.],                                              [  6.,   7.],                                              [ 10.,  11.]]\n\n\nDefined in src/operator/tensor/matrix_op.cc:L442\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Source input\n\n\naxis::int, required\n: Axis along which to be sliced, supports negative indexes.\n\n\nbegin::int, required\n: The beginning index along the axis to be sliced,  supports negative indexes.\n\n\nend::int or None, required\n: The ending index along the axis to be sliced,  supports negative indexes.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.smooth_l1\n \n \nMethod\n.\n\n\nsmooth_l1(data, scalar)\n\n\n\n\nCalculate Smooth L1 Loss(lhs, scalar) by summing\n\n\n.. math::\n\n\nf(x) =\n\\begin{cases}\n(\\sigma x)^2/2,\n \\text{if }x \n 1/\\sigma^2\\\\\n|x|-0.5/\\sigma^2,\n \\text{otherwise}\n\\end{cases}\n\n\n\n\nwhere :math:\nx\n is an element of the tensor \nlhs\n and :math:\n\\sigma\n is the scalar.\n\n\nExample::\n\n\nsmooth_l1([1, 2, 3, 4], sigma=1) = [0.5, 1.5, 2.5, 3.5]\n\n\nDefined in src/operator/tensor/elemwise_binary_scalar_op_extended.cc:L103\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: source input\n\n\nscalar::float\n: scalar input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax_cross_entropy\n \n \nMethod\n.\n\n\nsoftmax_cross_entropy(data, label)\n\n\n\n\nCalculate cross entropy of softmax output and one-hot label.\n\n\n\n\n\n\nThis operator computes the cross entropy in two steps:\n\n\n\n\nApplies softmax function on the input array.\n\n\nComputes and returns the cross entropy loss between the softmax output and the labels.\n\n\n\n\nThe softmax function and cross entropy loss is given by:\n\n\n\n\n\n\nSoftmax Function:\n\n\n\n\n\n\n.. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n\n\n\n\nCross Entropy Function:\n\n\n\n\n.. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)\n\n\n\n\n\n\nExample::\n\n\nx = [[1, 2, 3],        [11, 7, 5]]\n\n\nlabel = [2, 0]\n\n\nsoftmax(x) = [[0.09003057, 0.24472848, 0.66524094],                 [0.97962922, 0.01794253, 0.00242826]]\n\n\nsoftmax_cross_entropy(data, label) = - log(0.66524084) - log(0.97962922) = 0.4281871\n\n\nDefined in src/operator/loss_binary_op.cc:L59\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data\n\n\nlabel::NDArray-or-SymbolicNode\n: Input label\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.square\n \n \nMethod\n.\n\n\nsquare(data)\n\n\n\n\nReturns element-wise squared value of the input.\n\n\n.. math::    square(x) = x^2\n\n\nExample::\n\n\nsquare([2, 3, 4]) = [4, 9, 16]\n\n\nThe storage type of $square$ output depends upon the input storage type:\n\n\n\n\nsquare(default) = default\n\n\nsquare(row_sparse) = row_sparse\n\n\nsquare(csr) = csr\n\n\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L542\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.stack\n \n \nMethod\n.\n\n\nstack(data, axis, num_args)\n\n\n\n\nNote\n: stack takes variable number of positional inputs. So instead of calling as stack([x, y, z], num_args=3), one should call via stack(x, y, z), and num_args will be determined automatically.\n\n\nJoin a sequence of arrays along a new axis.\n\n\nThe axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.\n\n\nExamples::\n\n\nx = [1, 2]   y = [3, 4]\n\n\nstack(x, y) = [[1, 2],                  [3, 4]]   stack(x, y, axis=1) = [[1, 3],                          [2, 4]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode[]\n: List of arrays to stack\n\n\naxis::int, optional, default='0'\n: The axis in the result array along which the input arrays are stacked.\n\n\nnum_args::int, required\n: Number of inputs to be stacked.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.stop_gradient\n \n \nMethod\n.\n\n\nstop_gradient(data)\n\n\n\n\nstop_gradient is an alias of BlockGrad.\n\n\nStops gradient computation.\n\n\nStops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.\n\n\nExample::\n\n\nv1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)\n\n\nexecutor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]\n\n\nexecutor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]\n\n\nDefined in src/operator/tensor/elemwise_unary_op_basic.cc:L167\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum_axis\n \n \nMethod\n.\n\n\nsum_axis(data, axis, keepdims, exclude)\n\n\n\n\nsum_axis is an alias of sum.\n\n\nComputes the sum of array elements over given axes.\n\n\n.. Note::\n\n\nsum\n and \nsum_axis\n are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.\n\n\nExample::\n\n\ndata = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]\n\n\nsum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]\n\n\nsum(data, axis=[1,2])   [ 12.  19.  27.]\n\n\ndata = [[1,2,0],           [3,0,1],           [4,1,0]]\n\n\ncsr = cast_storage(data, 'csr')\n\n\nsum(csr, axis=0)   [ 8.  3.  1.]\n\n\nsum(csr, axis=1)   [ 3.  4.  5.]\n\n\nDefined in src/operator/tensor/broadcast_reduce_op_value.cc:L85\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\n\n\naxis::Shape(tuple), optional, default=[]\n: The axis or axes along which to perform the reduction.\n\n\n``\nThe default,\naxis=()\n, will compute over all elements into a\nscalar array with shape\n(1,)`.\n\n\nIf \naxis\n is int, a reduction is performed on a particular axis.\n\n\nIf \naxis\n is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.\n\n\nIf \nexclude\n is true, reduction will be performed on the axes that are\nNOT in axis instead.\n\n\nNegative values means indexing from right to left.\n\n``\n  *\nkeepdims::boolean, optional, default=0\n: If this is set to\nTrue\n, the reduced axes are left in the result as dimension with size one.\n  *\nexclude::boolean, optional, default=0\n: Whether to perform reduction on axis that are NOT in axis instead.\n  *\nname::Symbol\n: The name of the\nSymbolicNode\n. (e.g.\n:my_symbol\n), optional.\n  *\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this\nSymbolicNode`.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.swapaxes\n \n \nMethod\n.\n\n\nswapaxes(data, dim1, dim2)\n\n\n\n\nswapaxes is an alias of SwapAxis.\n\n\nInterchanges two axes of an array.\n\n\nExamples::\n\n\nx = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]\n\n\nx = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array\n\n\nswapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]\n\n\nDefined in src/operator/swapaxis.cc:L70\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input array.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.take\n \n \nMethod\n.\n\n\ntake(a, indices, axis, mode)\n\n\n\n\nTakes elements from an input array along the given axis.\n\n\nThis function slices the input array along a particular axis with the provided indices.\n\n\nGiven an input array with shape $(d0, d1, d2)$ and indices with shape $(i0, i1)$, the output will have shape $(i0, i1, d1, d2)$, computed by::\n\n\noutput[i,j,:,:] = input[indices[i,j],:,:]\n\n\n.. note::\n\n\n\n\naxis\n- Only slicing along axis 0 is supported for now.\n\n\nmode\n- Only \nclip\n mode is supported for now.\n\n\n\n\nExamples::\n\n\nx = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]\n\n\n// takes elements with specified indices along axis 0   take(x, [[0,1],[1,2]]) = [[[ 1.,  2.],                              [ 3.,  4.]],\n\n\n                        [[ 3.,  4.],\n                         [ 5.,  6.]]]\n\n\n\n\nDefined in src/operator/tensor/indexing_op.cc:L327\n\n\nArguments\n\n\n\n\na::NDArray-or-SymbolicNode\n: The input array.\n\n\nindices::NDArray-or-SymbolicNode\n: The indices of the values to be extracted.\n\n\naxis::int, optional, default='0'\n: The axis of input array to be taken.\n\n\nmode::{'clip', 'raise', 'wrap'},optional, default='clip'\n: Specify how out-of-bound indices bahave. \"clip\" means clip to the range. So, if all indices mentioned are too large, they are replaced by the index that addresses the last element along an axis.  \"wrap\" means to wrap around.  \"raise\" means to raise an error.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.tile\n \n \nMethod\n.\n\n\ntile(data, reps)\n\n\n\n\nRepeats the whole array multiple times.\n\n\nIf $reps$ has length \nd\n, and input array has dimension of \nn\n. There are three cases:\n\n\n\n\n\n\nn=d\n. Repeat \ni\n-th dimension of the input by $reps[i]$ times::\n\n\nx = [[1, 2],        [3, 4]]\n\n\ntile(x, reps=(2,3)) = [[ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.],                          [ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.]]\n  * \nn\nd\n. $reps$ is promoted to length \nn\n by pre-pending 1's to it. Thus for an input shape $(2,3)$, $repos=(2,)$ is treated as $(1,2)$::\n\n\n\n\n\n\ntile(x, reps=(2,)) = [[ 1.,  2.,  1.,  2.],\n                      [ 3.,  4.,  3.,  4.]]\n\n\n\n\n\n\n\n\nn\nd\n. The input is promoted to be d-dimensional by prepending new axes. So a shape $(2,2)$ array is promoted to $(1,2,2)$ for 3-D replication::\n\n\ntile(x, reps=(2,2,3)) = [[[ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.],                             [ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.]],\n\n\n[[ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.],\n                        [ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.]]]\n\n\n\n\n\n\nDefined in src/operator/tensor/matrix_op.cc:L621\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: Input data array\n\n\nreps::Shape(tuple), required\n: The number of times for repeating the tensor a. If reps has length d, the result will have dimension of max(d, a.ndim); If a.ndim \n d, a is promoted to be d-dimensional by prepending new axes. If a.ndim \n d, reps is promoted to a.ndim by pre-pending 1's to it.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.to_json\n \n \nMethod\n.\n\n\nto_json(self :: SymbolicNode)\n\n\n\n\nConvert a \nSymbolicNode\n into a JSON string.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.topk\n \n \nMethod\n.\n\n\ntopk(data, axis, k, ret_typ, is_ascend)\n\n\n\n\nReturns the top \nk\n elements in an input array along the given axis.\n\n\nExamples::\n\n\nx = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]\n\n\n// returns an index of the largest element on last axis   topk(x) = [[ 2.],              [ 1.]]\n\n\n// returns the value of top-2 largest elements on last axis   topk(x, ret_typ='value', k=2) = [[ 0.4,  0.3],                                    [ 0.3,  0.2]]\n\n\n// returns the value of top-2 smallest elements on last axis   topk(x, ret_typ='value', k=2, is_ascend=1) = [[ 0.2 ,  0.3],                                                [ 0.1 ,  0.2]]\n\n\n// returns the value of top-2 largest elements on axis 0   topk(x, axis=0, ret_typ='value', k=2) = [[ 0.3,  0.3,  0.4],                                            [ 0.1,  0.2,  0.2]]\n\n\n// flattens and then returns list of both values and indices   topk(x, ret_typ='both', k=2) = [[[ 0.4,  0.3], [ 0.3,  0.2]] ,  [[ 2.,  0.], [ 1.,  2.]]]\n\n\nDefined in src/operator/tensor/ordering_op.cc:L63\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input array\n\n\naxis::int or None, optional, default='-1'\n: Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.\n\n\nk::int, optional, default='1'\n: Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k \n 1.\n\n\nret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices'\n: The return type.\n\n\n\n\n\"value\" means to return the top k values, \"indices\" means to return the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return a list of both values and indices of top k elements.\n\n\n\n\nis_ascend::boolean, optional, default=0\n: Whether to choose k largest or k smallest elements. Top K largest elements will be chosen if set to false.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.uniform\n \n \nMethod\n.\n\n\nuniform(low, high, shape, ctx, dtype)\n\n\n\n\nuniform is an alias of _random_uniform.\n\n\nDraw random samples from a uniform distribution.\n\n\n.. note:: The existing alias $uniform$ is deprecated.\n\n\nSamples are uniformly distributed over the half-open interval \n[low, high)\n (includes \nlow\n, but excludes \nhigh\n).\n\n\nExample::\n\n\nuniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]\n\n\nDefined in src/operator/random/sample_op.cc:L66\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: Lower bound of the distribution.\n\n\nhigh::float, optional, default=1\n: Upper bound of the distribution.\n\n\nshape::Shape(tuple), optional, default=[]\n: Shape of the output.\n\n\nctx::string, optional, default=''\n: Context of output, in format \ncpu|gpu|cpu_pinned\n. Only used for imperative calls.\n\n\ndtype::{'None', 'float16', 'float32', 'float64'},optional, default='None'\n: DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.where\n \n \nMethod\n.\n\n\nwhere(condition, x, y)\n\n\n\n\nGiven three ndarrays, condition, x, and y, return an ndarray with the elements from x or y, depending on the elements from condition are true or false. x and y must have the same shape. If condition has the same shape as x, each element in the output array is from x if the corresponding element in the condition is true, and from y if false. If condition does not have the same shape as x, it must be a 1D array whose size is the same as x's first dimension size. Each row of the output array is from x's row if the corresponding element from condition is true, and from y's row if false.\n\n\nFrom:src/operator/tensor/control_flow_op.cc:40\n\n\nArguments\n\n\n\n\ncondition::NDArray-or-SymbolicNode\n: condition array\n\n\nx::NDArray-or-SymbolicNode\n:\n\n\ny::NDArray-or-SymbolicNode\n:\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros_like\n \n \nMethod\n.\n\n\nzeros_like(data)\n\n\n\n\nReturn an array of zeros with the same shape and type as the input array.\n\n\nThe storage type of $zeros_like$ output depends on the storage type of the input\n\n\n\n\nzeros_like(row_sparse) = row_sparse\n\n\nzeros_like(csr) = csr\n\n\nzeros_like(default) = default\n\n\n\n\nExamples::\n\n\nx = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]\n\n\nzeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]\n\n\nArguments\n\n\n\n\ndata::NDArray-or-SymbolicNode\n: The input\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nsource", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/symbolic-node/#symbolic-api", 
            "text": "#  MXNet.mx.SymbolicNode     Type .  SymbolicNode  SymbolicNode is the basic building block of the symbolic graph in MXNet.jl.  (self :: SymbolicNode)(args :: SymbolicNode...)\n(self :: SymbolicNode)(; kwargs...)  Make a new node by composing  self  with  args . Or the arguments can be specified using keyword arguments.  source  #  Base.cos     Method .  cos(data)  Computes the element-wise cosine of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    cos([0, \\pi/4, \\pi/2]) = [1, 0.707, 0]  The storage type of $cos$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L63  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.cosh     Method .  cosh(data)  Returns the hyperbolic cosine  of the input array, computed element-wise.  .. math::    cosh(x) = 0.5\\times(exp(x) + exp(-x))  The storage type of $cosh$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L216  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.reshape     Method .  reshape(sym::SymbolicNode, dim; reverse=false, name)\nreshape(sym::SymbolicNode, dim...; reverse=false, name)  Reshape SymbolicNode operator  Some dimensions of the shape can take special values from the set {0, -1, -2, -3, -4}. The significance of each is explained below:    0   copy this dimension from the input to the output shape.  Example:   input shape = (2,3,4), shape = (4,0,2), output shape = (4,3,2)  input shape = (2,3,4), shape = (2,0,0), output shape = (2,3,4)  -1  infers the dimension of the output shape by using the remainder of the input dimensions keeping the size of the new array same as that of the input array. At most one dimension of shape can be -1.   Example:   input shape = (2,3,4), shape = (6,1,-1), output shape = (6,1,4)  input shape = (2,3,4), shape = (3,-1,8), output shape = (3,1,8)  input shape = (2,3,4), shape=(-1,), output shape = (24,)  -2  copy all/remainder of the input dimensions to the output shape.   Example:   input shape = (2,3,4), shape = (-2,), output shape = (2,3,4)  input shape = (2,3,4), shape = (2,-2), output shape = (2,3,4)  input shape = (2,3,4), shape = (-2,1,1), output shape = (2,3,4,1,1)  -3  use the product of two consecutive dimensions of the input shape as the output dimension.   Example:   input shape = (2,3,4), shape = (-3,4), output shape = (6,4)  input shape = (2,3,4,5), shape = (-3,-3), output shape = (6,20)  input shape = (2,3,4), shape = (0,-3), output shape = (2,12)  input shape = (2,3,4), shape = (-3,-2), output shape = (6,4)  -4  split one dimension of the input into two dimensions passed subsequent to -4 in shape (can contain -1).   Example:   input shape = (2,3,4), shape = (-4,1,2,-2), output shape = (1,2,3,4)  input shape = (2,3,4), shape = (2,-4,-1,3,-2), output shape = (2,1,3,4)     If the argument  reverse  is set to  1 , then the special values are inferred from right to left.  Example:   with  reverse=false , for input shape = (10,5,4), shape = (-1,0), output shape would be (40,5)  with  reverse=true , output shape will be (50,4).   source  #  Base.sin     Method .  sin(data)  Computes the element-wise sine of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    sin([0, \\pi/4, \\pi/2]) = [0, 0.707, 1]  The storage type of $sin$ output depends upon the input storage type:   sin(default) = default  sin(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L46  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sinh     Method .  sinh(data)  Returns the hyperbolic sine of the input array, computed element-wise.  .. math::    sinh(x) = 0.5\\times(exp(x) - exp(-x))  The storage type of $sinh$ output depends upon the input storage type:   sinh(default) = default  sinh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L201  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.tan     Method .  tan(data)  Computes the element-wise tangent of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    tan([0, \\pi/4, \\pi/2]) = [0, 1, -inf]  The storage type of $tan$ output depends upon the input storage type:   tan(default) = default  tan(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L83  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.tanh     Method .  tanh(data)  Returns the hyperbolic tangent of the input array, computed element-wise.  .. math::    tanh(x) = sinh(x) / cosh(x)  The storage type of $tanh$ output depends upon the input storage type:   tanh(default) = default  tanh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L234  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Variable     Method .  Variable(name :: Union{Symbol, AbstractString})  Create a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.  Arguments   Dict{Symbol, AbstractString} attrs: The attributes associated with this  Variable .   source  #  MXNet.mx.arccos     Method .  arccos(data)  Returns element-wise inverse cosine of the input array.  The input should be in range  [-1, 1] . The output is in the closed interval :math: [0, \\pi]  .. math::    arccos([-1, -.707, 0, .707, 1]) = [\\pi, 3\\pi/4, \\pi/2, \\pi/4, 0]  The storage type of $arccos$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L123  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arccosh     Method .  arccosh(data)  Returns the element-wise inverse hyperbolic cosine of the input array, \ncomputed element-wise.  The storage type of $arccosh$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L264  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arcsin     Method .  arcsin(data)  Returns element-wise inverse sine of the input array.  The input should be in the range  [-1, 1] . The output is in the closed interval of [:math: -\\pi/2 , :math: \\pi/2 ].  .. math::    arcsin([-1, -.707, 0, .707, 1]) = [-\\pi/2, -\\pi/4, 0, \\pi/4, \\pi/2]  The storage type of $arcsin$ output depends upon the input storage type:   arcsin(default) = default  arcsin(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L104  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arcsinh     Method .  arcsinh(data)  Returns the element-wise inverse hyperbolic sine of the input array, \ncomputed element-wise.  The storage type of $arcsinh$ output depends upon the input storage type:   arcsinh(default) = default  arcsinh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L250  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arctan     Method .  arctan(data)  Returns element-wise inverse tangent of the input array.  The output is in the closed interval :math: [-\\pi/2, \\pi/2]  .. math::    arctan([-1, 0, 1]) = [-\\pi/4, 0, \\pi/4]  The storage type of $arctan$ output depends upon the input storage type:   arctan(default) = default  arctan(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L144  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.arctanh     Method .  arctanh(data)  Returns the element-wise inverse hyperbolic tangent of the input array, \ncomputed element-wise.  The storage type of $arctanh$ output depends upon the input storage type:   arctanh(default) = default  arctanh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L281  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.clip     Method .  clip(data, a_min, a_max)  Clips (limits) the values in an array.  Given an interval, values outside the interval are clipped to the interval edges. Clipping $x$ between  a_min  and  a_x  would be::  clip(x, a_min, a_max) = max(min(x, a_max), a_min))  Example::  x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nclip(x,1,8) = [ 1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  8.]  The storage type of $clip$ output depends on storage types of inputs and the a_min, a_max \nparameter values:   clip(default) = default  clip(row_sparse, a_min  = 0, a_max  = 0) = row_sparse  clip(csr, a_min  = 0, a_max  = 0) = csr  clip(row_sparse, a_min   0, a_max   0) = default  clip(row_sparse, a_min   0, a_max   0) = default  clip(csr, a_min   0, a_max   0) = csr  clip(csr, a_min   0, a_max   0) = csr   Defined in src/operator/tensor/matrix_op.cc:L486  Arguments   data::NDArray-or-SymbolicNode : Input array.  a_min::float, required : Minimum value  a_max::float, required : Maximum value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.expand_dims     Method .  expand_dims(data, axis)  Inserts a new axis of size 1 into the array shape  For example, given $x$ with shape $(2,3,4)$, then $expand_dims(x, axis=1)$ will return a new array with shape $(2,1,3,4)$.  Defined in src/operator/tensor/matrix_op.cc:L289  Arguments   data::NDArray-or-SymbolicNode : Source input  axis::int, required : Position where new axis is to be inserted. Suppose that the input  NDArray 's dimension is  ndim , the range of the inserted axis is  [-ndim, ndim]  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.log_softmax     Method .  log_softmax(data, axis)  Computes the log softmax of the input. This is equivalent to computing softmax followed by log.  Examples::     x = mx.nd.array([1, 2, .1]) mx.nd.log_softmax(x).asnumpy()     array([-1.41702998, -0.41702995, -2.31702995], dtype=float32)     x = mx.nd.array( [[1, 2, .1],[.1, 2, 1]] ) mx.nd.log_softmax(x, axis=0).asnumpy()     array([[-0.34115392, -0.69314718, -1.24115396],          [-1.24115396, -0.69314718, -0.34115392]], dtype=float32)  Arguments   data::NDArray-or-SymbolicNode : The input array.  axis::int, optional, default='-1' : The axis along which to compute softmax.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.relu     Method .  relu(data)  Computes rectified linear.  .. math::    max(features, 0)  The storage type of $relu$ output depends upon the input storage type:   relu(default) = default  relu(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L84  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sigmoid     Method .  sigmoid(data)  Computes sigmoid of x element-wise.  .. math::    y = 1 / (1 + exp(-x))  The storage type of $sigmoid$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L103  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.softmax     Method .  softmax(data, axis)  Applies the softmax function.  The resulting array contains elements in the range (0,1) and the elements along the given axis sum up to 1.  .. math::    softmax(\\mathbf{z}) j = \\frac{e^{z_j}}{\\sum ^K e^{z_k}}  for :math: j = 1, ..., K  Example::  x = [[ 1.  1.  1.]        [ 1.  1.  1.]]  softmax(x,axis=0) = [[ 0.5  0.5  0.5]                        [ 0.5  0.5  0.5]]  softmax(x,axis=1) = [[ 0.33333334,  0.33333334,  0.33333334],                        [ 0.33333334,  0.33333334,  0.33333334]]  Defined in src/operator/nn/softmax.cc:L54  Arguments   data::NDArray-or-SymbolicNode : The input array.  axis::int, optional, default='-1' : The axis along which to compute softmax.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.@var     Macro .  @var  symbols ...  A handy macro for creating  mx.Variable .  julia  x = @mx.var x\nMXNet.mx.SymbolicNode x\n\njulia  x, y, z = @mx.var x y z\n(MXNet.mx.SymbolicNode x, MXNet.mx.SymbolicNode y, MXNet.mx.SymbolicNode z)  source  #  Base.Iterators.Flatten     Method .  Flatten(data)  Flattens the input array into a 2-D array by collapsing the higher dimensions.  .. note::  Flatten  is deprecated. Use  flatten  instead.  For an input array with shape $(d1, d2, ..., dk)$,  flatten  operation reshapes the input array into an output array of shape $(d1, d2 ... dk)$.  Example::  x = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]  Defined in src/operator/tensor/matrix_op.cc:L208  Arguments   data::NDArray-or-SymbolicNode : Input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.:*     Method .  .*(x, y)  Elementwise multiplication of  SymbolicNode .  source  #  Base.:+     Method .  +(args...)\n.+(args...)  Elementwise summation of  SymbolicNode .  source  #  Base.:-     Method .  -(x, y)\n.-(x, y)  Elementwise substraction of  SymbolicNode . Operating with  Real  is available.  source  #  Base.:/     Method .  ./(x, y)   Elementwise dividing a  SymbolicNode  by a scalar or another  SymbolicNode   of the same shape.   Elementwise divide a scalar by an  SymbolicNode .  Matrix division (solving linear systems) is not implemented yet.   source  #  Base.:^     Function .  .^(x, y)  Elementwise power of  SymbolicNode  and  NDArray . Operating with  Real  is available.  source  #  Base.LinAlg.dot     Method .  dot(lhs, rhs, transpose_a, transpose_b)  Dot product of two arrays.  $dot$'s behavior depends on the input array dimensions:   1-D arrays: inner product of vectors  2-D arrays: matrix multiplication   N-D arrays: a sum product over the last axis of the first input and the first axis of the second input  For example, given 3-D $x$ with shape  (n,m,k)  and $y$ with shape  (k,r,s) , the result array will have shape  (n,m,r,s) . It is computed by::  dot(x,y)[i,j,a,b] = sum(x[i,j,:]*y[:,a,b])  Example::  x = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))   y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))   dot(x,y)[0,0,1,1] = 0   sum(x[0,0,:]*y[:,1,1]) = 0    The storage type of $dot$ output depends on storage types of inputs and transpose options:   dot(csr, default) = default  dot(csr.T, default) = row_sparse  dot(csr, row_sparse) = default  dot(default, csr) = csr  otherwise, $dot$ generates output with default storage   Defined in src/operator/tensor/dot.cc:L62  Arguments   lhs::NDArray-or-SymbolicNode : The first input  rhs::NDArray-or-SymbolicNode : The second input  transpose_a::boolean, optional, default=0 : If true then transpose the first input before dot.  transpose_b::boolean, optional, default=0 : If true then transpose the second input before dot.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.LinAlg.norm     Method .  norm(data)  Flattens the input array and then computes the l2 norm.  Examples::  x = [[1, 2],        [3, 4]]  norm(x) = [5.47722578]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L257  Arguments   data::NDArray-or-SymbolicNode : Source input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.Math.cbrt     Method .  cbrt(data)  Returns element-wise cube-root value of the input.  .. math::    cbrt(x) = \\sqrt[3]{x}  Example::  cbrt([1, 8, -125]) = [1, 2, -5]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L602  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.Math.gamma     Method .  gamma(data)  Returns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.  The storage type of $gamma$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base._div     Method .  _div(lhs, rhs)  _div is an alias of elemwise_div.  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base._sub     Method .  _sub(lhs, rhs)  _sub is an alias of elemwise_sub.  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.abs     Method .  abs(data)  Returns element-wise absolute value of the input.  Example::  abs([-2, 0, 3]) = [2, 0, 3]  The storage type of $abs$ output depends upon the input storage type:   abs(default) = default  abs(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L386  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.ceil     Method .  ceil(data)  Returns element-wise ceiling of the input.  The ceil of the scalar x is the smallest integer i, such that i  = x.  Example::  ceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]  The storage type of $ceil$ output depends upon the input storage type:   ceil(default) = default  ceil(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L464  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.copy     Method .  copy(self :: SymbolicNode)  Make a copy of a SymbolicNode. The same as making a deep copy.  source  #  Base.deepcopy     Method .  deepcopy(self :: SymbolicNode)  Make a deep copy of a SymbolicNode.  source  #  Base.exp     Method .  exp(data)  Returns element-wise exponential value of the input.  .. math::    exp(x) = e^x \\approx 2.718^x  Example::  exp([0, 1, 2]) = [1., 2.71828175, 7.38905621]  The storage type of $exp$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L642  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.expm1     Method .  expm1(data)  Returns $exp(x) - 1$ computed element-wise on the input.  This function provides greater precision than $exp(x) - 1$ for small values of $x$.  The storage type of $expm1$ output depends upon the input storage type:   expm1(default) = default  expm1(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L721  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.floor     Method .  floor(data)  Returns element-wise floor of the input.  The floor of the scalar x is the largest integer i, such that i  = x.  Example::  floor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]  The storage type of $floor$ output depends upon the input storage type:   floor(default) = default  floor(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L483  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.getindex     Method .  getindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})  Get a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of  list_outputs .  source  #  Base.identity     Method .  identity(data)  identity is an alias of _copy.  Returns a copy of the input.  From:src/operator/tensor/elemwise_unary_op_basic.cc:112  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log     Method .  log(data)  Returns element-wise Natural logarithmic value of the input.  The natural logarithm is logarithm in base  e , so that $log(exp(x)) = x$  The storage type of $log$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L654  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log10     Method .  log10(data)  Returns element-wise Base-10 logarithmic value of the input.  $10**log10(x) = x$  The storage type of $log10$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L666  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log1p     Method .  log1p(data)  Returns element-wise $log(1 + x)$ value of the input.  This function is more accurate than $log(1 + x)$  for small $x$ so that :math: 1+x\\approx 1  The storage type of $log1p$ output depends upon the input storage type:   log1p(default) = default  log1p(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L703  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.log2     Method .  log2(data)  Returns element-wise Base-2 logarithmic value of the input.  $2**log2(x) = x$  The storage type of $log2$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L678  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.mean     Method .  mean(data, axis, keepdims, exclude)  Computes the mean of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L101  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  Base.print     Function .  print([io::IO], sym::SymbolicNode)  Print the content of symbol, used for debug.  julia  layer = @mx.chain mx.Variable(:data)           = \n         mx.FullyConnected(name=:fc1, num_hidden=128) = \n         mx.Activation(name=:relu1, act_type=:relu)\nMXNet.mx.SymbolicNode(MXNet.mx.MX_SymbolHandle(Ptr{Void} @0x000055b29b9c3520))\n\njulia  print(layer)\nSymbol Outputs:\n        output[0]=relu1(0)\nVariable:data\nVariable:fc1_weight\nVariable:fc1_bias\n--------------------\nOp:FullyConnected, Name=fc1\nInputs:\n        arg[0]=data(0) version=0\n        arg[1]=fc1_weight(0) version=0\n        arg[2]=fc1_bias(0) version=0\nAttrs:\n        num_hidden=128\n--------------------\nOp:Activation, Name=relu1\nInputs:\n        arg[0]=fc1(0)\nAttrs:\n        act_type=relu  source  #  Base.prod     Method .  prod(data, axis, keepdims, exclude)  Computes the product of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L116  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  Base.repeat     Method .  repeat(data, repeats, axis)  Repeats elements of an array.  By default, $repeat$ flattens the input array into 1-D and then repeats the elements::  x = [[ 1, 2],        [ 3, 4]]  repeat(x, repeats=2) = [ 1.,  1.,  2.,  2.,  3.,  3.,  4.,  4.]  The parameter $axis$ specifies the axis along which to perform repeat::  repeat(x, repeats=2, axis=1) = [[ 1.,  1.,  2.,  2.],                                   [ 3.,  3.,  4.,  4.]]  repeat(x, repeats=2, axis=0) = [[ 1.,  2.],                                   [ 1.,  2.],                                   [ 3.,  4.],                                   [ 3.,  4.]]  repeat(x, repeats=2, axis=-1) = [[ 1.,  1.,  2.,  2.],                                    [ 3.,  3.,  4.,  4.]]  Defined in src/operator/tensor/matrix_op.cc:L560  Arguments   data::NDArray-or-SymbolicNode : Input data array  repeats::int, required : The number of repetitions for each element.  axis::int or None, optional, default='None' : The axis along which to repeat values. The negative numbers are interpreted counting from the backward. By default, use the flattened input array, and return a flat output array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.reverse     Method .  reverse(data, axis)  Reverses the order of elements along given axis while preserving array shape.  Note: reverse and flip are equivalent. We use reverse in the following examples.  Examples::  x = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]  reverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]  reverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]  Defined in src/operator/tensor/matrix_op.cc:L662  Arguments   data::NDArray-or-SymbolicNode : Input data array  axis::Shape(tuple), required : The axis which to reverse elements.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.round     Method .  round(data)  Returns element-wise rounded value to the nearest integer of the input.  Example::  round([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]  The storage type of $round$ output depends upon the input storage type:   round(default) = default  round(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L424  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sign     Method .  sign(data)  Returns element-wise sign of the input.  Example::  sign([-2, 0, 3]) = [-1, 0, 1]  The storage type of $sign$ output depends upon the input storage type:   sign(default) = default  sign(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L405  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sort     Method .  sort(data, axis, is_ascend)  Returns a sorted copy of an input array along the given axis.  Examples::  x = [[ 1, 4],        [ 3, 1]]  // sorts along the last axis   sort(x) = [[ 1.,  4.],              [ 1.,  3.]]  // flattens and then sorts   sort(x) = [ 1.,  1.,  3.,  4.]  // sorts along the first axis   sort(x, axis=0) = [[ 1.,  1.],                      [ 3.,  4.]]  // in a descend order   sort(x, is_ascend=0) = [[ 4.,  1.],                           [ 3.,  1.]]  Defined in src/operator/tensor/ordering_op.cc:L126  Arguments   data::NDArray-or-SymbolicNode : The input array  axis::int or None, optional, default='-1' : Axis along which to choose sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=1 : Whether to sort in ascending or descending order.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.split     Method .  split(data, num_outputs, axis, squeeze_axis)  split is an alias of SliceChannel.  Splits an array along a particular axis into multiple sub-arrays.  .. note:: $SliceChannel$ is deprecated. Use $split$ instead.  Note  that  num_outputs  should evenly divide the length of the axis along which to split the array.  Example::  x  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)  y = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]     [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]  y[0].shape = (3, 1, 1)  z = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]     [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]  z[0].shape = (1, 2, 1)  squeeze_axis=1  removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $1$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to true only if $input.shape[axis] == num_outputs$.  Example::  z = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]     [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]  z[0].shape = (2 ,1 )  Defined in src/operator/slice_channel.cc:L107  Arguments   data::NDArray-or-SymbolicNode : The input  num_outputs::int, required : Number of splits. Note that this should evenly divide the length of the  axis .  axis::int, optional, default='1' : Axis along which to split.  squeeze_axis::boolean, optional, default=0 : If true, Removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $true$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to $true$ only if $input.shape[axis] == num_outputs$.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sqrt     Method .  sqrt(data)  Returns element-wise square-root value of the input.  .. math::    \\textrm{sqrt}(x) = \\sqrt{x}  Example::  sqrt([4, 9, 16]) = [2, 3, 4]  The storage type of $sqrt$ output depends upon the input storage type:   sqrt(default) = default  sqrt(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L565  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.sum     Method .  sum(data, axis, keepdims, exclude)  Computes the sum of array elements over given axes.  .. Note::  sum  and  sum_axis  are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.  Example::  data = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]  sum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]  sum(data, axis=[1,2])   [ 12.  19.  27.]  data = [[1,2,0],           [3,0,1],           [4,1,0]]  csr = cast_storage(data, 'csr')  sum(csr, axis=0)   [ 8.  3.  1.]  sum(csr, axis=1)   [ 3.  4.  5.]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L85  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  Base.transpose     Method .  transpose(data, axes)  Permutes the dimensions of an array.  Examples::  x = [[ 1, 2],        [ 3, 4]]  transpose(x) = [[ 1.,  3.],                   [ 2.,  4.]]  x = [[[ 1.,  2.],         [ 3.,  4.]],     [[ 5.,  6.],\n    [ 7.,  8.]]]  transpose(x) = [[[ 1.,  5.],                    [ 3.,  7.]],                [[ 2.,  6.],\n               [ 4.,  8.]]]  transpose(x, axes=(1,0,2)) = [[[ 1.,  2.],                                  [ 5.,  6.]],                              [[ 3.,  4.],\n                             [ 7.,  8.]]]  Defined in src/operator/tensor/matrix_op.cc:L253  Arguments   data::NDArray-or-SymbolicNode : Source input  axes::Shape(tuple), optional, default=[] : Target axis order. By default the axes will be inverted.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  Base.trunc     Method .  trunc(data)  Return the element-wise truncated value of the input.  The truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.  Example::  trunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]  The storage type of $trunc$ output depends upon the input storage type:   trunc(default) = default  trunc(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L503  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Activation     Method .  Activation(data, act_type)  Applies an activation function element-wise to the input.  The following activation functions are supported:   relu : Rectified Linear Unit, :math: y = max(x, 0)  sigmoid : :math: y = \\frac{1}{1 + exp(-x)}  tanh : Hyperbolic tangent, :math: y = \\frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}  softrelu : Soft ReLU, or SoftPlus, :math: y = log(1 + exp(x))   Defined in src/operator/nn/activation.cc:L92  Arguments   data::NDArray-or-SymbolicNode : Input array to activation function.  act_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required : Activation function to be applied.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.BatchNorm     Method .  BatchNorm(data, gamma, beta, moving_mean, moving_var, eps, momentum, fix_gamma, use_global_stats, output_mean_var, axis, cudnn_off)  Batch normalization.  Normalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.  Assume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:  .. math::  data_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])  Then compute the normalized output, which has the same shape as input, as following:  .. math::  out[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]  Both  mean  and  var  returns a scalar by treating the input as a vector.  Assume the input has size  k  on axis 1, then both $gamma$ and $beta$ have shape  (k,) . If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.  Besides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are  k -length vectors. They are global statistics for the whole dataset, which are updated by::  moving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)  If $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.  The parameter $axis$ specifies which axis of the input shape denotes the 'channel' (separately normalized groups).  The default is 1.  Specifying -1 sets the channel axis to be the last item in the input shape.  Both $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.  Defined in src/operator/nn/batch_norm.cc:L400  Arguments   data::NDArray-or-SymbolicNode : Input data to batch normalization  gamma::NDArray-or-SymbolicNode : gamma array  beta::NDArray-or-SymbolicNode : beta array  moving_mean::NDArray-or-SymbolicNode : running mean of input  moving_var::NDArray-or-SymbolicNode : running variance of input  eps::double, optional, default=0.001 : Epsilon to prevent div 0. Must be no less than CUDNN_BN_MIN_EPSILON defined in cudnn.h when using cudnn (usually 1e-5)  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=1 : Fix gamma while training  use_global_stats::boolean, optional, default=0 : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.  output_mean_var::boolean, optional, default=0 : Output All,normal mean and var  axis::int, optional, default='1' : Specify which shape axis the channel is specified  cudnn_off::boolean, optional, default=0 : Do not select CUDNN operator, if available  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.BatchNorm_v1     Method .  BatchNorm_v1(data, gamma, beta, eps, momentum, fix_gamma, use_global_stats, output_mean_var)  Batch normalization.  Normalizes a data batch by mean and variance, and applies a scale $gamma$ as well as offset $beta$.  Assume the input has more than one dimension and we normalize along axis 1. We first compute the mean and variance along this axis:  .. math::  data_mean[i] = mean(data[:,i,:,...]) \\\n  data_var[i] = var(data[:,i,:,...])  Then compute the normalized output, which has the same shape as input, as following:  .. math::  out[:,i,:,...] = \\frac{data[:,i,:,...] - data_mean[i]}{\\sqrt{data_var[i]+\\epsilon}} * gamma[i] + beta[i]  Both  mean  and  var  returns a scalar by treating the input as a vector.  Assume the input has size  k  on axis 1, then both $gamma$ and $beta$ have shape  (k,) . If $output_mean_var$ is set to be true, then outputs both $data_mean$ and $data_var$ as well, which are needed for the backward pass.  Besides the inputs and the outputs, this operator accepts two auxiliary states, $moving_mean$ and $moving_var$, which are  k -length vectors. They are global statistics for the whole dataset, which are updated by::  moving_mean = moving_mean * momentum + data_mean * (1 - momentum)   moving_var = moving_var * momentum + data_var * (1 - momentum)  If $use_global_stats$ is set to be true, then $moving_mean$ and $moving_var$ are used instead of $data_mean$ and $data_var$ to compute the output. It is often used during inference.  Both $gamma$ and $beta$ are learnable parameters. But if $fix_gamma$ is true, then set $gamma$ to 1 and its gradient to 0.  Defined in src/operator/batch_norm_v1.cc:L90  Arguments   data::NDArray-or-SymbolicNode : Input data to batch normalization  gamma::NDArray-or-SymbolicNode : gamma array  beta::NDArray-or-SymbolicNode : beta array  eps::float, optional, default=0.001 : Epsilon to prevent div 0  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=1 : Fix gamma while training  use_global_stats::boolean, optional, default=0 : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.  output_mean_var::boolean, optional, default=0 : Output All,normal mean and var  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.BilinearSampler     Method .  BilinearSampler(data, grid)  Applies bilinear sampling to input feature map.  Bilinear Sampling is the key of  [NIPS2015] \\\"Spatial Transformer Networks\\\". The usage of the operator is very similar to remap function in OpenCV, except that the operator has the backward pass.  Given :math: data  and :math: grid , then the output is computed by  .. math::   x_{src} = grid[batch, 0, y_{dst}, x_{dst}] \\\n  y_{src} = grid[batch, 1, y_{dst}, x_{dst}] \\\n  output[batch, channel, y_{dst}, x_{dst}] = G(data[batch, channel, y_{src}, x_{src})  :math: x_{dst} , :math: y_{dst}  enumerate all spatial locations in :math: output , and :math: G()  denotes the bilinear interpolation kernel. The out-boundary points will be padded with zeros.The shape of the output will be (data.shape[0], data.shape[1], grid.shape[2], grid.shape[3]).  The operator assumes that :math: data  has 'NCHW' layout and :math: grid  has been normalized to [-1, 1].  BilinearSampler often cooperates with GridGenerator which generates sampling grids for BilinearSampler. GridGenerator supports two kinds of transformation: $affine$ and $warp$. If users want to design a CustomOp to manipulate :math: grid , please firstly refer to the code of GridGenerator.  Example 1::  Zoom out data two times  data = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])  affine_matrix = array([[2, 0, 0],                          [0, 2, 0]])  affine_matrix = reshape(affine_matrix, shape=(1, 6))  grid = GridGenerator(data=affine_matrix, transform_type='affine', target_shape=(4, 4))  out = BilinearSampler(data, grid)  out   [[[[ 0,   0,     0,   0],      [ 0,   3.5,   6.5, 0],      [ 0,   1.25,  2.5, 0],      [ 0,   0,     0,   0]]]  Example 2::  shift data horizontally by -1 pixel  data = array([[[[1, 4, 3, 6],                   [1, 8, 8, 9],                   [0, 4, 1, 5],                   [1, 0, 1, 3]]]])  warp_maxtrix = array([[[[1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1],                           [1, 1, 1, 1]],                          [[0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0],                           [0, 0, 0, 0]]]])  grid = GridGenerator(data=warp_matrix, transform_type='warp')   out = BilinearSampler(data, grid)  out   [[[[ 4,  3,  6,  0],      [ 8,  8,  9,  0],      [ 4,  1,  5,  0],      [ 0,  1,  3,  0]]]  Defined in src/operator/bilinear_sampler.cc:L245  Arguments   data::NDArray-or-SymbolicNode : Input data to the BilinearsamplerOp.  grid::NDArray-or-SymbolicNode : Input grid to the BilinearsamplerOp.grid has two channels: x_src, y_src  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.BlockGrad     Method .  BlockGrad(data)  Stops gradient computation.  Stops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.  Example::  v1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)  executor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]  executor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L167  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Cast     Method .  Cast(data, dtype)  Casts all elements of the input to a new type.  .. note:: $Cast$ is deprecated. Use $cast$ instead.  Example::  cast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L311  Arguments   data::NDArray-or-SymbolicNode : The input.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Output data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Concat     Method .  Concat(data, num_args, dim)  Note : Concat takes variable number of positional inputs. So instead of calling as Concat([x, y, z], num_args=3), one should call via Concat(x, y, z), and num_args will be determined automatically.  Joins input arrays along a given axis.  .. note::  Concat  is deprecated. Use  concat  instead.  The dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.  Example::  x = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]  concat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]  Note that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.  concat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]  Defined in src/operator/concat.cc:L104  Arguments   data::NDArray-or-SymbolicNode[] : List of arrays to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Convolution     Method .  Convolution(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  Compute  N -D convolution on  (N+2) -D input.  In the 2-D convolution, given input data with shape  (batch_size, channel, height, width) , the output is computed by  .. math::  out[n,i,:,:] = bias[i] + \\sum_{j=0}^{channel} data[n,j,:,:] \\star    weight[i,j,:,:]  where :math: \\star  is the 2-D cross-correlation operator.  For general 2-D convolution, the shapes are   data :  (batch_size, channel, height, width)  weight :  (num_filter, channel, kernel[0], kernel[1])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_height, out_width) .   Define::  f(x,k,p,s,d) = floor((x+2 p-d (k-1)-1)/s)+1  then we have::  out_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])  If $no_bias$ is set to be true, then the $bias$ term is ignored.  The default data $layout$ is  NCHW , namely  (batch_size, channel, height, width) . We can choose other layouts such as  NHWC .  If $num_group$ is larger than 1, denoted by  g , then split the input $data$ evenly into  g  parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the  i -th part of the data with the  i -th weight part. The output is obtained by concatenating all the  g  results.  1-D convolution does not have  height  dimension but only  width  in space.   data :  (batch_size, channel, width)  weight :  (num_filter, channel, kernel[0])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_width) .   3-D convolution adds an additional  depth  dimension besides  height  and  width . The shapes are   data :  (batch_size, channel, depth, height, width)  weight :  (num_filter, channel, kernel[0], kernel[1], kernel[2])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_depth, out_height, out_width) .   Both $weight$ and $bias$ are learnable parameters.  There are other options to tune the performance.    cudnn_tune : enable this option leads to higher startup time but may give faster speed. Options are   off : no tuning  limited_workspace :run test and pick the fastest algorithm that doesn't exceed workspace limit.  fastest : pick the fastest algorithm and ignore workspace limit.  None  (default): the behavior is determined by environment variable $MXNET_CUDNN_AUTOTUNE_DEFAULT$. 0 for off, 1 for limited workspace (default), 2 for fastest.  workspace : A large number leads to more (GPU) memory usage but may improve the performance.     Defined in src/operator/nn/convolution.cc:L170  Arguments   data::NDArray-or-SymbolicNode : Input data to the ConvolutionOp.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : Convolution kernel size: (w,), (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=[] : Convolution stride: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  dilate::Shape(tuple), optional, default=[] : Convolution dilate: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : Zero pad for convolution: (w,), (h, w) or (d, h, w). Defaults to no padding.  num_filter::int (non-negative), required : Convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions.  workspace::long (non-negative), optional, default=1024 : Maximum temporary workspace allowed for convolution (MB).  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algo by running performance test.  cudnn_off::boolean, optional, default=0 : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Convolution_v1     Method .  Convolution_v1(data, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  This operator is DEPRECATED. Apply convolution to input then add a bias.  Arguments   data::NDArray-or-SymbolicNode : Input data to the ConvolutionV1Op.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : convolution kernel size: (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=[] : convolution stride: (h, w) or (d, h, w)  dilate::Shape(tuple), optional, default=[] : convolution dilate: (h, w) or (d, h, w)  pad::Shape(tuple), optional, default=[] : pad for convolution: (h, w) or (d, h, w)  num_filter::int (non-negative), required : convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions. Equivalent to slicing input into num_group   partitions, apply convolution on each, then concatenate the results  workspace::long (non-negative), optional, default=1024 : Maximum tmp workspace allowed for convolution (MB).  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algo by running performance test.   Leads to higher startup time but may give faster speed. Options are:   'off': no tuning   'limited_workspace': run test and pick the fastest algorithm that doesn't exceed workspace limit.   'fastest': pick the fastest algorithm and ignore workspace limit.   If set to None (default), behavior is determined by environment   variable MXNET_CUDNN_AUTOTUNE_DEFAULT: 0 for off,   1 for limited workspace (default), 2 for fastest.  cudnn_off::boolean, optional, default=0 : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCHW for 2d and NCDHW for 3d.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Correlation     Method .  Correlation(data1, data2, kernel_size, max_displacement, stride1, stride2, pad_size, is_multiply)  Applies correlation to inputs.  The correlation layer performs multiplicative patch comparisons between two feature maps.  Given two multi-channel feature maps :math: f_{1}, f_{2} , with :math: w , :math: h , and :math: c  being their width, height, and number of channels, the correlation layer lets the network compare each patch from :math: f_{1}  with each patch from :math: f_{2} .  For now we consider only a single comparison of two patches. The 'correlation' of two patches centered at :math: x_{1}  in the first map and :math: x_{2}  in the second map is then defined as:  .. math::    c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k] \\times [-k,k]}   for a square patch of size :math: K:=2k+1 .  Note that the equation above is identical to one step of a convolution in neural networks, but instead of convolving data with a filter, it convolves data with other data. For this reason, it has no training weights.  Computing :math: c(x_{1}, x_{2})  involves :math: c * K^{2}  multiplications. Comparing all patch combinations involves :math: w^{2}*h^{2}  such computations.  Given a maximum displacement :math: d , for each location :math: x_{1}  it computes correlations :math: c(x_{1}, x_{2})  only in a neighborhood of size :math: D:=2d+1 , by limiting the range of :math: x_{2} . We use strides :math: s_{1}, s_{2} , to quantize :math: x_{1}  globally and to quantize :math: x_{2}  within the neighborhood centered around :math: x_{1} .  The final output is defined by the following expression:  .. math::   out[n, q, i, j] = c(x_{i, j}, x_{q})  where :math: i  and :math: j  enumerate spatial locations in :math: f_{1} , and :math: q  denotes the :math: q^{th}  neighborhood of :math: x_{i,j} .  Defined in src/operator/correlation.cc:L192  Arguments   data1::NDArray-or-SymbolicNode : Input data1 to the correlation.  data2::NDArray-or-SymbolicNode : Input data2 to the correlation.  kernel_size::int (non-negative), optional, default=1 : kernel size for Correlation must be an odd number  max_displacement::int (non-negative), optional, default=1 : Max displacement of Correlation  stride1::int (non-negative), optional, default=1 : stride1 quantize data1 globally  stride2::int (non-negative), optional, default=1 : stride2 quantize data2 within the neighborhood centered around data1  pad_size::int (non-negative), optional, default=0 : pad for Correlation  is_multiply::boolean, optional, default=1 : operation type is either multiplication or subduction  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Crop     Method .  Crop(data, num_args, offset, h_w, center_crop)  Note : Crop takes variable number of positional inputs. So instead of calling as Crop([x, y, z], num_args=3), one should call via Crop(x, y, z), and num_args will be determined automatically.  .. note::  Crop  is deprecated. Use  slice  instead.  Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used  Defined in src/operator/crop.cc:L50  Arguments   data::SymbolicNode or SymbolicNode[] : Tensor or List of Tensors, the second input will be used as crop_like shape reference  num_args::int, required : Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here  offset::Shape(tuple), optional, default=[0,0] : crop offset coordinate: (y, x)  h_w::Shape(tuple), optional, default=[0,0] : crop height and width: (h, w)  center_crop::boolean, optional, default=0 : If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Custom     Method .  Custom(data, op_type)  Apply a custom operator implemented in a frontend language (like Python).  Custom operators should override required methods like  forward  and  backward . The custom operator must be registered before it can be used. Please check the tutorial here: http://mxnet.io/how_to/new_op.html.  Defined in src/operator/custom/custom.cc:L378  Arguments   data::NDArray-or-SymbolicNode[] : Input data for the custom operator.  op_type::string : Name of the custom operator. This is the name that is passed to  mx.operator.register  to register the operator.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Deconvolution     Method .  Deconvolution(data, weight, bias, kernel, stride, dilate, pad, adj, target_shape, num_filter, num_group, workspace, no_bias, cudnn_tune, cudnn_off, layout)  Computes 1D or 2D transposed convolution (aka fractionally strided convolution) of the input tensor. This operation can be seen as the gradient of Convolution operation with respect to its input. Convolution usually reduces the size of the input. Transposed convolution works the other way, going from a smaller input to a larger output while preserving the connectivity pattern.  Arguments   data::NDArray-or-SymbolicNode : Input tensor to the deconvolution operation.  weight::NDArray-or-SymbolicNode : Weights representing the kernel.  bias::NDArray-or-SymbolicNode : Bias added to the result after the deconvolution operation.  kernel::Shape(tuple), required : Deconvolution kernel size: (w,), (h, w) or (d, h, w). This is same as the kernel size used for the corresponding convolution  stride::Shape(tuple), optional, default=[] : The stride used for the corresponding convolution: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  dilate::Shape(tuple), optional, default=[] : Dilation factor for each dimension of the input: (w,), (h, w) or (d, h, w). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : The amount of implicit zero padding added during convolution for each dimension of the input: (w,), (h, w) or (d, h, w). $(kernel-1)/2$ is usually a good choice. If  target_shape  is set,  pad  will be ignored and a padding that will generate the target shape will be used. Defaults to no padding.  adj::Shape(tuple), optional, default=[] : Adjustment for output shape: (w,), (h, w) or (d, h, w). If  target_shape  is set,  adj  will be ignored and computed accordingly.  target_shape::Shape(tuple), optional, default=[] : Shape of the output tensor: (w,), (h, w) or (d, h, w).  num_filter::int (non-negative), required : Number of output filters.  num_group::int (non-negative), optional, default=1 : Number of groups partition.  workspace::long (non-negative), optional, default=512 : Maximum temporal workspace allowed for deconvolution (MB).  no_bias::boolean, optional, default=1 : Whether to disable bias parameter.  cudnn_tune::{None, 'fastest', 'limited_workspace', 'off'},optional, default='None' : Whether to pick convolution algorithm by running performance test.  cudnn_off::boolean, optional, default=0 : Turn off cudnn for this layer.  layout::{None, 'NCDHW', 'NCHW', 'NCW', 'NDHWC', 'NHWC'},optional, default='None' : Set layout for input, output and weight. Empty for default layout, NCW for 1d, NCHW for 2d and NCDHW for 3d.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Dropout     Method .  Dropout(data, p, mode)  Applies dropout operation to input array.   During training, each element of the input is set to zero with probability p. The whole array is rescaled by :math: 1/(1-p)  to keep the expected sum of the input unchanged.  During testing, this operator does not change the input if mode is 'training'. If mode is 'always', the same computaion as during training will be applied.   Example::  random.seed(998)   input_array = array([[3., 0.5,  -0.5,  2., 7.],                       [2., -0.4,   7.,  3., 0.2]])   a = symbol.Variable('a')   dropout = symbol.Dropout(a, p = 0.2)   executor = dropout.simple_bind(a = input_array.shape)  If training  executor.forward(is_train = True, a = input_array)   executor.outputs   [[ 3.75   0.625 -0.     2.5    8.75 ]    [ 2.5   -0.5    8.75   3.75   0.   ]]  If testing  executor.forward(is_train = False, a = input_array)   executor.outputs   [[ 3.     0.5   -0.5    2.     7.   ]    [ 2.    -0.4    7.     3.     0.2  ]]  Defined in src/operator/nn/dropout.cc:L78  Arguments   data::NDArray-or-SymbolicNode : Input array to which dropout will be applied.  p::float, optional, default=0.5 : Fraction of the input that gets dropped out during training time.  mode::{'always', 'training'},optional, default='training' : Whether to only turn on dropout during training or to also turn on for inference.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.ElementWiseSum     Method .  ElementWiseSum(args)  ElementWiseSum is an alias of add_n.  Note : ElementWiseSum takes variable number of positional inputs. So instead of calling as ElementWiseSum([x, y, z], num_args=3), one should call via ElementWiseSum(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Embedding     Method .  Embedding(data, weight, input_dim, output_dim, dtype)  Maps integer indices to vector representations (embeddings).  This operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.  For an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).  If the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).  By default, if any index mentioned is too large, it is replaced by the index that addresses the last vector in an embedding matrix.  Examples::  input_dim = 4   output_dim = 5  // Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]  // Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]  // Mapped input x to its vector representation y.   Embedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                             [ 15.,  16.,  17.,  18.,  19.]],                         [[  0.,   1.,   2.,   3.,   4.],\n                        [ 10.,  11.,  12.,  13.,  14.]]]  Defined in src/operator/tensor/indexing_op.cc:L185  Arguments   data::NDArray-or-SymbolicNode : The input array to the embedding operator.  weight::NDArray-or-SymbolicNode : The embedding weight matrix.  input_dim::int, required : Vocabulary size of the input indices.  output_dim::int, required : Dimension of the embedding vectors.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Data type of weight.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.FullyConnected     Method .  FullyConnected(data, weight, bias, num_hidden, no_bias, flatten)  Applies a linear transformation: :math: Y = XW^T + b .  If $flatten$ is set to be true, then the shapes are:   data :  (batch_size, x1, x2, ..., xn)  weight :  (num_hidden, x1 * x2 * ... * xn)  bias :  (num_hidden,)  out :  (batch_size, num_hidden)   If $flatten$ is set to be false, then the shapes are:   data :  (x1, x2, ..., xn, input_dim)  weight :  (num_hidden, input_dim)  bias :  (num_hidden,)  out :  (x1, x2, ..., xn, num_hidden)   The learnable parameters include both $weight$ and $bias$.  If $no_bias$ is set to be true, then the $bias$ term is ignored.  Defined in src/operator/nn/fully_connected.cc:L98  Arguments   data::NDArray-or-SymbolicNode : Input data.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  num_hidden::int, required : Number of hidden nodes of the output.  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  flatten::boolean, optional, default=1 : Whether to collapse all but the first axis of the input data tensor.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.GridGenerator     Method .  GridGenerator(data, transform_type, target_shape)  Generates 2D sampling grid for bilinear sampling.  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  transform_type::{'affine', 'warp'}, required : The type of transformation. For  affine , input data should be an affine matrix of size (batch, 6). For  warp , input data should be an optical flow of size (batch, 2, h, w).  target_shape::Shape(tuple), optional, default=[0,0] : Specifies the output shape (H, W). This is required if transformation type is  affine . If transformation type is  warp , this parameter is ignored.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Group     Method .  Group(nodes :: SymbolicNode...)  Create a  SymbolicNode  by grouping nodes together.  source  #  MXNet.mx.IdentityAttachKLSparseReg     Method .  IdentityAttachKLSparseReg(data, sparseness_target, penalty, momentum)  Apply a sparse regularization to the output a sigmoid activation function.  Arguments   data::NDArray-or-SymbolicNode : Input data.  sparseness_target::float, optional, default=0.1 : The sparseness target  penalty::float, optional, default=0.001 : The tradeoff parameter for the sparseness penalty  momentum::float, optional, default=0.9 : The momentum for running average  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.InstanceNorm     Method .  InstanceNorm(data, gamma, beta, eps)  Applies instance normalization to the n-dimensional input array.  This operator takes an n-dimensional input array where (n 2) and normalizes the input using the following formula:  .. math::  out = \\frac{x - mean[data]}{ \\sqrt{Var[data]} + \\epsilon} * gamma + beta  This layer is similar to batch normalization layer ( BatchNorm ) with two differences: first, the normalization is carried out per example (instance), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as  contrast normalization .  If the input data is of shape [batch, channel, spacial_dim1, spacial_dim2, ...],  gamma  and  beta  parameters must be vectors of shape [channel].  This implementation is based on paper:  .. [1] Instance Normalization: The Missing Ingredient for Fast Stylization,    D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2).  Examples::  // Input of shape (2,1,2)   x = [[[ 1.1,  2.2]],        [[ 3.3,  4.4]]]  // gamma parameter of length 1   gamma = [1.5]  // beta parameter of length 1   beta = [0.5]  // Instance normalization is calculated with the above formula   InstanceNorm(x,gamma,beta) = [[[-0.997527  ,  1.99752665]],                                 [[-0.99752653,  1.99752724]]]  Defined in src/operator/instance_norm.cc:L95  Arguments   data::NDArray-or-SymbolicNode : An n-dimensional input array (n   2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].  gamma::NDArray-or-SymbolicNode : A vector of length 'channel', which multiplies the normalized input.  beta::NDArray-or-SymbolicNode : A vector of length 'channel', which is added to the product of the normalized input and the weight.  eps::float, optional, default=0.001 : An  epsilon  parameter to prevent division by 0.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.L2Normalization     Method .  L2Normalization(data, eps, mode)  Normalize the input array using the L2 norm.  For 1-D NDArray, it computes::  out = data / sqrt(sum(data ** 2) + eps)  For N-D NDArray, if the input array has shape (N, N, ..., N),  with $mode$ = $instance$, it normalizes each instance in the multidimensional array by its L2 norm.::  for i in 0...N     out[i,:,:,...,:] = data[i,:,:,...,:] / sqrt(sum(data[i,:,:,...,:] ** 2) + eps)  with $mode$ = $channel$, it normalizes each channel in the array by its L2 norm.::  for i in 0...N     out[:,i,:,...,:] = data[:,i,:,...,:] / sqrt(sum(data[:,i,:,...,:] ** 2) + eps)  with $mode$ = $spatial$, it normalizes the cross channel norm for each position in the array by its L2 norm.::  for dim in 2...N     for i in 0...N       out[.....,i,...] = take(out, indices=i, axis=dim) / sqrt(sum(take(out, indices=i, axis=dim) ** 2) + eps)           -dim-  Example::  x = [[[1,2],         [3,4]],        [[2,2],         [5,6]]]  L2Normalization(x, mode='instance')   =[[[ 0.18257418  0.36514837]      [ 0.54772252  0.73029673]]     [[ 0.24077171  0.24077171]      [ 0.60192931  0.72231513]]]  L2Normalization(x, mode='channel')   =[[[ 0.31622776  0.44721359]      [ 0.94868326  0.89442718]]     [[ 0.37139067  0.31622776]      [ 0.92847669  0.94868326]]]  L2Normalization(x, mode='spatial')   =[[[ 0.44721359  0.89442718]      [ 0.60000002  0.80000001]]     [[ 0.70710677  0.70710677]      [ 0.6401844   0.76822126]]]  Defined in src/operator/l2_normalization.cc:L93  Arguments   data::NDArray-or-SymbolicNode : Input array to normalize.  eps::float, optional, default=1e-10 : A small constant for numerical stability.  mode::{'channel', 'instance', 'spatial'},optional, default='instance' : Specify the dimension along which to compute L2 norm.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LRN     Method .  LRN(data, alpha, beta, knorm, nsize)  Applies local response normalization to the input.  The local response normalization layer performs \"lateral inhibition\" by normalizing over local input regions.  If :math: a_{x,y}^{i}  is the activity of a neuron computed by applying kernel :math: i  at position :math: (x, y)  and then applying the ReLU nonlinearity, the response-normalized activity :math: b_{x,y}^{i}  is given by the expression:  .. math::    b_{x,y}^{i} = \\frac{a_{x,y}^{i}}{\\Bigg({k + \\alpha \\sum_{j=max(0, i-\\frac{n}{2})}^{min(N-1, i+\\frac{n}{2})} (a_{x,y}^{j})^{2}}\\Bigg)^{\\beta}}  where the sum runs over :math: n  \"adjacent\" kernel maps at the same spatial position, and :math: N  is the total number of kernels in the layer.  Defined in src/operator/lrn.cc:L73  Arguments   data::NDArray-or-SymbolicNode : Input data.  alpha::float, optional, default=0.0001 : The variance scaling parameter :math: \u0007lpha  in the LRN expression.  beta::float, optional, default=0.75 : The power parameter :math: \beta  in the LRN expression.  knorm::float, optional, default=2 : The parameter :math: k  in the LRN expression.  nsize::int (non-negative), required : normalization window width in elements.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LeakyReLU     Method .  LeakyReLU(data, act_type, slope, lower_bound, upper_bound)  Applies Leaky rectified linear unit activation element-wise to the input.  Leaky ReLUs attempt to fix the \"dying ReLU\" problem by allowing a small  slope  when the input is negative and has a slope of one when input is positive.  The following modified ReLU Activation functions are supported:   elu : Exponential Linear Unit.  y = x   0 ? x : slope * (exp(x)-1)  leaky : Leaky ReLU.  y = x   0 ? x : slope * x  prelu : Parametric ReLU. This is same as  leaky  except that  slope  is learnt during training.  rrelu : Randomized ReLU. same as  leaky  but the  slope  is uniformly and randomly chosen from  [lower_bound, upper_bound)  for training, while fixed to be  (lower_bound+upper_bound)/2  for inference.   Defined in src/operator/leaky_relu.cc:L58  Arguments   data::NDArray-or-SymbolicNode : Input data to activation function.  act_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky' : Activation function to be applied.  slope::float, optional, default=0.25 : Init slope for the activation. (For leaky and elu only)  lower_bound::float, optional, default=0.125 : Lower bound of random slope. (For rrelu only)  upper_bound::float, optional, default=0.334 : Upper bound of random slope. (For rrelu only)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LinearRegressionOutput     Method .  LinearRegressionOutput(data, label, grad_scale)  Computes and optimizes for squared loss during backward propagation. Just outputs $data$ during forward propagation.  If :math: \\hat{y}_i  is the predicted value of the i-th sample, and :math: y_i  is the corresponding target value, then the squared loss estimated over :math: n  samples is defined as  :math: \\text{SquaredLoss}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left( y_i - \\hat{y}_i \\right)^2  .. note::    Use the LinearRegressionOutput as the final output layer of a net.  By default, gradients of this loss function are scaled by factor  1/n , where n is the number of training examples. The parameter  grad_scale  can be used to change this scale to  grad_scale/n .  Defined in src/operator/regression_output.cc:L70  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  label::NDArray-or-SymbolicNode : Input label to the function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.LogisticRegressionOutput     Method .  LogisticRegressionOutput(data, label, grad_scale)  Applies a logistic function to the input.  The logistic function, also known as the sigmoid function, is computed as :math: \\frac{1}{1+exp(-x)} .  Commonly, the sigmoid is used to squash the real-valued output of a linear model :math:wTx+b into the [0,1] range so that it can be interpreted as a probability. It is suitable for binary classification or probability prediction tasks.  .. note::    Use the LogisticRegressionOutput as the final output layer of a net.  By default, gradients of this loss function are scaled by factor  1/n , where n is the number of training examples. The parameter  grad_scale  can be used to change this scale to  grad_scale/n .  Defined in src/operator/regression_output.cc:L112  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  label::NDArray-or-SymbolicNode : Input label to the function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.MAERegressionOutput     Method .  MAERegressionOutput(data, label, grad_scale)  Computes mean absolute error of the input.  MAE is a risk metric corresponding to the expected value of the absolute error.  If :math: \\hat{y}_i  is the predicted value of the i-th sample, and :math: y_i  is the corresponding target value, then the mean absolute error (MAE) estimated over :math: n  samples is defined as  :math: \\text{MAE}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left| y_i - \\hat{y}_i \\right|  .. note::    Use the MAERegressionOutput as the final output layer of a net.  By default, gradients of this loss function are scaled by factor  1/n , where n is the number of training examples. The parameter  grad_scale  can be used to change this scale to  grad_scale/n .  Defined in src/operator/regression_output.cc:L91  Arguments   data::NDArray-or-SymbolicNode : Input data to the function.  label::NDArray-or-SymbolicNode : Input label to the function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.MakeLoss     Method .  MakeLoss(data, grad_scale, valid_thresh, normalization)  Make your own loss function in network construction.  This operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.  For example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::  cross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = MakeLoss(cross_entropy)  We will need to use $MakeLoss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.  In addition, we can give a scale to the loss by setting $grad_scale$, so that the gradient of the loss will be rescaled in the backpropagation.  .. note:: This operator should be used as a Symbol instead of NDArray.  Defined in src/operator/make_loss.cc:L71  Arguments   data::NDArray-or-SymbolicNode : Input array.  grad_scale::float, optional, default=1 : Gradient scale as a supplement to unary and binary operators  valid_thresh::float, optional, default=0 : clip each element in the array to 0 when it is less than $valid_thresh$. This is used when $normalization$ is set to $'valid'$.  normalization::{'batch', 'null', 'valid'},optional, default='null' : If this is set to null, the output gradient will not be normalized. If this is set to batch, the output gradient will be divided by the batch size. If this is set to valid, the output gradient will be divided by the number of valid input elements.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Pad     Method .  Pad(data, mode, pad_width, constant_value)  Pads an input array with a constant or edge values of the array.  .. note::  Pad  is deprecated. Use  pad  instead.  .. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in  pad_width  to be zero.  This operation pads an input array with either a  constant_value  or edge values along each axis of the input array. The amount of padding is specified by  pad_width .  pad_width  is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The  pad_width  should be of length $2*N$ where $N$ is the number of dimensions of the array.  For dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.  Example::  x = [[[[  1.   2.   3.]           [  4.   5.   6.]]       [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]  pad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]  pad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]  Defined in src/operator/pad.cc:L766  Arguments   data::NDArray-or-SymbolicNode : An n-dimensional input array.  mode::{'constant', 'edge', 'reflect'}, required : Padding type to use. \"constant\" pads with  constant_value  \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.  pad_width::Shape(tuple), required : Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.  constant_value::double, optional, default=0 : The value used for padding when  mode  is \"constant\".  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Pooling     Method .  Pooling(data, global_pool, cudnn_off, kernel, pool_type, pooling_convention, stride, pad)  Performs pooling on the input.  The shapes for 1-D pooling are   data :  (batch_size, channel, width) ,  out :  (batch_size, num_filter, out_width) .   The shapes for 2-D pooling are   data :  (batch_size, channel, height, width)   out :  (batch_size, num_filter, out_height, out_width) , with::  out_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])    The definition of  f  depends on $pooling_convention$, which has two options:    valid  (default)::  f(x, k, p, s) = floor((x+2*p-k)/s)+1\n  *  full , which is compatible with Caffe::  f(x, k, p, s) = ceil((x+2*p-k)/s)+1    But $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.  Three pooling options are supported by $pool_type$:   avg : average pooling  max : max pooling  sum : sum pooling   For 3-D pooling, an additional  depth  dimension is added before  height . Namely the input data will have shape  (batch_size, channel, depth, height, width) .  Defined in src/operator/nn/pooling.cc:L133  Arguments   data::NDArray-or-SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=0 : Ignore kernel size, do global pooling based on current input feature map.  cudnn_off::boolean, optional, default=0 : Turn off cudnn pooling and use MXNet pooling operator.  kernel::Shape(tuple), required : Pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  pooling_convention::{'full', 'valid'},optional, default='valid' : Pooling convention to be applied.  stride::Shape(tuple), optional, default=[] : Stride: for pooling (y, x) or (d, y, x). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : Pad for pooling: (y, x) or (d, y, x). Defaults to no padding.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Pooling_v1     Method .  Pooling_v1(data, global_pool, kernel, pool_type, pooling_convention, stride, pad)  This operator is DEPRECATED. Perform pooling on the input.  The shapes for 2-D pooling is   data :  (batch_size, channel, height, width)   out :  (batch_size, num_filter, out_height, out_width) , with::  out_height = f(height, kernel[0], pad[0], stride[0])   out_width = f(width, kernel[1], pad[1], stride[1])    The definition of  f  depends on $pooling_convention$, which has two options:    valid  (default)::  f(x, k, p, s) = floor((x+2*p-k)/s)+1\n  *  full , which is compatible with Caffe::  f(x, k, p, s) = ceil((x+2*p-k)/s)+1    But $global_pool$ is set to be true, then do a global pooling, namely reset $kernel=(height, width)$.  Three pooling options are supported by $pool_type$:   avg : average pooling  max : max pooling  sum : sum pooling   1-D pooling is special case of 2-D pooling with  weight=1  and  kernel[1]=1 .  For 3-D pooling, an additional  depth  dimension is added before  height . Namely the input data will have shape  (batch_size, channel, depth, height, width) .  Defined in src/operator/pooling_v1.cc:L104  Arguments   data::NDArray-or-SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=0 : Ignore kernel size, do global pooling based on current input feature map.  kernel::Shape(tuple), required : pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  pooling_convention::{'full', 'valid'},optional, default='valid' : Pooling convention to be applied.  stride::Shape(tuple), optional, default=[] : stride: for pooling (y, x) or (d, y, x)  pad::Shape(tuple), optional, default=[] : pad for pooling: (y, x) or (d, y, x)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.RNN     Method .  RNN(data, parameters, state, state_cell, state_size, num_layers, bidirectional, mode, p, state_outputs)  Applies a recurrent layer to input.  Arguments   data::NDArray-or-SymbolicNode : Input data to RNN  parameters::NDArray-or-SymbolicNode : Vector of all RNN trainable parameters concatenated  state::NDArray-or-SymbolicNode : initial hidden state of the RNN  state_cell::NDArray-or-SymbolicNode : initial cell state for LSTM networks (only for LSTM)  state_size::int (non-negative), required : size of the state for each layer  num_layers::int (non-negative), required : number of stacked layers  bidirectional::boolean, optional, default=0 : whether to use bidirectional recurrent layers  mode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required : the type of RNN to compute  p::float, optional, default=0 : Dropout probability, fraction of the input that gets dropped out at training time  state_outputs::boolean, optional, default=0 : Whether to have the states as symbol outputs.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.ROIPooling     Method .  ROIPooling(data, rois, pooled_size, spatial_scale)  Performs region of interest(ROI) pooling on the input array.  ROI pooling is a variant of a max pooling layer, in which the output size is fixed and region of interest is a parameter. Its purpose is to perform max pooling on the inputs of non-uniform sizes to obtain fixed-size feature maps. ROI pooling is a neural-net layer mostly used in training a  Fast R-CNN  network for object detection.  This operator takes a 4D feature map as an input array and region proposals as  rois , then it pools over sub-regions of input and produces a fixed-sized output array regardless of the ROI size.  To crop the feature map accordingly, you can resize the bounding box coordinates by changing the parameters  rois  and  spatial_scale .  The cropped feature maps are pooled by standard max pooling operation to a fixed size output indicated by a  pooled_size  parameter. batch_size will change to the number of region bounding boxes after  ROIPooling .  The size of each region of interest doesn't have to be perfectly divisible by the number of pooling sections( pooled_size ).  Example::  x = [[[[  0.,   1.,   2.,   3.,   4.,   5.],          [  6.,   7.,   8.,   9.,  10.,  11.],          [ 12.,  13.,  14.,  15.,  16.,  17.],          [ 18.,  19.,  20.,  21.,  22.,  23.],          [ 24.,  25.,  26.,  27.,  28.,  29.],          [ 30.,  31.,  32.,  33.,  34.,  35.],          [ 36.,  37.,  38.,  39.,  40.,  41.],          [ 42.,  43.,  44.,  45.,  46.,  47.]]]]  // region of interest i.e. bounding box coordinates.   y = [[0,0,0,4,4]]  // returns array of shape (2,2) according to the given roi with max pooling.   ROIPooling(x, y, (2,2), 1.0) = [[[[ 14.,  16.],                                     [ 26.,  28.]]]]  // region of interest is changed due to the change in  spacial_scale  parameter.   ROIPooling(x, y, (2,2), 0.7) = [[[[  7.,   9.],                                     [ 19.,  21.]]]]  Defined in src/operator/roi_pooling.cc:L287  Arguments   data::NDArray-or-SymbolicNode : The input array to the pooling operator,  a 4D Feature maps  rois::NDArray-or-SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]], where (x1, y1) and (x2, y2) are top left and bottom right corners of designated region of interest.  batch_index  indicates the index of corresponding image in the input array  pooled_size::Shape(tuple), required : ROI pooling output shape (h,w)  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SVMOutput     Method .  SVMOutput(data, label, margin, regularization_coefficient, use_linear)  Computes support vector machine based transformation of the input.  This tutorial demonstrates using SVM as output layer for classification instead of softmax: https://github.com/dmlc/mxnet/tree/master/example/svm_mnist.  Arguments   data::NDArray-or-SymbolicNode : Input data for SVM transformation.  label::NDArray-or-SymbolicNode : Class label for the input data.  margin::float, optional, default=1 : The loss function penalizes outputs that lie outside this margin. Default margin is 1.  regularization_coefficient::float, optional, default=1 : Regularization parameter for the SVM. This balances the tradeoff between coefficient size and error.  use_linear::boolean, optional, default=0 : Whether to use L1-SVM objective. L2-SVM objective is used by default.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SequenceLast     Method .  SequenceLast(data, sequence_length, use_sequence_length)  Takes the last element of a sequence.  This function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns a (n-1)-dimensional array of the form [batch_size, other_feature_dims].  Parameter  sequence_length  is used to handle variable-length sequences.  sequence_length  should be an input array of positive ints of dimension [batch_size]. To use this parameter, set  use_sequence_length  to  True , otherwise each example in the batch is assumed to have the max sequence length.  .. note:: Alternatively, you can also use  take  operator.  Example::  x = [[[  1.,   2.,   3.],          [  4.,   5.,   6.],          [  7.,   8.,   9.]],      [[ 10.,   11.,   12.],\n     [ 13.,   14.,   15.],\n     [ 16.,   17.,   18.]],\n\n    [[  19.,   20.,   21.],\n     [  22.,   23.,   24.],\n     [  25.,   26.,   27.]]]  // returns last sequence when sequence_length parameter is not used    SequenceLast(x) = [[  19.,   20.,   21.],                       [  22.,   23.,   24.],                       [  25.,   26.,   27.]]  // sequence_length y is used    SequenceLast(x, y=[1,1,1], use_sequence_length=True) =             [[  1.,   2.,   3.],              [  4.,   5.,   6.],              [  7.,   8.,   9.]]  // sequence_length y is used    SequenceLast(x, y=[1,2,3], use_sequence_length=True) =             [[  1.,    2.,   3.],              [  13.,  14.,  15.],              [  25.,  26.,  27.]]  Defined in src/operator/sequence_last.cc:L92  Arguments   data::NDArray-or-SymbolicNode : n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n 2  sequence_length::NDArray-or-SymbolicNode : vector of sequence lengths of the form [batch_size]  use_sequence_length::boolean, optional, default=0 : If set to true, this layer takes in an extra input parameter  sequence_length  to specify variable length sequence  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SequenceMask     Method .  SequenceMask(data, sequence_length, use_sequence_length, value)  Sets all elements outside the sequence to a constant value.  This function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.  Parameter  sequence_length  is used to handle variable-length sequences.  sequence_length  should be an input array of positive ints of dimension [batch_size]. To use this parameter, set  use_sequence_length  to  True , otherwise each example in the batch is assumed to have the max sequence length and this operator works as the  identity  operator.  Example::  x = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],      [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]  // Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]  // Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]  // works as identity operator when sequence_length parameter is not used    SequenceMask(x) = [[[  1.,   2.,   3.],                        [  4.,   5.,   6.]],                    [[  7.,   8.,   9.],\n                   [ 10.,  11.,  12.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]  // sequence_length [1,1] means 1 of each batch will be kept    // and other rows are masked with default mask value = 0    SequenceMask(x, y=[1,1], use_sequence_length=True) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],               [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]],\n\n             [[  0.,   0.,   0.],\n              [  0.,   0.,   0.]]]  // sequence_length [2,3] means 2 of batch B1 and 3 of batch B2 will be kept    // and other rows are masked with value = 1    SequenceMask(x, y=[2,3], use_sequence_length=True, value=1) =                 [[[  1.,   2.,   3.],                   [  4.,   5.,   6.]],               [[  7.,   8.,   9.],\n              [  10.,  11.,  12.]],\n\n             [[   1.,   1.,   1.],\n              [  16.,  17.,  18.]]]  Defined in src/operator/sequence_mask.cc:L114  Arguments   data::NDArray-or-SymbolicNode : n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] where n 2  sequence_length::NDArray-or-SymbolicNode : vector of sequence lengths of the form [batch_size]  use_sequence_length::boolean, optional, default=0 : If set to true, this layer takes in an extra input parameter  sequence_length  to specify variable length sequence  value::float, optional, default=0 : The value to be used as a mask.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SequenceReverse     Method .  SequenceReverse(data, sequence_length, use_sequence_length)  Reverses the elements of each sequence.  This function takes an n-dimensional input array of the form [max_sequence_length, batch_size, other_feature_dims] and returns an array of the same shape.  Parameter  sequence_length  is used to handle variable-length sequences.  sequence_length  should be an input array of positive ints of dimension [batch_size]. To use this parameter, set  use_sequence_length  to  True , otherwise each example in the batch is assumed to have the max sequence length.  Example::  x = [[[  1.,   2.,   3.],          [  4.,   5.,   6.]],      [[  7.,   8.,   9.],\n     [ 10.,  11.,  12.]],\n\n    [[ 13.,  14.,   15.],\n     [ 16.,  17.,   18.]]]  // Batch 1    B1 = [[  1.,   2.,   3.],          [  7.,   8.,   9.],          [ 13.,  14.,  15.]]  // Batch 2    B2 = [[  4.,   5.,   6.],          [ 10.,  11.,  12.],          [ 16.,  17.,  18.]]  // returns reverse sequence when sequence_length parameter is not used    SequenceReverse(x) = [[[ 13.,  14.,   15.],                           [ 16.,  17.,   18.]],                       [[  7.,   8.,   9.],\n                      [ 10.,  11.,  12.]],\n\n                     [[  1.,   2.,   3.],\n                      [  4.,   5.,   6.]]]  // sequence_length [2,2] means 2 rows of    // both batch B1 and B2 will be reversed.    SequenceReverse(x, y=[2,2], use_sequence_length=True) =                      [[[  7.,   8.,   9.],                        [ 10.,  11.,  12.]],                    [[  1.,   2.,   3.],\n                   [  4.,   5.,   6.]],\n\n                  [[ 13.,  14.,   15.],\n                   [ 16.,  17.,   18.]]]  // sequence_length [2,3] means 2 of batch B2 and 3 of batch B3    // will be reversed.    SequenceReverse(x, y=[2,3], use_sequence_length=True) =                     [[[  7.,   8.,   9.],                       [ 16.,  17.,  18.]],                   [[  1.,   2.,   3.],\n                  [ 10.,  11.,  12.]],\n\n                 [[ 13.,  14,   15.],\n                  [  4.,   5.,   6.]]]  Defined in src/operator/sequence_reverse.cc:L113  Arguments   data::NDArray-or-SymbolicNode : n-dimensional input array of the form [max_sequence_length, batch_size, other dims] where n 2  sequence_length::NDArray-or-SymbolicNode : vector of sequence lengths of the form [batch_size]  use_sequence_length::boolean, optional, default=0 : If set to true, this layer takes in an extra input parameter  sequence_length  to specify variable length sequence  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SliceChannel     Method .  SliceChannel(data, num_outputs, axis, squeeze_axis)  Splits an array along a particular axis into multiple sub-arrays.  .. note:: $SliceChannel$ is deprecated. Use $split$ instead.  Note  that  num_outputs  should evenly divide the length of the axis along which to split the array.  Example::  x  = [[[ 1.]           [ 2.]]          [[ 3.]           [ 4.]]          [[ 5.]           [ 6.]]]    x.shape = (3, 2, 1)  y = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)    y = [[[ 1.]]         [[ 3.]]         [[ 5.]]]     [[[ 2.]]\n    [[ 4.]]\n    [[ 6.]]]  y[0].shape = (3, 1, 1)  z = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)    z = [[[ 1.]          [ 2.]]]     [[[ 3.]\n     [ 4.]]]\n\n   [[[ 5.]\n     [ 6.]]]  z[0].shape = (1, 2, 1)  squeeze_axis=1  removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $1$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to true only if $input.shape[axis] == num_outputs$.  Example::  z = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)    z = [[ 1.]         [ 2.]]     [[ 3.]\n    [ 4.]]\n\n   [[ 5.]\n    [ 6.]]  z[0].shape = (2 ,1 )  Defined in src/operator/slice_channel.cc:L107  Arguments   data::NDArray-or-SymbolicNode : The input  num_outputs::int, required : Number of splits. Note that this should evenly divide the length of the  axis .  axis::int, optional, default='1' : Axis along which to split.  squeeze_axis::boolean, optional, default=0 : If true, Removes the axis with length 1 from the shapes of the output arrays.  Note  that setting  squeeze_axis  to $true$ removes axis with length 1 only along the  axis  which it is split. Also  squeeze_axis  can be set to $true$ only if $input.shape[axis] == num_outputs$.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.Softmax     Method .  Softmax(data, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad, smooth_alpha)  Please use  SoftmaxOutput .  .. note::  This operator has been renamed to  SoftmaxOutput , which   computes the gradient of cross-entropy loss w.r.t softmax output.   To just compute softmax output, use the  softmax  operator.  Defined in src/operator/softmax_output.cc:L138  Arguments   data::NDArray-or-SymbolicNode : Input array.  grad_scale::float, optional, default=1 : Scales the gradient by a float factor.  ignore_label::float, optional, default=-1 : The instances whose  labels  ==  ignore_label  will be ignored during backward, if  use_ignore  is set to $true$).  multi_output::boolean, optional, default=0 : If set to $true$, the softmax function will be computed along axis $1$. This is applied when the shape of input array differs from the shape of label array.  use_ignore::boolean, optional, default=0 : If set to $true$, the  ignore_label  value will not contribute to the backward gradient.  preserve_shape::boolean, optional, default=0 : If set to $true$, the softmax function will be computed along the last axis ($-1$).  normalization::{'batch', 'null', 'valid'},optional, default='null' : Normalizes the gradient.  out_grad::boolean, optional, default=0 : Multiplies gradient with output gradient element-wise.  smooth_alpha::float, optional, default=0 : Constant for computing a label smoothed version of cross-entropyfor the backwards pass.  This constant gets subtracted from theone-hot encoding of the gold label and distributed uniformly toall other labels.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SoftmaxActivation     Method .  SoftmaxActivation(data, mode)  Applies softmax activation to input. This is intended for internal layers.  .. note::  This operator has been deprecated, please use  softmax .  If  mode  = $instance$, this operator will compute a softmax for each instance in the batch. This is the default mode.  If  mode  = $channel$, this operator will compute a k-class softmax at each position of each instance, where  k  = $num_channel$. This mode can only be used when the input array has at least 3 dimensions. This can be used for  fully convolutional network ,  image segmentation , etc.  Example::     input_array = mx.nd.array([[3., 0.5, -0.5, 2., 7.],                            [2., -.4, 7.,   3., 0.2]]) softmax_act = mx.nd.SoftmaxActivation(input_array) print softmax_act.asnumpy()     [[  1.78322066e-02   1.46375655e-03   5.38485940e-04   6.56010211e-03   9.73605454e-01]    [  6.56221947e-03   5.95310994e-04   9.73919690e-01   1.78379621e-02   1.08472735e-03]]  Defined in src/operator/nn/softmax_activation.cc:L67  Arguments   data::NDArray-or-SymbolicNode : Input array to activation function.  mode::{'channel', 'instance'},optional, default='instance' : Specifies how to compute the softmax. If set to $instance$, it computes softmax for each instance. If set to $channel$, It computes cross channel softmax for each position of each instance.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SoftmaxOutput     Method .  SoftmaxOutput(data, label, grad_scale, ignore_label, multi_output, use_ignore, preserve_shape, normalization, out_grad, smooth_alpha)  Computes the gradient of cross entropy loss with respect to softmax output.    This operator computes the gradient in two steps. The cross entropy loss does not actually need to be computed.   Applies softmax function on the input array.  Computes and returns the gradient of cross entropy loss w.r.t. the softmax output.   The softmax function, cross entropy loss and gradient is given by:    Softmax Function:  .. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n      * Cross Entropy Function:  .. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)\n      * The gradient of cross entropy loss w.r.t softmax output:  .. math:: \\text{gradient} = \\text{output} - \\text{label}\n  * During forward propagation, the softmax function is computed for each instance in the input array.    For general  N -D input arrays with shape :math: (d_1, d_2, ..., d_n) . The size is :math: s=d_1 \\cdot d_2 \\cdot \\cdot \\cdot d_n . We can use the parameters  preserve_shape  and  multi_output  to specify the way to compute softmax:   By default,  preserve_shape  is $false$. This operator will reshape the input array into a 2-D array with shape :math: (d_1, \\frac{s}{d_1})  and then compute the softmax function for each row in the reshaped array, and afterwards reshape it back to the original shape :math: (d_1, d_2, ..., d_n) .  If  preserve_shape  is $true$, the softmax function will be computed along the last axis ( axis  = $-1$).  If  multi_output  is $true$, the softmax function will be computed along the second axis ( axis  = $1$).   During backward propagation, the gradient of cross-entropy loss w.r.t softmax output array is computed. The provided label can be a one-hot label array or a probability label array.    If the parameter  use_ignore  is $true$,  ignore_label  can specify input instances with a particular label to be ignored during backward propagation.  This has no effect when softmax  output  has same shape as  label .  Example::  data = [[1,2,3,4],[2,2,2,2],[3,3,3,3],[4,4,4,4]]   label = [1,0,2,3]   ignore_label = 1   SoftmaxOutput(data=data, label = label,\n                multi_output=true, use_ignore=true,\n                ignore_label=ignore_label)", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/symbolic-node/#forward-softmax-output", 
            "text": "[[ 0.0320586   0.08714432  0.23688284  0.64391428]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]    [ 0.25        0.25        0.25        0.25      ]]", 
            "title": "forward softmax output"
        }, 
        {
            "location": "/api/symbolic-node/#backward-gradient-output", 
            "text": "[[ 0.    0.    0.    0.  ]    [-0.75  0.25  0.25  0.25]    [ 0.25  0.25 -0.75  0.25]    [ 0.25  0.25  0.25 -0.75]]", 
            "title": "backward gradient output"
        }, 
        {
            "location": "/api/symbolic-node/#notice-that-the-first-row-is-all-0-because-label0-is-1-which-is-equal-to-ignore_label", 
            "text": "* The parameter `grad_scale` can be used to rescale the gradient, which is often used to give each loss function different weights.\n  * This operator also supports various ways to normalize the gradient by `normalization`, The `normalization` is applied if softmax output has different shape than the labels. The `normalization` mode can be set to the followings:   $'null'$: do nothing.  $'batch'$: divide the gradient by the batch size.  $'valid'$: divide the gradient by the number of instances which are not ignored.       Defined in src/operator/softmax_output.cc:L123  Arguments   data::NDArray-or-SymbolicNode : Input array.  label::NDArray-or-SymbolicNode : Ground truth label.  grad_scale::float, optional, default=1 : Scales the gradient by a float factor.  ignore_label::float, optional, default=-1 : The instances whose  labels  ==  ignore_label  will be ignored during backward, if  use_ignore  is set to $true$).  multi_output::boolean, optional, default=0 : If set to $true$, the softmax function will be computed along axis $1$. This is applied when the shape of input array differs from the shape of label array.  use_ignore::boolean, optional, default=0 : If set to $true$, the  ignore_label  value will not contribute to the backward gradient.  preserve_shape::boolean, optional, default=0 : If set to $true$, the softmax function will be computed along the last axis ($-1$).  normalization::{'batch', 'null', 'valid'},optional, default='null' : Normalizes the gradient.  out_grad::boolean, optional, default=0 : Multiplies gradient with output gradient element-wise.  smooth_alpha::float, optional, default=0 : Constant for computing a label smoothed version of cross-entropyfor the backwards pass.  This constant gets subtracted from theone-hot encoding of the gold label and distributed uniformly toall other labels.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SpatialTransformer     Method .  SpatialTransformer(data, loc, target_shape, transform_type, sampler_type)  Applies a spatial transformer to input feature map.  Arguments   data::NDArray-or-SymbolicNode : Input data to the SpatialTransformerOp.  loc::NDArray-or-SymbolicNode : localisation net, the output dim should be 6 when transform_type is affine. You shold initialize the weight and bias with identity tranform.  target_shape::Shape(tuple), optional, default=[0,0] : output shape(h, w) of spatial transformer: (y, x)  transform_type::{'affine'}, required : transformation type  sampler_type::{'bilinear'}, required : sampling type  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.SwapAxis     Method .  SwapAxis(data, dim1, dim2)  Interchanges two axes of an array.  Examples::  x = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]  x = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array  swapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]  Defined in src/operator/swapaxis.cc:L70  Arguments   data::NDArray-or-SymbolicNode : Input array.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.UpSampling     Method .  UpSampling(data, scale, num_filter, sample_type, multi_input_mode, num_args, workspace)  Note : UpSampling takes variable number of positional inputs. So instead of calling as UpSampling([x, y, z], num_args=3), one should call via UpSampling(x, y, z), and num_args will be determined automatically.  Performs nearest neighbor/bilinear up sampling to inputs.  Arguments   data::NDArray-or-SymbolicNode[] : Array of tensors to upsample  scale::int (non-negative), required : Up sampling scale  num_filter::int (non-negative), optional, default=0 : Input filter. Only used by bilinear sample_type.  sample_type::{'bilinear', 'nearest'}, required : upsampling method  multi_input_mode::{'concat', 'sum'},optional, default='concat' : How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.  num_args::int, required : Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale h_0,scale w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._CachedOp     Method .  _CachedOp()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._CrossDeviceCopy     Method .  _CrossDeviceCopy()  Special op to copy data cross device  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._CustomFunction     Method .  _CustomFunction()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Div     Method .  _Div(lhs, rhs)  _Div is an alias of elemwise_div.  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._DivScalar     Method .  _DivScalar(data, scalar)  _DivScalar is an alias of _div_scalar.  Divide an array with a scalar.  $_div_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Equal     Method .  _Equal(lhs, rhs)  _Equal is an alias of _equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._EqualScalar     Method .  _EqualScalar(data, scalar)  _EqualScalar is an alias of _equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Greater     Method .  _Greater(lhs, rhs)  _Greater is an alias of _greater.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._GreaterEqualScalar     Method .  _GreaterEqualScalar(data, scalar)  _GreaterEqualScalar is an alias of _greater_equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._GreaterScalar     Method .  _GreaterScalar(data, scalar)  _GreaterScalar is an alias of _greater_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Greater_Equal     Method .  _Greater_Equal(lhs, rhs)  _Greater_Equal is an alias of _greater_equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Hypot     Method .  _Hypot(lhs, rhs)  _Hypot is an alias of _hypot.  Given the \"legs\" of a right triangle, return its hypotenuse.  Defined in src/operator/tensor/elemwise_binary_op_extended.cc:L79  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._HypotScalar     Method .  _HypotScalar(data, scalar)  _HypotScalar is an alias of _hypot_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Lesser     Method .  _Lesser(lhs, rhs)  _Lesser is an alias of _lesser.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._LesserEqualScalar     Method .  _LesserEqualScalar(data, scalar)  _LesserEqualScalar is an alias of _lesser_equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._LesserScalar     Method .  _LesserScalar(data, scalar)  _LesserScalar is an alias of _lesser_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Lesser_Equal     Method .  _Lesser_Equal(lhs, rhs)  _Lesser_Equal is an alias of _lesser_equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Maximum     Method .  _Maximum(lhs, rhs)  _Maximum is an alias of _maximum.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MaximumScalar     Method .  _MaximumScalar(data, scalar)  _MaximumScalar is an alias of _maximum_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Minimum     Method .  _Minimum(lhs, rhs)  _Minimum is an alias of _minimum.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MinimumScalar     Method .  _MinimumScalar(data, scalar)  _MinimumScalar is an alias of _minimum_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Minus     Method .  _Minus(lhs, rhs)  _Minus is an alias of elemwise_sub.  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MinusScalar     Method .  _MinusScalar(data, scalar)  _MinusScalar is an alias of _minus_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Mod     Method .  _Mod(lhs, rhs)  _Mod is an alias of _mod.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._ModScalar     Method .  _ModScalar(data, scalar)  _ModScalar is an alias of _mod_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Mul     Method .  _Mul(lhs, rhs)  _Mul is an alias of elemwise_mul.  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._MulScalar     Method .  _MulScalar(data, scalar)  _MulScalar is an alias of _mul_scalar.  Multiply an array with a scalar.  $_mul_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._NDArray     Method .  _NDArray(data, info)  Stub for implementing an operator implemented in native frontend language with ndarray.  Arguments   data::NDArray-or-SymbolicNode[] : Input data for the custom operator.  info::ptr, required :  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Native     Method .  _Native(data, info, need_top_grad)  Stub for implementing an operator implemented in native frontend language.  Arguments   data::NDArray-or-SymbolicNode[] : Input data for the custom operator.  info::ptr, required :  need_top_grad::boolean, optional, default=1 : Whether this layer needs out grad for backward. Should be false for loss layers.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._NoGradient     Method .  _NoGradient()  Place holder for variable who cannot perform gradient  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._NotEqualScalar     Method .  _NotEqualScalar(data, scalar)  _NotEqualScalar is an alias of _not_equal_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Not_Equal     Method .  _Not_Equal(lhs, rhs)  _Not_Equal is an alias of _not_equal.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Plus     Method .  _Plus(lhs, rhs)  _Plus is an alias of elemwise_add.  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._PlusScalar     Method .  _PlusScalar(data, scalar)  _PlusScalar is an alias of _plus_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._Power     Method .  _Power(lhs, rhs)  _Power is an alias of _power.  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._PowerScalar     Method .  _PowerScalar(data, scalar)  _PowerScalar is an alias of _power_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._RDivScalar     Method .  _RDivScalar(data, scalar)  _RDivScalar is an alias of _rdiv_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._RMinusScalar     Method .  _RMinusScalar(data, scalar)  _RMinusScalar is an alias of _rminus_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._RModScalar     Method .  _RModScalar(data, scalar)  _RModScalar is an alias of _rmod_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._RPowerScalar     Method .  _RPowerScalar(data, scalar)  _RPowerScalar is an alias of _rpower_scalar.  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._add     Method .  _add(lhs, rhs)  _add is an alias of elemwise_add.  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._arange     Method .  _arange(start, stop, step, repeat, ctx, dtype)  Return evenly spaced values within a given interval. Similar to Numpy  Arguments   start::double, required : Start of interval. The interval includes this value. The default start value is 0.  stop::double or None, optional, default=None : End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.  step::double, optional, default=1 : Spacing between values.  repeat::int, optional, default='1' : The repeating time of all elements. E.g repeat=3, the element a will be repeated three times \u2013  a, a, a.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'int64', 'uint8'},optional, default='float32' : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Activation     Method .  _backward_Activation()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_BatchNorm     Method .  _backward_BatchNorm()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_BatchNorm_v1     Method .  _backward_BatchNorm_v1()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_BilinearSampler     Method .  _backward_BilinearSampler()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_CachedOp     Method .  _backward_CachedOp()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Concat     Method .  _backward_Concat()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Convolution     Method .  _backward_Convolution()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Convolution_v1     Method .  _backward_Convolution_v1()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Correlation     Method .  _backward_Correlation()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Crop     Method .  _backward_Crop()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Custom     Method .  _backward_Custom()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_CustomFunction     Method .  _backward_CustomFunction()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Deconvolution     Method .  _backward_Deconvolution()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Dropout     Method .  _backward_Dropout()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Embedding     Method .  _backward_Embedding()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_FullyConnected     Method .  _backward_FullyConnected()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_GridGenerator     Method .  _backward_GridGenerator()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_IdentityAttachKLSparseReg     Method .  _backward_IdentityAttachKLSparseReg()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_InstanceNorm     Method .  _backward_InstanceNorm()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_L2Normalization     Method .  _backward_L2Normalization()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LRN     Method .  _backward_LRN()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LeakyReLU     Method .  _backward_LeakyReLU()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LinearRegressionOutput     Method .  _backward_LinearRegressionOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_LogisticRegressionOutput     Method .  _backward_LogisticRegressionOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_MAERegressionOutput     Method .  _backward_MAERegressionOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_MakeLoss     Method .  _backward_MakeLoss()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Pad     Method .  _backward_Pad()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Pooling     Method .  _backward_Pooling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Pooling_v1     Method .  _backward_Pooling_v1()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_RNN     Method .  _backward_RNN()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_ROIPooling     Method .  _backward_ROIPooling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SVMOutput     Method .  _backward_SVMOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SequenceLast     Method .  _backward_SequenceLast()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SequenceMask     Method .  _backward_SequenceMask()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SequenceReverse     Method .  _backward_SequenceReverse()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SliceChannel     Method .  _backward_SliceChannel()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_Softmax     Method .  _backward_Softmax()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SoftmaxActivation     Method .  _backward_SoftmaxActivation()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SoftmaxOutput     Method .  _backward_SoftmaxOutput()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SparseEmbedding     Method .  _backward_SparseEmbedding()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SpatialTransformer     Method .  _backward_SpatialTransformer()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_SwapAxis     Method .  _backward_SwapAxis()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_UpSampling     Method .  _backward_UpSampling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__CrossDeviceCopy     Method .  _backward__CrossDeviceCopy()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__NDArray     Method .  _backward__NDArray()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__Native     Method .  _backward__Native()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_CTCLoss     Method .  _backward__contrib_CTCLoss()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_DeformableConvolution     Method .  _backward__contrib_DeformableConvolution()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_DeformablePSROIPooling     Method .  _backward__contrib_DeformablePSROIPooling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_MultiBoxDetection     Method .  _backward__contrib_MultiBoxDetection()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_MultiBoxPrior     Method .  _backward__contrib_MultiBoxPrior()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_MultiBoxTarget     Method .  _backward__contrib_MultiBoxTarget()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_MultiProposal     Method .  _backward__contrib_MultiProposal()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_PSROIPooling     Method .  _backward__contrib_PSROIPooling()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_Proposal     Method .  _backward__contrib_Proposal()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_count_sketch     Method .  _backward__contrib_count_sketch()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_fft     Method .  _backward__contrib_fft()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward__contrib_ifft     Method .  _backward__contrib_ifft()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_abs     Method .  _backward_abs(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_add     Method .  _backward_add()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arccos     Method .  _backward_arccos(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arccosh     Method .  _backward_arccosh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arcsin     Method .  _backward_arcsin(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arcsinh     Method .  _backward_arcsinh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arctan     Method .  _backward_arctan(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_arctanh     Method .  _backward_arctanh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_batch_dot     Method .  _backward_batch_dot()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_add     Method .  _backward_broadcast_add()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_div     Method .  _backward_broadcast_div()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_hypot     Method .  _backward_broadcast_hypot()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_maximum     Method .  _backward_broadcast_maximum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_minimum     Method .  _backward_broadcast_minimum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_mod     Method .  _backward_broadcast_mod()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_mul     Method .  _backward_broadcast_mul()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_power     Method .  _backward_broadcast_power()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_broadcast_sub     Method .  _backward_broadcast_sub()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_cast     Method .  _backward_cast()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_cbrt     Method .  _backward_cbrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_clip     Method .  _backward_clip()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_contrib_bipartite_matching     Method .  _backward_contrib_bipartite_matching(is_ascend, threshold, topk)  Arguments   is_ascend::boolean, optional, default=0 : Use ascend order for scores instead of descending. Please set threshold accordingly.  threshold::float, required : Ignore matching when score   thresh, if is_ascend=false, or ignore score   thresh, if is_ascend=true.  topk::int, optional, default='-1' : Limit the number of matches to topk, set -1 for no limit  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_contrib_box_iou     Method .  _backward_contrib_box_iou(format)  Arguments   format::{'center', 'corner'},optional, default='corner' : The box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_contrib_box_nms     Method .  _backward_contrib_box_nms(overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)  Arguments   overlap_thresh::float, optional, default=0.5 : Overlapping(IoU) threshold to suppress object with smaller score.  topk::int, optional, default='-1' : Apply nms to topk boxes with descending scores, -1 to no restriction.  coord_start::int, optional, default='2' : Start index of the consecutive 4 coordinates.  score_index::int, optional, default='1' : Index of the scores/confidence of boxes.  id_index::int, optional, default='-1' : Optional, index of the class categories, -1 to disable.  force_suppress::boolean, optional, default=0 : Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category  in_format::{'center', 'corner'},optional, default='corner' : The input box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   out_format::{'center', 'corner'},optional, default='corner' : The output box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_copy     Method .  _backward_copy()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_cos     Method .  _backward_cos(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_cosh     Method .  _backward_cosh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_degrees     Method .  _backward_degrees(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_div     Method .  _backward_div()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_div_scalar     Method .  _backward_div_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_dot     Method .  _backward_dot(transpose_a, transpose_b)  Arguments   transpose_a::boolean, optional, default=0 : If true then transpose the first input before dot.  transpose_b::boolean, optional, default=0 : If true then transpose the second input before dot.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_expm1     Method .  _backward_expm1(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_gamma     Method .  _backward_gamma(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_gammaln     Method .  _backward_gammaln(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_hypot     Method .  _backward_hypot()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_hypot_scalar     Method .  _backward_hypot_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_gelqf     Method .  _backward_linalg_gelqf()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_gemm     Method .  _backward_linalg_gemm()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_gemm2     Method .  _backward_linalg_gemm2()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_potrf     Method .  _backward_linalg_potrf()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_potri     Method .  _backward_linalg_potri()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_sumlogdiag     Method .  _backward_linalg_sumlogdiag()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_syevd     Method .  _backward_linalg_syevd()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_syrk     Method .  _backward_linalg_syrk()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_trmm     Method .  _backward_linalg_trmm()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_linalg_trsm     Method .  _backward_linalg_trsm()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_log     Method .  _backward_log(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_log10     Method .  _backward_log10(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_log1p     Method .  _backward_log1p(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_log2     Method .  _backward_log2(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_log_softmax     Method .  _backward_log_softmax(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_max     Method .  _backward_max()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_maximum     Method .  _backward_maximum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_maximum_scalar     Method .  _backward_maximum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_mean     Method .  _backward_mean()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_min     Method .  _backward_min()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_minimum     Method .  _backward_minimum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_minimum_scalar     Method .  _backward_minimum_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_mod     Method .  _backward_mod()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_mod_scalar     Method .  _backward_mod_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_mul     Method .  _backward_mul()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_mul_scalar     Method .  _backward_mul_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_nanprod     Method .  _backward_nanprod()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_nansum     Method .  _backward_nansum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_pick     Method .  _backward_pick()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_power     Method .  _backward_power()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_power_scalar     Method .  _backward_power_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_prod     Method .  _backward_prod()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_radians     Method .  _backward_radians(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rcbrt     Method .  _backward_rcbrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rdiv_scalar     Method .  _backward_rdiv_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_reciprocal     Method .  _backward_reciprocal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_relu     Method .  _backward_relu(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_repeat     Method .  _backward_repeat()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_reverse     Method .  _backward_reverse()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rmod_scalar     Method .  _backward_rmod_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rpower_scalar     Method .  _backward_rpower_scalar(lhs, rhs, scalar)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  scalar::float : scalar value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_rsqrt     Method .  _backward_rsqrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sample_multinomial     Method .  _backward_sample_multinomial()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sigmoid     Method .  _backward_sigmoid(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sign     Method .  _backward_sign(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sin     Method .  _backward_sin(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sinh     Method .  _backward_sinh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_slice     Method .  _backward_slice()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_slice_axis     Method .  _backward_slice_axis()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_smooth_l1     Method .  _backward_smooth_l1(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_softmax     Method .  _backward_softmax(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_softmax_cross_entropy     Method .  _backward_softmax_cross_entropy()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sparse_retain     Method .  _backward_sparse_retain()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sqrt     Method .  _backward_sqrt(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_square     Method .  _backward_square(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_square_sum     Method .  _backward_square_sum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_stack     Method .  _backward_stack()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sub     Method .  _backward_sub()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_sum     Method .  _backward_sum()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_take     Method .  _backward_take()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_tan     Method .  _backward_tan(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_tanh     Method .  _backward_tanh(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_tile     Method .  _backward_tile()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_topk     Method .  _backward_topk()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._backward_where     Method .  _backward_where()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._broadcast_backward     Method .  _broadcast_backward()  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_CTCLoss     Method .  _contrib_CTCLoss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)  Connectionist Temporal Classification Loss.  The shapes of the inputs and outputs:   data :  (sequence_length, batch_size, alphabet_size)  label :  (batch_size, label_sequence_length)  out :  (batch_size)   The  data  tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When  blank_label  is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.  $label$ is an index matrix of integers. When  blank_label  is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when  blank_label  is $\"last\"$, the value  (alphabet_size-1)  is reserved for blank label.  If a sequence of labels is shorter than  label_sequence_length , use the special padding value at the end of the sequence to conform it to the correct length. The padding value is  0  when  blank_label  is $\"first\"$, and  -1  otherwise.  For example, suppose the vocabulary is  [a, b, c] , and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When  blank_label  is $\"first\"$, we can index the labels as  {'a': 1, 'b': 2, 'c': 3} , and we reserve the 0-th channel for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]  When  blank_label  is $\"last\"$, we can index the labels as  {'a': 0, 'b': 1, 'c': 2} , and we reserve the channel index 3 for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]  $out$ is a list of CTC loss values, one per example in the batch.  See  Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks , A. Graves  et al . for more information on the definition and the algorithm.  Defined in src/operator/contrib/ctc_loss.cc:L115  Arguments   data::NDArray-or-SymbolicNode : Input data to the ctc_loss op.  label::NDArray-or-SymbolicNode : Ground-truth labels for the loss.  data_lengths::NDArray-or-SymbolicNode : Lengths of data for each of the samples. Only required when use_data_lengths is true.  label_lengths::NDArray-or-SymbolicNode : Lengths of labels for each of the samples. Only required when use_label_lengths is true.  use_data_lengths::boolean, optional, default=0 : Whether the data lenghts are decided by  data_lengths . If false, the lengths are equal to the max sequence length.  use_label_lengths::boolean, optional, default=0 : Whether the label lenghts are decided by  label_lengths , or derived from  padding_mask . If false, the lengths are derived from the first occurrence of the value of  padding_mask . The value of  padding_mask  is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See  blank_label .  blank_label::{'first', 'last'},optional, default='first' : Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_DeformableConvolution     Method .  _contrib_DeformableConvolution(data, offset, weight, bias, kernel, stride, dilate, pad, num_filter, num_group, num_deformable_group, workspace, no_bias, layout)  Compute 2-D deformable convolution on 4-D input.  The deformable convolution operation is described in https://arxiv.org/abs/1703.06211  For 2-D deformable convolution, the shapes are   data :  (batch_size, channel, height, width)  offset :  (batch_size, num_deformable_group * kernel[0] * kernel[1], height, width)  weight :  (num_filter, channel, kernel[0], kernel[1])  bias :  (num_filter,)  out :  (batch_size, num_filter, out_height, out_width) .   Define::  f(x,k,p,s,d) = floor((x+2 p-d (k-1)-1)/s)+1  then we have::  out_height=f(height, kernel[0], pad[0], stride[0], dilate[0])   out_width=f(width, kernel[1], pad[1], stride[1], dilate[1])  If $no_bias$ is set to be true, then the $bias$ term is ignored.  The default data $layout$ is  NCHW , namely  (batch_size, channle, height, width) .  If $num_group$ is larger than 1, denoted by  g , then split the input $data$ evenly into  g  parts along the channel axis, and also evenly split $weight$ along the first dimension. Next compute the convolution on the  i -th part of the data with the  i -th weight part. The output is obtained by concating all the  g  results.  If $num_deformable_group$ is larger than 1, denoted by  dg , then split the input $offset$ evenly into  dg  parts along the channel axis, and also evenly split $out$ evenly into  dg  parts along the channel axis. Next compute the deformable convolution, apply the  i -th part of the offset part on the  i -th out.  Both $weight$ and $bias$ are learnable parameters.  Defined in src/operator/contrib/deformable_convolution.cc:L100  Arguments   data::NDArray-or-SymbolicNode : Input data to the DeformableConvolutionOp.  offset::NDArray-or-SymbolicNode : Input offset to the DeformableConvolutionOp.  weight::NDArray-or-SymbolicNode : Weight matrix.  bias::NDArray-or-SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : Convolution kernel size: (h, w) or (d, h, w)  stride::Shape(tuple), optional, default=[] : Convolution stride: (h, w) or (d, h, w). Defaults to 1 for each dimension.  dilate::Shape(tuple), optional, default=[] : Convolution dilate: (h, w) or (d, h, w). Defaults to 1 for each dimension.  pad::Shape(tuple), optional, default=[] : Zero pad for convolution: (h, w) or (d, h, w). Defaults to no padding.  num_filter::int (non-negative), required : Convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of group partitions.  num_deformable_group::int (non-negative), optional, default=1 : Number of deformable group partitions.  workspace::long (non-negative), optional, default=1024 : Maximum temperal workspace allowed for convolution (MB).  no_bias::boolean, optional, default=0 : Whether to disable bias parameter.  layout::{None, 'NCDHW', 'NCHW', 'NCW'},optional, default='None' : Set layout for input, output and weight. Empty for   default layout: NCW for 1d, NCHW for 2d and NCDHW for 3d.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_DeformablePSROIPooling     Method .  _contrib_DeformablePSROIPooling(data, rois, trans, spatial_scale, output_dim, group_size, pooled_size, part_size, sample_per_part, trans_std, no_trans)  Performs deformable position-sensitive region-of-interest pooling on inputs. The DeformablePSROIPooling operation is described in https://arxiv.org/abs/1703.06211 .batch_size will change to the number of region bounding boxes after DeformablePSROIPooling  Arguments   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  trans::SymbolicNode : transition parameter  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers  output_dim::int, required : fix output dim  group_size::int, required : fix group size  pooled_size::int, required : fix pooled size  part_size::int, optional, default='0' : fix part size  sample_per_part::int, optional, default='1' : fix samples per part  trans_std::float, optional, default=0 : fix transition std  no_trans::boolean, optional, default=0 : Whether to disable trans parameter.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_MultiBoxDetection     Method .  _contrib_MultiBoxDetection(cls_prob, loc_pred, anchor, clip, threshold, background_id, nms_threshold, force_suppress, variances, nms_topk)  Convert multibox detection predictions.  Arguments   cls_prob::NDArray-or-SymbolicNode : Class probabilities.  loc_pred::NDArray-or-SymbolicNode : Location regression predictions.  anchor::NDArray-or-SymbolicNode : Multibox prior anchor boxes  clip::boolean, optional, default=1 : Clip out-of-boundary boxes.  threshold::float, optional, default=0.01 : Threshold to be a positive prediction.  background_id::int, optional, default='0' : Background id.  nms_threshold::float, optional, default=0.5 : Non-maximum suppression threshold.  force_suppress::boolean, optional, default=0 : Suppress all detections regardless of class_id.  variances::tuple of  float , optional, default=[0.1,0.1,0.2,0.2] : Variances to be decoded from box regression output.  nms_topk::int, optional, default='-1' : Keep maximum top k detections before nms, -1 for no limit.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_MultiBoxPrior     Method .  _contrib_MultiBoxPrior(data, sizes, ratios, clip, steps, offsets)  Generate prior(anchor) boxes from data, sizes and ratios.  Arguments   data::NDArray-or-SymbolicNode : Input data.  sizes::tuple of  float , optional, default=[1] : List of sizes of generated MultiBoxPriores.  ratios::tuple of  float , optional, default=[1] : List of aspect ratios of generated MultiBoxPriores.  clip::boolean, optional, default=0 : Whether to clip out-of-boundary boxes.  steps::tuple of  float , optional, default=[-1,-1] : Priorbox step across y and x, -1 for auto calculation.  offsets::tuple of  float , optional, default=[0.5,0.5] : Priorbox center offsets, y and x respectively  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_MultiBoxTarget     Method .  _contrib_MultiBoxTarget(anchor, label, cls_pred, overlap_threshold, ignore_label, negative_mining_ratio, negative_mining_thresh, minimum_negative_samples, variances)  Compute Multibox training targets  Arguments   anchor::NDArray-or-SymbolicNode : Generated anchor boxes.  label::NDArray-or-SymbolicNode : Object detection labels.  cls_pred::NDArray-or-SymbolicNode : Class predictions.  overlap_threshold::float, optional, default=0.5 : Anchor-GT overlap threshold to be regarded as a positive match.  ignore_label::float, optional, default=-1 : Label for ignored anchors.  negative_mining_ratio::float, optional, default=-1 : Max negative to positive samples ratio, use -1 to disable mining  negative_mining_thresh::float, optional, default=0.5 : Threshold used for negative mining.  minimum_negative_samples::int, optional, default='0' : Minimum number of negative samples.  variances::tuple of  float , optional, default=[0.1,0.1,0.2,0.2] : Variances to be encoded in box regression target.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_MultiProposal     Method .  _contrib_MultiProposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)  Generate region proposals via RPN  Arguments   cls_score::NDArray-or-SymbolicNode : Score of how likely proposal is object.  bbox_pred::NDArray-or-SymbolicNode : BBox Predicted deltas from anchors for proposals  im_info::NDArray-or-SymbolicNode : Image size and scale.  rpn_pre_nms_top_n::int, optional, default='6000' : Number of top scoring boxes to keep after applying NMS to RPN proposals  rpn_post_nms_top_n::int, optional, default='300' : Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU  = this threshold  threshold::float, optional, default=0.7 : NMS value, below which to suppress.  rpn_min_size::int, optional, default='16' : Minimum height or width in proposal  scales::tuple of  float , optional, default=[4,8,16,32] : Used to generate anchor windows by enumerating scales  ratios::tuple of  float , optional, default=[0.5,1,2] : Used to generate anchor windows by enumerating ratios  feature_stride::int, optional, default='16' : The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.  output_score::boolean, optional, default=0 : Add score to outputs  iou_loss::boolean, optional, default=0 : Usage of IoU Loss  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_PSROIPooling     Method .  _contrib_PSROIPooling(data, rois, spatial_scale, output_dim, pooled_size, group_size)  Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after PSROIPooling  Arguments   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers  output_dim::int, required : fix output dim  pooled_size::int, required : fix pooled size  group_size::int, optional, default='0' : fix group size  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_Proposal     Method .  _contrib_Proposal(cls_score, bbox_pred, im_info, rpn_pre_nms_top_n, rpn_post_nms_top_n, threshold, rpn_min_size, scales, ratios, feature_stride, output_score, iou_loss)  Generate region proposals via RPN  Arguments   cls_score::NDArray-or-SymbolicNode : Score of how likely proposal is object.  bbox_pred::NDArray-or-SymbolicNode : BBox Predicted deltas from anchors for proposals  im_info::NDArray-or-SymbolicNode : Image size and scale.  rpn_pre_nms_top_n::int, optional, default='6000' : Number of top scoring boxes to keep after applying NMS to RPN proposals  rpn_post_nms_top_n::int, optional, default='300' : Overlap threshold used for non-maximumsuppresion(suppress boxes with IoU  = this threshold  threshold::float, optional, default=0.7 : NMS value, below which to suppress.  rpn_min_size::int, optional, default='16' : Minimum height or width in proposal  scales::tuple of  float , optional, default=[4,8,16,32] : Used to generate anchor windows by enumerating scales  ratios::tuple of  float , optional, default=[0.5,1,2] : Used to generate anchor windows by enumerating ratios  feature_stride::int, optional, default='16' : The size of the receptive field each unit in the convolution layer of the rpn,for example the product of all stride's prior to this layer.  output_score::boolean, optional, default=0 : Add score to outputs  iou_loss::boolean, optional, default=0 : Usage of IoU Loss  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_SparseEmbedding     Method .  _contrib_SparseEmbedding(data, weight, input_dim, output_dim, dtype)  Maps integer indices to vector representations (embeddings).  This operator maps words to real-valued vectors in a high-dimensional space, called word embeddings. These embeddings can capture semantic and syntactic properties of the words. For example, it has been noted that in the learned embedding spaces, similar words tend to be close to each other and dissimilar words far apart.  For an input array of shape (d1, ..., dK), the shape of an output array is (d1, ..., dK, output_dim). All the input values should be integers in the range [0, input_dim).  If the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be (ip0, op0).  The storage type of weight must be  row_sparse , and the gradient of the weight will be of  row_sparse  storage type, too.  .. Note::  `SparseEmbedding` is designed for the use case where `input_dim` is very large (e.g. 100k).\nThe operator is available on both CPU and GPU.  Examples::  input_dim = 4   output_dim = 5  // Each row in weight matrix y represents a word. So, y = (w0,w1,w2,w3)   y = [[  0.,   1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.,   9.],        [ 10.,  11.,  12.,  13.,  14.],        [ 15.,  16.,  17.,  18.,  19.]]  // Input array x represents n-grams(2-gram). So, x = [(w1,w3), (w0,w2)]   x = [[ 1.,  3.],        [ 0.,  2.]]  // Mapped input x to its vector representation y.   SparseEmbedding(x, y, 4, 5) = [[[  5.,   6.,   7.,   8.,   9.],                                  [ 15.,  16.,  17.,  18.,  19.]],                              [[  0.,   1.,   2.,   3.,   4.],\n                             [ 10.,  11.,  12.,  13.,  14.]]]  Defined in src/operator/tensor/indexing_op.cc:L254  Arguments   data::NDArray-or-SymbolicNode : The input array to the embedding operator.  weight::NDArray-or-SymbolicNode : The embedding weight matrix.  input_dim::int, required : Vocabulary size of the input indices.  output_dim::int, required : Dimension of the embedding vectors.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Data type of weight.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_bipartite_matching     Method .  _contrib_bipartite_matching(data, is_ascend, threshold, topk)  Compute bipartite matching.   The matching is performed on score matrix with shape [B, N, M]   B: batch_size  N: number of rows to match  M: number of columns as reference to be matched against.   Returns:   x : matched column indices. -1 indicating non-matched elements in rows.   y : matched row indices.  Note::  Zero gradients are back-propagated in this op for now.  Example::  s = [[0.5, 0.6], [0.1, 0.2], [0.3, 0.4]]\nx, y = bipartite_matching(x, threshold=1e-12, is_ascend=False)\nx = [1, -1, 0]\ny = [2\uff0c 0]  Defined in src/operator/contrib/bounding_box.cc:L169  Arguments   data::NDArray-or-SymbolicNode : The input  is_ascend::boolean, optional, default=0 : Use ascend order for scores instead of descending. Please set threshold accordingly.  threshold::float, required : Ignore matching when score   thresh, if is_ascend=false, or ignore score   thresh, if is_ascend=true.  topk::int, optional, default='-1' : Limit the number of matches to topk, set -1 for no limit  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_box_iou     Method .  _contrib_box_iou(lhs, rhs, format)  Bounding box overlap of two arrays.   The overlap is defined as Intersection-over-Union, aka, IOU.   lhs: (a_1, a_2, ..., a_n, 4) array  rhs: (b_1, b_2, ..., b_n, 4) array  output: (a_1, a_2, ..., a_n, b_1, b_2, ..., b_n) array   Note::  Zero gradients are back-propagated in this op for now.  Example::  x = [[0.5, 0.5, 1.0, 1.0], [0.0, 0.0, 0.5, 0.5]]\ny = [0.25, 0.25, 0.75, 0.75]\nbox_iou(x, y, format='corner') = [[0.1428], [0.1428]]  Defined in src/operator/contrib/bounding_box.cc:L123  Arguments   lhs::NDArray-or-SymbolicNode : The first input  rhs::NDArray-or-SymbolicNode : The second input  format::{'center', 'corner'},optional, default='corner' : The box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_box_nms     Method .  _contrib_box_nms(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)  Apply non-maximum suppression to input.  The output will be sorted in descending order according to  score . Boxes with overlaps larger than  overlap_thresh  and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.  During back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.  Input requirements:   Input tensor have at least 2 dimensions, (n, k), any higher dims will be regarded   as batch, e.g. (a, b, c, d, n, k) == (a b c*d, n, k)   n is the number of boxes in each batch  k is the width of each box item.   By default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.   id_index : optional, use -1 to ignore, useful if  force_suppress=False , which means   we will skip highly overlapped boxes if one is  apple  while the other is  car .   coord_start : required, default=2, the starting index of the 4 coordinates.   Two formats are supported:    corner : [xmin, ymin, xmax, ymax]    center : [x, y, width, height]   score_index : required, default=1, box score/confidence.   When two boxes overlap IOU    overlap_thresh , the one with smaller score will be suppressed.   in_format  and  out_format : default='corner', specify in/out box formats.   Examples::  x = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]  exe.backward  in_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]  Defined in src/operator/contrib/bounding_box.cc:L82  Arguments   data::NDArray-or-SymbolicNode : The input  overlap_thresh::float, optional, default=0.5 : Overlapping(IoU) threshold to suppress object with smaller score.  topk::int, optional, default='-1' : Apply nms to topk boxes with descending scores, -1 to no restriction.  coord_start::int, optional, default='2' : Start index of the consecutive 4 coordinates.  score_index::int, optional, default='1' : Index of the scores/confidence of boxes.  id_index::int, optional, default='-1' : Optional, index of the class categories, -1 to disable.  force_suppress::boolean, optional, default=0 : Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category  in_format::{'center', 'corner'},optional, default='corner' : The input box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   out_format::{'center', 'corner'},optional, default='corner' : The output box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_box_non_maximum_suppression     Method .  _contrib_box_non_maximum_suppression(data, overlap_thresh, topk, coord_start, score_index, id_index, force_suppress, in_format, out_format)  _contrib_box_non_maximum_suppression is an alias of _contrib_box_nms.  Apply non-maximum suppression to input.  The output will be sorted in descending order according to  score . Boxes with overlaps larger than  overlap_thresh  and smaller scores will be removed and filled with -1, the corresponding position will be recorded for backward propogation.  During back-propagation, the gradient will be copied to the original position according to the input index. For positions that have been suppressed, the in_grad will be assigned 0. In summary, gradients are sticked to its boxes, will either be moved or discarded according to its original index in input.  Input requirements:   Input tensor have at least 2 dimensions, (n, k), any higher dims will be regarded   as batch, e.g. (a, b, c, d, n, k) == (a b c*d, n, k)   n is the number of boxes in each batch  k is the width of each box item.   By default, a box is [id, score, xmin, ymin, xmax, ymax, ...], additional elements are allowed.   id_index : optional, use -1 to ignore, useful if  force_suppress=False , which means   we will skip highly overlapped boxes if one is  apple  while the other is  car .   coord_start : required, default=2, the starting index of the 4 coordinates.   Two formats are supported:    corner : [xmin, ymin, xmax, ymax]    center : [x, y, width, height]   score_index : required, default=1, box score/confidence.   When two boxes overlap IOU    overlap_thresh , the one with smaller score will be suppressed.   in_format  and  out_format : default='corner', specify in/out box formats.   Examples::  x = [[0, 0.5, 0.1, 0.1, 0.2, 0.2], [1, 0.4, 0.1, 0.1, 0.2, 0.2],        [0, 0.3, 0.1, 0.1, 0.14, 0.14], [2, 0.6, 0.5, 0.5, 0.7, 0.8]]   box_nms(x, overlap_thresh=0.1, coord_start=2, score_index=1, id_index=0,       force_suppress=True, in_format='corner', out_typ='corner') =       [[2, 0.6, 0.5, 0.5, 0.7, 0.8], [0, 0.5, 0.1, 0.1, 0.2, 0.2],        [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]   out_grad = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],               [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]]  exe.backward  in_grad = [[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0, 0, 0, 0, 0, 0],              [0, 0, 0, 0, 0, 0], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]  Defined in src/operator/contrib/bounding_box.cc:L82  Arguments   data::NDArray-or-SymbolicNode : The input  overlap_thresh::float, optional, default=0.5 : Overlapping(IoU) threshold to suppress object with smaller score.  topk::int, optional, default='-1' : Apply nms to topk boxes with descending scores, -1 to no restriction.  coord_start::int, optional, default='2' : Start index of the consecutive 4 coordinates.  score_index::int, optional, default='1' : Index of the scores/confidence of boxes.  id_index::int, optional, default='-1' : Optional, index of the class categories, -1 to disable.  force_suppress::boolean, optional, default=0 : Optional, if set false and id_index is provided, nms will only apply to boxes belongs to the same category  in_format::{'center', 'corner'},optional, default='corner' : The input box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   out_format::{'center', 'corner'},optional, default='corner' : The output box encoding type.   \"corner\" means boxes are encoded as [xmin, ymin, xmax, ymax], \"center\" means boxes are encodes as [x, y, width, height].   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_count_sketch     Method .  _contrib_count_sketch(data, h, s, out_dim, processing_batch_size)  Apply CountSketch to input: map a d-dimension data to k-dimension data\"  .. note::  count_sketch  is only available on GPU.  Assume input data has shape (N, d), sign hash table s has shape (N, d), index hash table h has shape (N, d) and mapping dimension out_dim = k, each element in s is either +1 or -1, each element in h is random integer from 0 to k-1. Then the operator computs:  .. math::    out[h[i]] += data[i] * s[i]  Example::  out_dim = 5    x = [[1.2, 2.5, 3.4],[3.2, 5.7, 6.6]]    h = [[0, 3, 4]]    s = [[1, -1, 1]]    mx.contrib.ndarray.count_sketch(data=x, h=h, s=s, out_dim = 5) = [[1.2, 0, 0, -2.5, 3.4],                                                                      [3.2, 0, 0, -5.7, 6.6]]  Defined in src/operator/contrib/count_sketch.cc:L67  Arguments   data::NDArray-or-SymbolicNode : Input data to the CountSketchOp.  h::NDArray-or-SymbolicNode : The index vector  s::NDArray-or-SymbolicNode : The sign vector  out_dim::int, required : The output dimension.  processing_batch_size::int, optional, default='32' : How many sketch vectors to process at one time.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_ctc_loss     Method .  _contrib_ctc_loss(data, label, data_lengths, label_lengths, use_data_lengths, use_label_lengths, blank_label)  _contrib_ctc_loss is an alias of _contrib_CTCLoss.  Connectionist Temporal Classification Loss.  The shapes of the inputs and outputs:   data :  (sequence_length, batch_size, alphabet_size)  label :  (batch_size, label_sequence_length)  out :  (batch_size)   The  data  tensor consists of sequences of activation vectors (without applying softmax), with i-th channel in the last dimension corresponding to i-th label for i between 0 and alphabet_size-1 (i.e always 0-indexed). Alphabet size should include one additional value reserved for blank label. When  blank_label  is $\"first\"$, the $0$-th channel is be reserved for activation of blank label, or otherwise if it is \"last\", $(alphabet_size-1)$-th channel should be reserved for blank label.  $label$ is an index matrix of integers. When  blank_label  is $\"first\"$, the value 0 is then reserved for blank label, and should not be passed in this matrix. Otherwise, when  blank_label  is $\"last\"$, the value  (alphabet_size-1)  is reserved for blank label.  If a sequence of labels is shorter than  label_sequence_length , use the special padding value at the end of the sequence to conform it to the correct length. The padding value is  0  when  blank_label  is $\"first\"$, and  -1  otherwise.  For example, suppose the vocabulary is  [a, b, c] , and in one batch we have three sequences 'ba', 'cbb', and 'abac'. When  blank_label  is $\"first\"$, we can index the labels as  {'a': 1, 'b': 2, 'c': 3} , and we reserve the 0-th channel for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[2, 1, 0, 0], [3, 2, 2, 0], [1, 2, 1, 3]]  When  blank_label  is $\"last\"$, we can index the labels as  {'a': 0, 'b': 1, 'c': 2} , and we reserve the channel index 3 for blank label in data tensor. The resulting  label  tensor should be padded to be::  [[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]  $out$ is a list of CTC loss values, one per example in the batch.  See  Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks , A. Graves  et al . for more information on the definition and the algorithm.  Defined in src/operator/contrib/ctc_loss.cc:L115  Arguments   data::NDArray-or-SymbolicNode : Input data to the ctc_loss op.  label::NDArray-or-SymbolicNode : Ground-truth labels for the loss.  data_lengths::NDArray-or-SymbolicNode : Lengths of data for each of the samples. Only required when use_data_lengths is true.  label_lengths::NDArray-or-SymbolicNode : Lengths of labels for each of the samples. Only required when use_label_lengths is true.  use_data_lengths::boolean, optional, default=0 : Whether the data lenghts are decided by  data_lengths . If false, the lengths are equal to the max sequence length.  use_label_lengths::boolean, optional, default=0 : Whether the label lenghts are decided by  label_lengths , or derived from  padding_mask . If false, the lengths are derived from the first occurrence of the value of  padding_mask . The value of  padding_mask  is $0$ when first CTC label is reserved for blank, and $-1$ when last label is reserved for blank. See  blank_label .  blank_label::{'first', 'last'},optional, default='first' : Set the label that is reserved for blank label.If \"first\", 0-th label is reserved, and label values for tokens in the vocabulary are between $1$ and $alphabet_size-1$, and the padding mask is $-1$. If \"last\", last label value $alphabet_size-1$ is reserved for blank label instead, and label values for tokens in the vocabulary are between $0$ and $alphabet_size-2$, and the padding mask is $0$.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_dequantize     Method .  _contrib_dequantize(input, min_range, max_range, out_type)  Dequantize the input tensor into a float tensor. [min_range, max_range] are scalar floats that spcify the range for the output data.  Each value of the tensor will undergo the following:  out[i] = min_range + (in[i] * (max_range - min_range) / range(INPUT_TYPE))  here  range(T) = numeric_limits T ::max() - numeric_limits T ::min()  Defined in src/operator/contrib/dequantize.cc:L41  Arguments   input::NDArray-or-SymbolicNode : A ndarray/symbol of type  uint8  min_range::NDArray-or-SymbolicNode : The minimum scalar value possibly produced for the input  max_range::NDArray-or-SymbolicNode : The maximum scalar value possibly produced for the input  out_type::{'float32'}, required : Output data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_fft     Method .  _contrib_fft(data, compute_size)  Apply 1D FFT to input\"  .. note::  fft  is only available on GPU.  Currently accept 2 input data shapes: (N, d) or (N1, N2, N3, d), data can only be real numbers. The output data has shape: (N, 2 d) or (N1, N2, N3, 2 d). The format is: [real0, imag0, real1, imag1, ...].  Example::  data = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.fft(data = mx.nd.array(data,ctx = mx.gpu(0)))  Defined in src/operator/contrib/fft.cc:L56  Arguments   data::NDArray-or-SymbolicNode : Input data to the FFTOp.  compute_size::int, optional, default='128' : Maximum size of sub-batch to be forwarded at one time  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_ifft     Method .  _contrib_ifft(data, compute_size)  Apply 1D ifft to input\"  .. note::  ifft  is only available on GPU.  Currently accept 2 input data shapes: (N, d) or (N1, N2, N3, d). Data is in format: [real0, imag0, real1, imag1, ...]. Last dimension must be an even number. The output data has shape: (N, d/2) or (N1, N2, N3, d/2). It is only the real part of the result.  Example::  data = np.random.normal(0,1,(3,4))    out = mx.contrib.ndarray.ifft(data = mx.nd.array(data,ctx = mx.gpu(0)))  Defined in src/operator/contrib/ifft.cc:L58  Arguments   data::NDArray-or-SymbolicNode : Input data to the IFFTOp.  compute_size::int, optional, default='128' : Maximum size of sub-batch to be forwarded at one time  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._contrib_quantize     Method .  _contrib_quantize(input, min_range, max_range, out_type)  Quantize a input tensor from float to  out_type , with user-specified  min_range  and  max_range .  [min_range, max_range] are scalar floats that spcify the range for the input data. Each value of the tensor will undergo the following:  out[i] = (in[i] - min_range) * range(OUTPUT_TYPE) / (max_range - min_range)  here  range(T) = numeric_limits T ::max() - numeric_limits T ::min()  Defined in src/operator/contrib/quantize.cc:L41  Arguments   input::NDArray-or-SymbolicNode : A ndarray/symbol of type  float32  min_range::NDArray-or-SymbolicNode : The minimum scalar value possibly produced for the input  max_range::NDArray-or-SymbolicNode : The maximum scalar value possibly produced for the input  out_type::{'uint8'},optional, default='uint8' : Output data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._copy     Method .  _copy(data)  Returns a copy of the input.  From:src/operator/tensor/elemwise_unary_op_basic.cc:112  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._copyto     Method .  _copyto(data)  Arguments   data::NDArray : input data  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._crop_assign     Method .  _crop_assign(lhs, rhs, begin, end, step)  _crop_assign is an alias of _slice_assign.  Assign the rhs to a cropped subset of lhs.  Requirements   output should be explicitly given and be the same as lhs.  lhs and rhs are of the same data type, and on the same device.   From:src/operator/tensor/matrix_op.cc:381  Arguments   lhs::NDArray-or-SymbolicNode : Source input  rhs::NDArray-or-SymbolicNode : value to assign  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._crop_assign_scalar     Method .  _crop_assign_scalar(data, scalar, begin, end, step)  _crop_assign_scalar is an alias of _slice_assign_scalar.  (Assign the scalar to a cropped subset of the input.  Requirements   output should be explicitly given and be the same as input   )  From:src/operator/tensor/matrix_op.cc:406  Arguments   data::NDArray-or-SymbolicNode : Source input  scalar::float, optional, default=0 : The scalar value for assignment.  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._cvcopyMakeBorder     Method .  _cvcopyMakeBorder(src, top, bot, left, right, type, value, values)  Pad image border with OpenCV.   Arguments   src::NDArray : source image  top::int, required : Top margin.  bot::int, required : Bottom margin.  left::int, required : Left margin.  right::int, required : Right margin.  type::int, optional, default='0' : Filling type (default=cv2.BORDER_CONSTANT).  value::double, optional, default=0 : (Deprecated! Use $values$ instead.) Fill with single value.  values::tuple of  double , optional, default=[] : Fill with value(RGB[A] or gray), up to 4 channels.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._cvimdecode     Method .  _cvimdecode(buf, flag, to_rgb)  Decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.  Arguments   buf::NDArray : Buffer containing binary encoded image  flag::int, optional, default='1' : Convert decoded image to grayscale (0) or color (1).  to_rgb::boolean, optional, default=1 : Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._cvimread     Method .  _cvimread(filename, flag, to_rgb)  Read and decode image with OpenCV.  Note: return image in RGB by default, instead of OpenCV's default BGR.  Arguments   filename::string, required : Name of the image file to be loaded.  flag::int, optional, default='1' : Convert decoded image to grayscale (0) or color (1).  to_rgb::boolean, optional, default=1 : Whether to convert decoded image to mxnet's default RGB format (instead of opencv's default BGR).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._cvimresize     Method .  _cvimresize(src, w, h, interp)  Resize image with OpenCV.   Arguments   src::NDArray : source image  w::int, required : Width of resized image.  h::int, required : Height of resized image.  interp::int, optional, default='1' : Interpolation method (default=cv2.INTER_LINEAR).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._div_scalar     Method .  _div_scalar(data, scalar)  Divide an array with a scalar.  $_div_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L175  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._equal     Method .  _equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._equal_scalar     Method .  _equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._full     Method .  _full(shape, ctx, dtype, value)  fill target with a scalar value  Arguments   shape::Shape(tuple), optional, default=[] : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.  value::double, required : Value with which to fill newly created tensor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._grad_add     Method .  _grad_add(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater     Method .  _greater(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater_equal     Method .  _greater_equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater_equal_scalar     Method .  _greater_equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._greater_scalar     Method .  _greater_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._hypot     Method .  _hypot(lhs, rhs)  Given the \"legs\" of a right triangle, return its hypotenuse.  Defined in src/operator/tensor/elemwise_binary_op_extended.cc:L79  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._hypot_scalar     Method .  _hypot_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._identity_with_attr_like_rhs     Method .  _identity_with_attr_like_rhs(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : First input.  rhs::NDArray-or-SymbolicNode : Second input.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._imdecode     Method .  _imdecode(mean, index, x0, y0, x1, y1, c, size)  Decode an image, clip to (x0, y0, x1, y1), subtract mean, and write to buffer  Arguments   mean::NDArray-or-SymbolicNode : image mean  index::int : buffer position for output  x0::int : x0  y0::int : y0  x1::int : x1  y1::int : y1  c::int : channel  size::int : length of str_img  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser     Method .  _lesser(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser_equal     Method .  _lesser_equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser_equal_scalar     Method .  _lesser_equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._lesser_scalar     Method .  _lesser_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_gelqf     Method .  _linalg_gelqf(A)  LQ factorization for general matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , we compute the LQ factorization (LAPACK  gelqf , followed by  orglq ).  A  must have shape  (x, y)  with  x  = y , and must have full rank  =x . The LQ factorization consists of  L  with shape  (x, x)  and  Q  with shape  (x, y) , so that:  A  =  L  *  Q  Here,  L  is lower triangular (upper triangle equal to zero) with nonzero diagonal, and  Q  is row-orthonormal, meaning that  Q  *  Q \\ :sup: T  is equal to the identity matrix of shape  (x, x) .  If  n 2 ,  gelqf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]  // Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]  Defined in src/operator/tensor/la_op.cc:L529  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be factorized  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_gemm     Method .  _linalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)  Performs general matrix multiplication and accumulation. Input are tensors  A ,  B ,  C , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B ) +  beta  *  C  Here,  alpha  and  beta  are scalar parameters, and  op()  is either the identity or matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]  // Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]  Defined in src/operator/tensor/la_op.cc:L69  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  C::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.  beta::double, optional, default=1 : Scalar factor multiplied with C.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_gemm2     Method .  _linalg_gemm2(A, B, transpose_a, transpose_b, alpha)  Performs general matrix multiplication. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B )  Here  alpha  is a scalar parameter and  op()  is either the identity or the matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]  // Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]  Defined in src/operator/tensor/la_op.cc:L128  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_potrf     Method .  _linalg_potrf(A)  Performs Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the Cholesky factor  L  of the symmetric, positive definite matrix  A  is computed.  L  is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:  A  =  L  *  L \\ :sup: T  If  n 2 ,  potrf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]  // Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]  Defined in src/operator/tensor/la_op.cc:L178  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be decomposed  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_potri     Method .  _linalg_potri(A)  Performs matrix inversion from a Cholesky factorization. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:  out  =  A \\ :sup: -T  *  A \\ :sup: -1  In other words, if  A  is the Cholesky factor of a symmetric positive definite matrix  B  (obtained by  potrf ), then  out  =  B \\ :sup: -1  If  n 2 ,  potri  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  .. note:: Use this operator only if you are certain you need the inverse of  B , and           cannot use the Cholesky factor  A  ( potrf ), together with backsubstitution           ( trsm ). The latter is numerically much safer, and also cheaper.  Examples::  // Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]  // Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]  Defined in src/operator/tensor/la_op.cc:L236  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_sumlogdiag     Method .  _linalg_sumlogdiag(A)  Computes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).  If  n 2 ,  sumlogdiag  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]  // Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]  Defined in src/operator/tensor/la_op.cc:L405  Arguments   A::NDArray-or-SymbolicNode : Tensor of square matrices  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_syevd     Method .  _linalg_syevd(A)  Eigendecomposition for symmetric matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  must be symmetric, of shape  (x, x) . We compute the eigendecomposition, resulting in the orthonormal matrix  U  of eigenvectors, shape  (x, x) , and the vector  L  of eigenvalues, shape  (x,) , so that:  U  *  A  =  diag(L)  *  U  Here:  U  *  U \\ :sup: T  =  U \\ :sup: T  *  U  =  I  where  I  is the identity matrix. Also,  L(0)  = L(1)  = L(2)  = ...  (ascending order).  If  n 2 ,  syevd  is performed separately on the trailing two dimensions of  A  (batch mode). In this case,  U  has  n  dimensions like  A , and  L  has  n-1  dimensions.  .. note:: The operator supports float32 and float64 data types only.  .. note:: Derivatives for this operator are defined only if  A  is such that all its           eigenvalues are distinct, and the eigengaps are not too small. If you need           gradients, do not apply this operator to matrices with multiple eigenvalues.  Examples::  // Single symmetric eigendecomposition    A = [[1., 2.], [2., 4.]]    U, L = syevd(A)    U = [[0.89442719, -0.4472136],         [0.4472136, 0.89442719]]    L = [0., 5.]  // Batch symmetric eigendecomposition    A = [[[1., 2.], [2., 4.]],         [[1., 2.], [2., 5.]]]    U, L = syevd(A)    U = [[[0.89442719, -0.4472136],          [0.4472136, 0.89442719]],         [[0.92387953, -0.38268343],          [0.38268343, 0.92387953]]]    L = [[0., 5.],         [0.17157288, 5.82842712]]  Defined in src/operator/tensor/la_op.cc:L598  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be factorized  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_syrk     Method .  _linalg_syrk(A, transpose, alpha)  Multiplication of matrix with its transpose. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the operator performs the BLAS3 function  syrk :  out  =  alpha  *  A  *  A \\ :sup: T  if  transpose=False , or  out  =  alpha  *  A \\ :sup: T  \\ *  A  if  transpose=True .  If  n 2 ,  syrk  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]  // Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]  Defined in src/operator/tensor/la_op.cc:L461  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  transpose::boolean, optional, default=0 : Use transpose of input matrix.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_trmm     Method .  _linalg_trmm(A, B, transpose, rightside, alpha)  Performs multiplication with a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trmm :  out  =  alpha  *  op \\ ( A ) *  B  if  rightside=False , or  out  =  alpha  *  B  *  op \\ ( A )  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trmm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]  // Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L293  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._linalg_trsm     Method .  _linalg_trsm(A, B, transpose, rightside, alpha)  Solves matrix equation involving a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trsm , solving for  out  in:  op \\ ( A ) *  out  =  alpha  *  B  if  rightside=False , or  out  *  op \\ ( A ) =  alpha  *  B  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trsm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]  // Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L356  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._maximum     Method .  _maximum(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._maximum_scalar     Method .  _maximum_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minimum     Method .  _minimum(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minimum_scalar     Method .  _minimum_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minus     Method .  _minus(lhs, rhs)  _minus is an alias of elemwise_sub.  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._minus_scalar     Method .  _minus_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._mod     Method .  _mod(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._mod_scalar     Method .  _mod_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._mul     Method .  _mul(lhs, rhs)  _mul is an alias of elemwise_mul.  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._mul_scalar     Method .  _mul_scalar(data, scalar)  Multiply an array with a scalar.  $_mul_scalar$ only operates on data array of input if input is sparse.  For example, if input of shape (100, 100) has only 2 non zero elements, i.e. input.data = [5, 6], scalar = nan, it will result output.data = [nan, nan] instead of 10000 nans.  Defined in src/operator/tensor/elemwise_binary_scalar_op_basic.cc:L153  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._not_equal     Method .  _not_equal(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._not_equal_scalar     Method .  _not_equal_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._onehot_encode     Method .  _onehot_encode(lhs, rhs)  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._ones     Method .  _ones(shape, ctx, dtype)  fill target with ones  Arguments   shape::Shape(tuple), optional, default=[] : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._plus     Method .  _plus(lhs, rhs)  _plus is an alias of elemwise_add.  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._plus_scalar     Method .  _plus_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._power     Method .  _power(lhs, rhs)  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._power_scalar     Method .  _power_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._random_exponential     Method .  _random_exponential(lam, shape, ctx, dtype)  Draw random samples from an exponential distribution.  Samples are distributed according to an exponential distribution parametrized by  lambda  (rate).  Example::  exponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]  Defined in src/operator/random/sample_op.cc:L115  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the exponential distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._random_gamma     Method .  _random_gamma(alpha, beta, shape, ctx, dtype)  Draw random samples from a gamma distribution.  Samples are distributed according to a gamma distribution parametrized by  alpha  (shape) and  beta  (scale).  Example::  gamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]  Defined in src/operator/random/sample_op.cc:L100  Arguments   alpha::float, optional, default=1 : Alpha parameter (shape) of the gamma distribution.  beta::float, optional, default=1 : Beta parameter (scale) of the gamma distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._random_generalized_negative_binomial     Method .  _random_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)  Draw random samples from a generalized negative binomial distribution.  Samples are distributed according to a generalized negative binomial distribution parametrized by  mu  (mean) and  alpha  (dispersion).  alpha  is defined as  1/k  where  k  is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.  Example::  generalized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]  Defined in src/operator/random/sample_op.cc:L168  Arguments   mu::float, optional, default=1 : Mean of the negative binomial distribution.  alpha::float, optional, default=1 : Alpha (dispersion) parameter of the negative binomial distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._random_negative_binomial     Method .  _random_negative_binomial(k, p, shape, ctx, dtype)  Draw random samples from a negative binomial distribution.  Samples are distributed according to a negative binomial distribution parametrized by  k  (limit of unsuccessful experiments) and  p  (failure probability in each experiment). Samples will always be returned as a floating point data type.  Example::  negative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]  Defined in src/operator/random/sample_op.cc:L149  Arguments   k::int, optional, default='1' : Limit of unsuccessful experiments.  p::float, optional, default=1 : Failure probability in each experiment.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._random_normal     Method .  _random_normal(loc, scale, shape, ctx, dtype)  Draw random samples from a normal (Gaussian) distribution.  .. note:: The existing alias $normal$ is deprecated.  Samples are distributed according to a normal distribution parametrized by  loc  (mean) and  scale  (standard deviation).  Example::  normal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]  Defined in src/operator/random/sample_op.cc:L85  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._random_poisson     Method .  _random_poisson(lam, shape, ctx, dtype)  Draw random samples from a Poisson distribution.  Samples are distributed according to a Poisson distribution parametrized by  lambda  (rate). Samples will always be returned as a floating point data type.  Example::  poisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]  Defined in src/operator/random/sample_op.cc:L132  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the Poisson distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._random_uniform     Method .  _random_uniform(low, high, shape, ctx, dtype)  Draw random samples from a uniform distribution.  .. note:: The existing alias $uniform$ is deprecated.  Samples are uniformly distributed over the half-open interval  [low, high)  (includes  low , but excludes  high ).  Example::  uniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]  Defined in src/operator/random/sample_op.cc:L66  Arguments   low::float, optional, default=0 : Lower bound of the distribution.  high::float, optional, default=1 : Upper bound of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._rdiv_scalar     Method .  _rdiv_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._rminus_scalar     Method .  _rminus_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._rmod_scalar     Method .  _rmod_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._rpower_scalar     Method .  _rpower_scalar(data, scalar)  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_exponential     Method .  _sample_exponential(lam, shape, dtype)  Concurrent sampling from multiple exponential distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]  // Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]  Defined in src/operator/random/multisample_op.cc:L284  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_gamma     Method .  _sample_gamma(alpha, shape, dtype, beta)  Concurrent sampling from multiple gamma distributions with parameters  alpha  (shape) and  beta  (scale).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  alpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]  // Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]  // Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]  Defined in src/operator/random/multisample_op.cc:L282  Arguments   alpha::NDArray-or-SymbolicNode : Alpha (shape) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  beta::NDArray-or-SymbolicNode : Beta (scale) parameters of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_generalized_negative_binomial     Method .  _sample_generalized_negative_binomial(mu, shape, dtype, alpha)  Concurrent sampling from multiple generalized negative binomial distributions with parameters  mu  (mean) and  alpha  (dispersion).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  mu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]  // Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]  // Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]  Defined in src/operator/random/multisample_op.cc:L293  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  alpha::NDArray-or-SymbolicNode : Alpha (dispersion) parameters of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_multinomial     Method .  _sample_multinomial(data, shape, get_prob, dtype)  Concurrent sampling from multiple multinomial distributions.  data  is an  n  dimensional array whose last dimension has length  k , where  k  is the number of possible outcomes of each multinomial distribution. This operator will draw  shape  samples from each distribution. If shape is empty one sample will be drawn from each distribution.  If  get_prob  is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.  Note that the input distribution must be normalized, i.e.  data  must sum to 1 along its last axis.  Examples::  probs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]  // Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]  // Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]  // requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]  Arguments   data::NDArray-or-SymbolicNode : Distribution probabilities. Must sum to one on the last axis.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  get_prob::boolean, optional, default=0 : Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.  dtype::{'int32'},optional, default='int32' : DType of the output in case this can't be inferred. Only support int32 for now.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_negative_binomial     Method .  _sample_negative_binomial(k, shape, dtype, p)  Concurrent sampling from multiple negative binomial distributions with parameters  k  (failure limit) and  p  (failure probability).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  k = [ 20, 49 ]    p = [ 0.4 , 0.77 ]  // Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]  // Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]  Defined in src/operator/random/multisample_op.cc:L289  Arguments   k::NDArray-or-SymbolicNode : Limits of unsuccessful experiments.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  p::NDArray-or-SymbolicNode : Failure probabilities in each experiment.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_normal     Method .  _sample_normal(mu, shape, dtype, sigma)  Concurrent sampling from multiple normal distributions with parameters  mu  (mean) and  sigma  (standard deviation).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  mu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]  // Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]  Defined in src/operator/random/multisample_op.cc:L279  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  sigma::NDArray-or-SymbolicNode : Standard deviations of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_poisson     Method .  _sample_poisson(lam, shape, dtype)  Concurrent sampling from multiple Poisson distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Samples will always be returned as a floating point data type.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]  // Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]  Defined in src/operator/random/multisample_op.cc:L286  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sample_uniform     Method .  _sample_uniform(low, shape, dtype, high)  Concurrent sampling from multiple uniform distributions on the intervals given by  [low,high) .  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  low = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]  // Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]  Defined in src/operator/random/multisample_op.cc:L277  Arguments   low::NDArray-or-SymbolicNode : Lower bounds of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  high::NDArray-or-SymbolicNode : Upper bounds of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._scatter_elemwise_div     Method .  _scatter_elemwise_div(lhs, rhs)  Divides arguments element-wise.  If the left-hand-side input is 'row_sparse', then only the values which exist in the left-hand sparse array are computed.  The 'missing' values are ignored.  The storage type of $_scatter_elemwise_div$ output depends on storage types of inputs   _scatter_elemwise_div(row_sparse, row_sparse) = row_sparse  _scatter_elemwise_div(row_sparse, dense) = row_sparse  _scatter_elemwise_div(row_sparse, csr) = row_sparse  otherwise, $_scatter_elemwise_div$ behaves exactly like elemwise_div and generates output   with default storage  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._scatter_minus_scalar     Method .  _scatter_minus_scalar(data, scalar)  Subtracts a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.  The storage type of $_scatter_minus_scalar$ output depends on storage types of inputs   _scatter_minus_scalar(row_sparse, scalar) = row_sparse  _scatter_minus_scalar(csr, scalar) = csr  otherwise, $_scatter_minus_scalar$ behaves exactly like _minus_scalar and generates output   with default storage  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._scatter_plus_scalar     Method .  _scatter_plus_scalar(data, scalar)  Adds a scalar to a tensor element-wise.  If the left-hand-side input is 'row_sparse' or 'csr', then only the values which exist in the left-hand sparse array are computed. The 'missing' values are ignored.  The storage type of $_scatter_plus_scalar$ output depends on storage types of inputs   _scatter_plus_scalar(row_sparse, scalar) = row_sparse  _scatter_plus_scalar(csr, scalar) = csr  otherwise, $_scatter_plus_scalar$ behaves exactly like _plus_scalar and generates output   with default storage  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._scatter_set_nd     Method .  _scatter_set_nd(data, indices, shape)  This operator has the same functionality as scatter_nd except that it does not reset the elements not indexed by the input index  NDArray  in the input data  NDArray .  .. note:: This operator is for internal use only.  Examples::  data = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   out = [[1, 1], [1, 1]]   scatter_nd(data=data, indices=indices, out=out)   out = [[0, 1], [2, 3]]  Arguments   data::NDArray-or-SymbolicNode : data  indices::NDArray-or-SymbolicNode : indices  shape::Shape(tuple), required : Shape of output.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._slice_assign     Method .  _slice_assign(lhs, rhs, begin, end, step)  Assign the rhs to a cropped subset of lhs.  Requirements   output should be explicitly given and be the same as lhs.  lhs and rhs are of the same data type, and on the same device.   From:src/operator/tensor/matrix_op.cc:381  Arguments   lhs::NDArray-or-SymbolicNode : Source input  rhs::NDArray-or-SymbolicNode : value to assign  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._slice_assign_scalar     Method .  _slice_assign_scalar(data, scalar, begin, end, step)  (Assign the scalar to a cropped subset of the input.  Requirements   output should be explicitly given and be the same as input   )  From:src/operator/tensor/matrix_op.cc:406  Arguments   data::NDArray-or-SymbolicNode : Source input  scalar::float, optional, default=0 : The scalar value for assignment.  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_ElementWiseSum     Method .  _sparse_ElementWiseSum(args)  _sparse_ElementWiseSum is an alias of add_n.  Note : _sparse_ElementWiseSum takes variable number of positional inputs. So instead of calling as _sparse_ElementWiseSum([x, y, z], num_args=3), one should call via _sparse_ElementWiseSum(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_abs     Method .  _sparse_abs(data)  _sparse_abs is an alias of abs.  Returns element-wise absolute value of the input.  Example::  abs([-2, 0, 3]) = [2, 0, 3]  The storage type of $abs$ output depends upon the input storage type:   abs(default) = default  abs(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L386  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_adam_update     Method .  _sparse_adam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)  _sparse_adam_update is an alias of adam_update.  Update function for Adam optimizer. Adam is seen as a generalization of AdaGrad.  Adam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).  .. math::  g_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }  It updates the weights using::  m = beta1 m + (1-beta1) grad  v = beta2 v + (1-beta2) (grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)  If w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::  for row in grad.indices:      m[row] = beta1 m[row] + (1-beta1) grad[row]      v[row] = beta2 v[row] + (1-beta2) (grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)  Defined in src/operator/optimizer_op.cc:L208  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mean::NDArray-or-SymbolicNode : Moving mean  var::NDArray-or-SymbolicNode : Moving variance  lr::float, required : Learning rate  beta1::float, optional, default=0.9 : The decay rate for the 1st moment estimates.  beta2::float, optional, default=0.999 : The decay rate for the 2nd moment estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_add_n     Method .  _sparse_add_n(args)  _sparse_add_n is an alias of add_n.  Note : _sparse_add_n takes variable number of positional inputs. So instead of calling as _sparse_add_n([x, y, z], num_args=3), one should call via _sparse_add_n(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_arccos     Method .  _sparse_arccos(data)  _sparse_arccos is an alias of arccos.  Returns element-wise inverse cosine of the input array.  The input should be in range  [-1, 1] . The output is in the closed interval :math: [0, \\pi]  .. math::    arccos([-1, -.707, 0, .707, 1]) = [\\pi, 3\\pi/4, \\pi/2, \\pi/4, 0]  The storage type of $arccos$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L123  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_arccosh     Method .  _sparse_arccosh(data)  _sparse_arccosh is an alias of arccosh.  Returns the element-wise inverse hyperbolic cosine of the input array, \ncomputed element-wise.  The storage type of $arccosh$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L264  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_arcsin     Method .  _sparse_arcsin(data)  _sparse_arcsin is an alias of arcsin.  Returns element-wise inverse sine of the input array.  The input should be in the range  [-1, 1] . The output is in the closed interval of [:math: -\\pi/2 , :math: \\pi/2 ].  .. math::    arcsin([-1, -.707, 0, .707, 1]) = [-\\pi/2, -\\pi/4, 0, \\pi/4, \\pi/2]  The storage type of $arcsin$ output depends upon the input storage type:   arcsin(default) = default  arcsin(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L104  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_arcsinh     Method .  _sparse_arcsinh(data)  _sparse_arcsinh is an alias of arcsinh.  Returns the element-wise inverse hyperbolic sine of the input array, \ncomputed element-wise.  The storage type of $arcsinh$ output depends upon the input storage type:   arcsinh(default) = default  arcsinh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L250  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_arctan     Method .  _sparse_arctan(data)  _sparse_arctan is an alias of arctan.  Returns element-wise inverse tangent of the input array.  The output is in the closed interval :math: [-\\pi/2, \\pi/2]  .. math::    arctan([-1, 0, 1]) = [-\\pi/4, 0, \\pi/4]  The storage type of $arctan$ output depends upon the input storage type:   arctan(default) = default  arctan(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L144  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_arctanh     Method .  _sparse_arctanh(data)  _sparse_arctanh is an alias of arctanh.  Returns the element-wise inverse hyperbolic tangent of the input array, \ncomputed element-wise.  The storage type of $arctanh$ output depends upon the input storage type:   arctanh(default) = default  arctanh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L281  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_cast_storage     Method .  _sparse_cast_storage(data, stype)  _sparse_cast_storage is an alias of cast_storage.  Casts tensor storage type to the new type.  When an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:   for csr, zero values will not be retained  for row_sparse, row slices of all zeros will not be retained   The storage type of $cast_storage$ output depends on stype parameter:   cast_storage(csr, 'default') = default  cast_storage(row_sparse, 'default') = default  cast_storage(default, 'csr') = csr  cast_storage(default, 'row_sparse') = row_sparse   Example::  dense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]  Defined in src/operator/tensor/cast_storage.cc:L69  Arguments   data::NDArray-or-SymbolicNode : The input.  stype::{'csr', 'default', 'row_sparse'}, required : Output storage type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_ceil     Method .  _sparse_ceil(data)  _sparse_ceil is an alias of ceil.  Returns element-wise ceiling of the input.  The ceil of the scalar x is the smallest integer i, such that i  = x.  Example::  ceil([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  2.,  2.,  3.]  The storage type of $ceil$ output depends upon the input storage type:   ceil(default) = default  ceil(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L464  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_clip     Method .  _sparse_clip(data, a_min, a_max)  _sparse_clip is an alias of clip.  Clips (limits) the values in an array.  Given an interval, values outside the interval are clipped to the interval edges. Clipping $x$ between  a_min  and  a_x  would be::  clip(x, a_min, a_max) = max(min(x, a_max), a_min))  Example::  x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nclip(x,1,8) = [ 1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  8.]  The storage type of $clip$ output depends on storage types of inputs and the a_min, a_max \nparameter values:   clip(default) = default  clip(row_sparse, a_min  = 0, a_max  = 0) = row_sparse  clip(csr, a_min  = 0, a_max  = 0) = csr  clip(row_sparse, a_min   0, a_max   0) = default  clip(row_sparse, a_min   0, a_max   0) = default  clip(csr, a_min   0, a_max   0) = csr  clip(csr, a_min   0, a_max   0) = csr   Defined in src/operator/tensor/matrix_op.cc:L486  Arguments   data::NDArray-or-SymbolicNode : Input array.  a_min::float, required : Minimum value  a_max::float, required : Maximum value  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_cos     Method .  _sparse_cos(data)  _sparse_cos is an alias of cos.  Computes the element-wise cosine of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    cos([0, \\pi/4, \\pi/2]) = [1, 0.707, 0]  The storage type of $cos$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L63  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_cosh     Method .  _sparse_cosh(data)  _sparse_cosh is an alias of cosh.  Returns the hyperbolic cosine  of the input array, computed element-wise.  .. math::    cosh(x) = 0.5\\times(exp(x) + exp(-x))  The storage type of $cosh$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L216  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_degrees     Method .  _sparse_degrees(data)  _sparse_degrees is an alias of degrees.  Converts each element of the input array from radians to degrees.  .. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]  The storage type of $degrees$ output depends upon the input storage type:   degrees(default) = default  degrees(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L163  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_dot     Method .  _sparse_dot(lhs, rhs, transpose_a, transpose_b)  _sparse_dot is an alias of dot.  Dot product of two arrays.  $dot$'s behavior depends on the input array dimensions:   1-D arrays: inner product of vectors  2-D arrays: matrix multiplication   N-D arrays: a sum product over the last axis of the first input and the first axis of the second input  For example, given 3-D $x$ with shape  (n,m,k)  and $y$ with shape  (k,r,s) , the result array will have shape  (n,m,r,s) . It is computed by::  dot(x,y)[i,j,a,b] = sum(x[i,j,:]*y[:,a,b])  Example::  x = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))   y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))   dot(x,y)[0,0,1,1] = 0   sum(x[0,0,:]*y[:,1,1]) = 0    The storage type of $dot$ output depends on storage types of inputs and transpose options:   dot(csr, default) = default  dot(csr.T, default) = row_sparse  dot(csr, row_sparse) = default  dot(default, csr) = csr  otherwise, $dot$ generates output with default storage   Defined in src/operator/tensor/dot.cc:L62  Arguments   lhs::NDArray-or-SymbolicNode : The first input  rhs::NDArray-or-SymbolicNode : The second input  transpose_a::boolean, optional, default=0 : If true then transpose the first input before dot.  transpose_b::boolean, optional, default=0 : If true then transpose the second input before dot.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_elemwise_add     Method .  _sparse_elemwise_add(lhs, rhs)  _sparse_elemwise_add is an alias of elemwise_add.  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_elemwise_div     Method .  _sparse_elemwise_div(lhs, rhs)  _sparse_elemwise_div is an alias of elemwise_div.  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_elemwise_mul     Method .  _sparse_elemwise_mul(lhs, rhs)  _sparse_elemwise_mul is an alias of elemwise_mul.  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_elemwise_sub     Method .  _sparse_elemwise_sub(lhs, rhs)  _sparse_elemwise_sub is an alias of elemwise_sub.  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_exp     Method .  _sparse_exp(data)  _sparse_exp is an alias of exp.  Returns element-wise exponential value of the input.  .. math::    exp(x) = e^x \\approx 2.718^x  Example::  exp([0, 1, 2]) = [1., 2.71828175, 7.38905621]  The storage type of $exp$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L642  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_expm1     Method .  _sparse_expm1(data)  _sparse_expm1 is an alias of expm1.  Returns $exp(x) - 1$ computed element-wise on the input.  This function provides greater precision than $exp(x) - 1$ for small values of $x$.  The storage type of $expm1$ output depends upon the input storage type:   expm1(default) = default  expm1(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L721  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_fix     Method .  _sparse_fix(data)  _sparse_fix is an alias of fix.  Returns element-wise rounded value to the nearest \ninteger towards zero of the input.  Example::  fix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]  The storage type of $fix$ output depends upon the input storage type:   fix(default) = default  fix(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L521  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_floor     Method .  _sparse_floor(data)  _sparse_floor is an alias of floor.  Returns element-wise floor of the input.  The floor of the scalar x is the largest integer i, such that i  = x.  Example::  floor([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-3., -2.,  1.,  1.,  2.]  The storage type of $floor$ output depends upon the input storage type:   floor(default) = default  floor(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L483  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_ftrl_update     Method .  _sparse_ftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)  _sparse_ftrl_update is an alias of ftrl_update.  Update function for Ftrl optimizer. Referenced from  Ad Click Prediction: a View from the Trenches , available at http://dl.acm.org/citation.cfm?id=2488200.  It updates the weights using::  rescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad 2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad 2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z)   lamda1)  If w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::  for row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row] 2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row] 2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row])   lamda1)  Defined in src/operator/optimizer_op.cc:L341  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  z::NDArray-or-SymbolicNode : z  n::NDArray-or-SymbolicNode : Square of grad  lr::float, required : Learning rate  lamda1::float, optional, default=0.01 : The L1 regularization coefficient.  beta::float, optional, default=1 : Per-Coordinate Learning Rate beta.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_gamma     Method .  _sparse_gamma(data)  _sparse_gamma is an alias of gamma.  Returns the gamma function (extension of the factorial function \nto the reals), computed element-wise on the input array.  The storage type of $gamma$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_gammaln     Method .  _sparse_gammaln(data)  _sparse_gammaln is an alias of gammaln.  Returns element-wise log of the absolute value of the gamma function \nof the input.  The storage type of $gammaln$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_log     Method .  _sparse_log(data)  _sparse_log is an alias of log.  Returns element-wise Natural logarithmic value of the input.  The natural logarithm is logarithm in base  e , so that $log(exp(x)) = x$  The storage type of $log$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L654  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_log10     Method .  _sparse_log10(data)  _sparse_log10 is an alias of log10.  Returns element-wise Base-10 logarithmic value of the input.  $10**log10(x) = x$  The storage type of $log10$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L666  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_log1p     Method .  _sparse_log1p(data)  _sparse_log1p is an alias of log1p.  Returns element-wise $log(1 + x)$ value of the input.  This function is more accurate than $log(1 + x)$  for small $x$ so that :math: 1+x\\approx 1  The storage type of $log1p$ output depends upon the input storage type:   log1p(default) = default  log1p(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L703  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_log2     Method .  _sparse_log2(data)  _sparse_log2 is an alias of log2.  Returns element-wise Base-2 logarithmic value of the input.  $2**log2(x) = x$  The storage type of $log2$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L678  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_make_loss     Method .  _sparse_make_loss(data)  _sparse_make_loss is an alias of make_loss.  Make your own loss function in network construction.  This operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.  For example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::  cross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)  We will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.  The storage type of $make_loss$ output depends upon the input storage type:   make_loss(default) = default  make_loss(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L200  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_mean     Method .  _sparse_mean(data, axis, keepdims, exclude)  _sparse_mean is an alias of mean.  Computes the mean of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L101  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx._sparse_negative     Method .  _sparse_negative(data)  _sparse_negative is an alias of negative.  Numerical negative of the argument, element-wise.  The storage type of $negative$ output depends upon the input storage type:   negative(default) = default  negative(row_sparse) = row_sparse  negative(csr) = csr   Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_radians     Method .  _sparse_radians(data)  _sparse_radians is an alias of radians.  Converts each element of the input array from degrees to radians.  .. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]  The storage type of $radians$ output depends upon the input storage type:   radians(default) = default  radians(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L182  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_relu     Method .  _sparse_relu(data)  _sparse_relu is an alias of relu.  Computes rectified linear.  .. math::    max(features, 0)  The storage type of $relu$ output depends upon the input storage type:   relu(default) = default  relu(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L84  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_retain     Method .  _sparse_retain(data, indices)  pick rows specified by user input index array from a row sparse matrix and save them in the output sparse matrix.  Example::  data = [[1, 2], [3, 4], [5, 6]]   indices = [0, 1, 3]   shape = (4, 2)   rsp_in = row_sparse(data, indices)   to_retain = [0, 3]   rsp_out = retain(rsp_in, to_retain)   rsp_out.values = [[1, 2], [5, 6]]   rsp_out.indices = [0, 3]  The storage type of $retain$ output depends on storage types of inputs   retain(row_sparse, default) = row_sparse  otherwise, $retain$ is not supported   Defined in src/operator/tensor/sparse_retain.cc:L53  Arguments   data::NDArray-or-SymbolicNode : The input array for sparse_retain operator.  indices::NDArray-or-SymbolicNode : The index array of rows ids that will be retained.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_rint     Method .  _sparse_rint(data)  _sparse_rint is an alias of rint.  Returns element-wise rounded value to the nearest integer of the input.  .. note::   For input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.  For input $-n.5$ both $rint$ and $round$ returns $-n-1$.   Example::  rint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]  The storage type of $rint$ output depends upon the input storage type:   rint(default) = default  rint(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L445  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_round     Method .  _sparse_round(data)  _sparse_round is an alias of round.  Returns element-wise rounded value to the nearest integer of the input.  Example::  round([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  2., -2.,  2.,  2.]  The storage type of $round$ output depends upon the input storage type:   round(default) = default  round(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L424  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_rsqrt     Method .  _sparse_rsqrt(data)  _sparse_rsqrt is an alias of rsqrt.  Returns element-wise inverse square-root value of the input.  .. math::    rsqrt(x) = 1/\\sqrt{x}  Example::  rsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]  The storage type of $rsqrt$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L585  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sgd_mom_update     Method .  _sparse_sgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)  _sparse_sgd_mom_update is an alias of sgd_mom_update.  Momentum update function for Stochastic Gradient Descent (SDG) optimizer.  Momentum update has better convergence rates on neural networks. Mathematically it looks like below:  .. math::  v_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t  It updates the weights using::  v = momentum * v - learning_rate * gradient   weight += v  Where the parameter $momentum$ is the decay rate of momentum estimates at each epoch.  If weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::  for row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]  Defined in src/operator/optimizer_op.cc:L94  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mom::NDArray-or-SymbolicNode : Momentum  lr::float, required : Learning rate  momentum::float, optional, default=0 : The decay rate of momentum estimates at each epoch.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sgd_update     Method .  _sparse_sgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)  _sparse_sgd_update is an alias of sgd_update.  Update function for Stochastic Gradient Descent (SDG) optimizer.  It updates the weights using::  weight = weight - learning_rate * gradient  If weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::  for row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]  Defined in src/operator/optimizer_op.cc:L54  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  lr::float, required : Learning rate  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sigmoid     Method .  _sparse_sigmoid(data)  _sparse_sigmoid is an alias of sigmoid.  Computes sigmoid of x element-wise.  .. math::    y = 1 / (1 + exp(-x))  The storage type of $sigmoid$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L103  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sign     Method .  _sparse_sign(data)  _sparse_sign is an alias of sign.  Returns element-wise sign of the input.  Example::  sign([-2, 0, 3]) = [-1, 0, 1]  The storage type of $sign$ output depends upon the input storage type:   sign(default) = default  sign(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L405  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sin     Method .  _sparse_sin(data)  _sparse_sin is an alias of sin.  Computes the element-wise sine of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    sin([0, \\pi/4, \\pi/2]) = [0, 0.707, 1]  The storage type of $sin$ output depends upon the input storage type:   sin(default) = default  sin(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L46  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sinh     Method .  _sparse_sinh(data)  _sparse_sinh is an alias of sinh.  Returns the hyperbolic sine of the input array, computed element-wise.  .. math::    sinh(x) = 0.5\\times(exp(x) - exp(-x))  The storage type of $sinh$ output depends upon the input storage type:   sinh(default) = default  sinh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L201  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_slice     Method .  _sparse_slice(data, begin, end, step)  _sparse_slice is an alias of slice.  Slices a region of the array.  .. note:: $crop$ is deprecated. Use $slice$ instead.  This function returns a sliced array between the indices given by  begin  and  end  with the corresponding  step .  For an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m  = n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.  The resulting array's  k -th dimension contains elements from the  k -th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).  If the  k -th elements are  None  in the sequence of  begin ,  end , and  step , the following rule will be used to set default values. If  s_k  is  None , set  s_k=1 . If  s_k   0 , set  b_k=0 ,  e_k=d_k ; else, set  b_k=d_k-1 ,  e_k=-1 .  The storage type of $slice$ output depends on storage types of inputs   slice(csr) = csr  otherwise, $slice$ generates output with default storage   .. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.  Example::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]  Defined in src/operator/tensor/matrix_op.cc:L355  Arguments   data::NDArray-or-SymbolicNode : Source input  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sqrt     Method .  _sparse_sqrt(data)  _sparse_sqrt is an alias of sqrt.  Returns element-wise square-root value of the input.  .. math::    \\textrm{sqrt}(x) = \\sqrt{x}  Example::  sqrt([4, 9, 16]) = [2, 3, 4]  The storage type of $sqrt$ output depends upon the input storage type:   sqrt(default) = default  sqrt(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L565  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_square     Method .  _sparse_square(data)  _sparse_square is an alias of square.  Returns element-wise squared value of the input.  .. math::    square(x) = x^2  Example::  square([2, 3, 4]) = [4, 9, 16]  The storage type of $square$ output depends upon the input storage type:   square(default) = default  square(row_sparse) = row_sparse  square(csr) = csr   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L542  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_stop_gradient     Method .  _sparse_stop_gradient(data)  _sparse_stop_gradient is an alias of BlockGrad.  Stops gradient computation.  Stops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.  Example::  v1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)  executor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]  executor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L167  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_sum     Method .  _sparse_sum(data, axis, keepdims, exclude)  _sparse_sum is an alias of sum.  Computes the sum of array elements over given axes.  .. Note::  sum  and  sum_axis  are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.  Example::  data = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]  sum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]  sum(data, axis=[1,2])   [ 12.  19.  27.]  data = [[1,2,0],           [3,0,1],           [4,1,0]]  csr = cast_storage(data, 'csr')  sum(csr, axis=0)   [ 8.  3.  1.]  sum(csr, axis=1)   [ 3.  4.  5.]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L85  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx._sparse_tan     Method .  _sparse_tan(data)  _sparse_tan is an alias of tan.  Computes the element-wise tangent of the input array.  The input should be in radians (:math: 2\\pi  rad equals 360 degrees).  .. math::    tan([0, \\pi/4, \\pi/2]) = [0, 1, -inf]  The storage type of $tan$ output depends upon the input storage type:   tan(default) = default  tan(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L83  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_tanh     Method .  _sparse_tanh(data)  _sparse_tanh is an alias of tanh.  Returns the hyperbolic tangent of the input array, computed element-wise.  .. math::    tanh(x) = sinh(x) / cosh(x)  The storage type of $tanh$ output depends upon the input storage type:   tanh(default) = default  tanh(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L234  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_trunc     Method .  _sparse_trunc(data)  _sparse_trunc is an alias of trunc.  Return the element-wise truncated value of the input.  The truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.  Example::  trunc([-2.1, -1.9, 1.5, 1.9, 2.1]) = [-2., -1.,  1.,  1.,  2.]  The storage type of $trunc$ output depends upon the input storage type:   trunc(default) = default  trunc(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L503  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._sparse_zeros_like     Method .  _sparse_zeros_like(data)  _sparse_zeros_like is an alias of zeros_like.  Return an array of zeros with the same shape and type as the input array.  The storage type of $zeros_like$ output depends on the storage type of the input   zeros_like(row_sparse) = row_sparse  zeros_like(csr) = csr  zeros_like(default) = default   Examples::  x = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]  zeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]  Arguments   data::NDArray-or-SymbolicNode : The input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx._square_sum     Method .  _square_sum(data, axis, keepdims, exclude)  Computes the square sum of array elements over a given axis for row-sparse matrix. This is a temporary solution for fusing ops square and sum together for row-sparse matrix to save memory for storing gradients. It will become deprecated once the functionality of fusing operators is finished in the future.  Example::  dns = mx.nd.array([[0, 0], [1, 2], [0, 0], [3, 4], [0, 0]])   rsp = dns.tostype('row_sparse')   sum = mx.nd._internal._square_sum(rsp, axis=1)   sum = [0, 5, 0, 25, 0]  Defined in src/operator/tensor/square_sum.cc:L63  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx._zeros     Method .  _zeros(shape, ctx, dtype)  fill target with zeros  Arguments   shape::Shape(tuple), optional, default=[] : The shape of the output  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned .Only used for imperative calls.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.adam_update     Method .  adam_update(weight, grad, mean, var, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)  Update function for Adam optimizer. Adam is seen as a generalization of AdaGrad.  Adam update consists of the following steps, where g represents gradient and m, v are 1st and 2nd order moment estimates (mean and variance).  .. math::  g_t = \\nabla J(W_{t-1})\\\n m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n W_t = W_{t-1} - \\alpha \\frac{ m_t }{ \\sqrt{ v_t } + \\epsilon }  It updates the weights using::  m = beta1 m + (1-beta1) grad  v = beta2 v + (1-beta2) (grad**2)  w += - learning_rate * m / (sqrt(v) + epsilon)  If w, m and v are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, m and v)::  for row in grad.indices:      m[row] = beta1 m[row] + (1-beta1) grad[row]      v[row] = beta2 v[row] + (1-beta2) (grad[row]**2)      w[row] += - learning_rate * m[row] / (sqrt(v[row]) + epsilon)  Defined in src/operator/optimizer_op.cc:L208  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mean::NDArray-or-SymbolicNode : Moving mean  var::NDArray-or-SymbolicNode : Moving variance  lr::float, required : Learning rate  beta1::float, optional, default=0.9 : The decay rate for the 1st moment estimates.  beta2::float, optional, default=0.999 : The decay rate for the 2nd moment estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.add_n     Method .  add_n(args)  Note : add_n takes variable number of positional inputs. So instead of calling as add_n([x, y, z], num_args=3), one should call via add_n(x, y, z), and num_args will be determined automatically.  Adds all input arguments element-wise.  .. math::    add_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n  $add_n$ is potentially more efficient than calling $add$ by  n  times.  The storage type of $add_n$ output depends on storage types of inputs   add_n(row_sparse, row_sparse, ..) = row_sparse  otherwise, $add_n$ generates output with default storage   Defined in src/operator/tensor/elemwise_sum.cc:L123  Arguments   args::NDArray-or-SymbolicNode[] : Positional input arguments  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argmax     Method .  argmax(data, axis, keepdims)  Returns indices of the maximum values along an axis.  In the case of multiple occurrences of maximum values, the indices corresponding to the first occurrence are returned.  Examples::  x = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]  // argmax along axis 0   argmax(x, axis=0) = [ 1.,  1.,  1.]  // argmax along axis 1   argmax(x, axis=1) = [ 2.,  2.]  // argmax along axis 1 keeping same dims as an input array   argmax(x, axis=1, keepdims=True) = [[ 2.],                                       [ 2.]]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L52  Arguments   data::NDArray-or-SymbolicNode : The input  axis::int or None, optional, default='None' : The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$  keepdims::boolean, optional, default=0 : If this is set to  True , the reduced axis is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argmax_channel     Method .  argmax_channel(data)  Returns argmax indices of each channel from the input array.  The result will be an NDArray of shape (num_channel,).  In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.  Examples::  x = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]  argmax_channel(x) = [ 2.,  2.]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L97  Arguments   data::NDArray-or-SymbolicNode : The input array  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argmin     Method .  argmin(data, axis, keepdims)  Returns indices of the minimum values along an axis.  In the case of multiple occurrences of minimum values, the indices corresponding to the first occurrence are returned.  Examples::  x = [[ 0.,  1.,  2.],        [ 3.,  4.,  5.]]  // argmin along axis 0   argmin(x, axis=0) = [ 0.,  0.,  0.]  // argmin along axis 1   argmin(x, axis=1) = [ 0.,  0.]  // argmin along axis 1 keeping same dims as an input array   argmin(x, axis=1, keepdims=True) = [[ 0.],                                       [ 0.]]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L77  Arguments   data::NDArray-or-SymbolicNode : The input  axis::int or None, optional, default='None' : The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$  keepdims::boolean, optional, default=0 : If this is set to  True , the reduced axis is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.argsort     Method .  argsort(data, axis, is_ascend)  Returns the indices that would sort an input array along the given axis.  This function performs sorting along the given axis and returns an array of indices having same shape as an input array that index data in sorted order.  Examples::  x = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]  // sort along axis -1   argsort(x) = [[ 1.,  0.,  2.],                 [ 0.,  2.,  1.]]  // sort along axis 0   argsort(x, axis=0) = [[ 1.,  0.,  1.]                         [ 0.,  1.,  0.]]  // flatten and then sort   argsort(x) = [ 3.,  1.,  5.,  0.,  4.,  2.]  Defined in src/operator/tensor/ordering_op.cc:L176  Arguments   data::NDArray-or-SymbolicNode : The input array  axis::int or None, optional, default='-1' : Axis along which to sort the input tensor. If not given, the flattened array is used. Default is -1.  is_ascend::boolean, optional, default=1 : Whether to sort in ascending or descending order.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.batch_dot     Method .  batch_dot(lhs, rhs, transpose_a, transpose_b)  Batchwise dot product.  $batch_dot$ is used to compute dot product of $x$ and $y$ when $x$ and $y$ are data in batch, namely 3D arrays in shape of  (batch_size, :, :) .  For example, given $x$ with shape  (batch_size, n, m)  and $y$ with shape  (batch_size, m, k) , the result array will have shape  (batch_size, n, k) , which is computed by::  batch_dot(x,y)[i,:,:] = dot(x[i,:,:], y[i,:,:])  Defined in src/operator/tensor/dot.cc:L110  Arguments   lhs::NDArray-or-SymbolicNode : The first input  rhs::NDArray-or-SymbolicNode : The second input  transpose_a::boolean, optional, default=0 : If true then transpose the first input before dot.  transpose_b::boolean, optional, default=0 : If true then transpose the second input before dot.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.batch_take     Method .  batch_take(a, indices)  Takes elements from a data batch.  .. note::    batch_take  is deprecated. Use  pick  instead.  Given an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::  output[i] = input[i, indices[i]]  Examples::  x = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]  // takes elements with specified indices   batch_take(x, [0,1,0]) = [ 1.  4.  5.]  Defined in src/operator/tensor/indexing_op.cc:L382  Arguments   a::NDArray-or-SymbolicNode : The input array  indices::NDArray-or-SymbolicNode : The index array  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_add     Method .  broadcast_add(lhs, rhs)  Returns element-wise sum of the input arrays with broadcasting.  broadcast_plus  is an alias to the function  broadcast_add .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]  broadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_axes     Method .  broadcast_axes(data, axis, size)  broadcast_axes is an alias of broadcast_axis.  Broadcasts the input array over particular axes.  Broadcasting is allowed on axes with size 1, such as from  (2,1,3,1)  to  (2,8,3,9) . Elements will be duplicated on the broadcasted axes.  Example::  // given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]  // broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L207  Arguments   data::NDArray-or-SymbolicNode : The input  axis::Shape(tuple), optional, default=[] : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=[] : Target sizes of the broadcasting axes.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_axis     Method .  broadcast_axis(data, axis, size)  Broadcasts the input array over particular axes.  Broadcasting is allowed on axes with size 1, such as from  (2,1,3,1)  to  (2,8,3,9) . Elements will be duplicated on the broadcasted axes.  Example::  // given x of shape (1,2,1)    x = [[[ 1.],          [ 2.]]]  // broadcast x on on axis 2    broadcast_axis(x, axis=2, size=3) = [[[ 1.,  1.,  1.],                                          [ 2.,  2.,  2.]]]    // broadcast x on on axes 0 and 2    broadcast_axis(x, axis=(0,2), size=(2,3)) = [[[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]],                                                 [[ 1.,  1.,  1.],                                                  [ 2.,  2.,  2.]]]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L207  Arguments   data::NDArray-or-SymbolicNode : The input  axis::Shape(tuple), optional, default=[] : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=[] : Target sizes of the broadcasting axes.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_div     Method .  broadcast_div(lhs, rhs)  Returns element-wise division of the input arrays with broadcasting.  Example::  x = [[ 6.,  6.,  6.],         [ 6.,  6.,  6.]]  y = [[ 2.],         [ 3.]]  broadcast_div(x, y) = [[ 3.,  3.,  3.],                           [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L157  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_equal     Method .  broadcast_equal(lhs, rhs)  Returns the result of element-wise  equal to  (==) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_equal(x, y) = [[ 0.,  0.,  0.],                             [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L46  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_greater     Method .  broadcast_greater(lhs, rhs)  Returns the result of element-wise  greater than  ( ) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_greater(x, y) = [[ 1.,  1.,  1.],                               [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L82  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_greater_equal     Method .  broadcast_greater_equal(lhs, rhs)  Returns the result of element-wise  greater than or equal to  ( =) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_greater_equal(x, y) = [[ 1.,  1.,  1.],                                     [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L100  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_hypot     Method .  broadcast_hypot(lhs, rhs)  Returns the hypotenuse of a right angled triangle, given its \"legs\" with broadcasting.  It is equivalent to doing :math: sqrt(x_1^2 + x_2^2) .  Example::  x = [[ 3.,  3.,  3.]]  y = [[ 4.],         [ 4.]]  broadcast_hypot(x, y) = [[ 5.,  5.,  5.],                             [ 5.,  5.,  5.]]  z = [[ 0.],         [ 4.]]  broadcast_hypot(x, z) = [[ 3.,  3.,  3.],                             [ 5.,  5.,  5.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L156  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_lesser     Method .  broadcast_lesser(lhs, rhs)  Returns the result of element-wise  lesser than  ( ) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_lesser(x, y) = [[ 0.,  0.,  0.],                              [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L118  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_lesser_equal     Method .  broadcast_lesser_equal(lhs, rhs)  Returns the result of element-wise  lesser than or equal to  ( =) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_lesser_equal(x, y) = [[ 0.,  0.,  0.],                                    [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L136  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_maximum     Method .  broadcast_maximum(lhs, rhs)  Returns element-wise maximum of the input arrays with broadcasting.  This function compares two input arrays and returns a new array having the element-wise maxima.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_maximum(x, y) = [[ 1.,  1.,  1.],                               [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L80  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_minimum     Method .  broadcast_minimum(lhs, rhs)  Returns element-wise minimum of the input arrays with broadcasting.  This function compares two input arrays and returns a new array having the element-wise minima.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_maximum(x, y) = [[ 0.,  0.,  0.],                               [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L115  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_minus     Method .  broadcast_minus(lhs, rhs)  broadcast_minus is an alias of broadcast_sub.  Returns element-wise difference of the input arrays with broadcasting.  broadcast_minus  is an alias to the function  broadcast_sub .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]  broadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_mod     Method .  broadcast_mod(lhs, rhs)  Returns element-wise modulo of the input arrays with broadcasting.  Example::  x = [[ 8.,  8.,  8.],         [ 8.,  8.,  8.]]  y = [[ 2.],         [ 3.]]  broadcast_mod(x, y) = [[ 0.,  0.,  0.],                           [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L190  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_mul     Method .  broadcast_mul(lhs, rhs)  Returns element-wise product of the input arrays with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_mul(x, y) = [[ 0.,  0.,  0.],                           [ 1.,  1.,  1.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L123  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_not_equal     Method .  broadcast_not_equal(lhs, rhs)  Returns the result of element-wise  not equal to  (!=) comparison operation with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_not_equal(x, y) = [[ 1.,  1.,  1.],                                 [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_logic.cc:L64  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_plus     Method .  broadcast_plus(lhs, rhs)  broadcast_plus is an alias of broadcast_add.  Returns element-wise sum of the input arrays with broadcasting.  broadcast_plus  is an alias to the function  broadcast_add .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_add(x, y) = [[ 1.,  1.,  1.],                           [ 2.,  2.,  2.]]  broadcast_plus(x, y) = [[ 1.,  1.,  1.],                            [ 2.,  2.,  2.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L51  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_power     Method .  broadcast_power(lhs, rhs)  Returns result of first array elements raised to powers from second array, element-wise with broadcasting.  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_power(x, y) = [[ 2.,  2.,  2.],                             [ 4.,  4.,  4.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_extended.cc:L45  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_sub     Method .  broadcast_sub(lhs, rhs)  Returns element-wise difference of the input arrays with broadcasting.  broadcast_minus  is an alias to the function  broadcast_sub .  Example::  x = [[ 1.,  1.,  1.],         [ 1.,  1.,  1.]]  y = [[ 0.],         [ 1.]]  broadcast_sub(x, y) = [[ 1.,  1.,  1.],                           [ 0.,  0.,  0.]]  broadcast_minus(x, y) = [[ 1.,  1.,  1.],                             [ 0.,  0.,  0.]]  Defined in src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L90  Arguments   lhs::NDArray-or-SymbolicNode : First input to the function  rhs::NDArray-or-SymbolicNode : Second input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.broadcast_to     Method .  broadcast_to(data, shape)  Broadcasts the input array to a new shape.  Broadcasting is a mechanism that allows NDArrays to perform arithmetic operations with arrays of different shapes efficiently without creating multiple copies of arrays. Also see,  Broadcasting  https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html _ for more explanation.  Broadcasting is allowed on axes with size 1, such as from  (2,1,3,1)  to  (2,8,3,9) . Elements will be duplicated on the broadcasted axes.  For example::  broadcast_to([[1,2,3]], shape=(2,3)) = [[ 1.,  2.,  3.],                                            [ 1.,  2.,  3.]])  The dimension which you do not want to change can also be kept as  0  which means copy the original value. So with  shape=(2,0) , we will obtain the same result as in the above example.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L231  Arguments   data::NDArray-or-SymbolicNode : The input  shape::Shape(tuple), optional, default=[] : The shape of the desired array. We can set the dim to zero if it's same as the original. E.g  A = broadcast_to(B, shape=(10, 0, 0))  has the same meaning as  A = broadcast_axis(B, axis=0, size=10) .  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.cast     Method .  cast(data, dtype)  cast is an alias of Cast.  Casts all elements of the input to a new type.  .. note:: $Cast$ is deprecated. Use $cast$ instead.  Example::  cast([0.9, 1.3], dtype='int32') = [0, 1]    cast([1e20, 11.1], dtype='float16') = [inf, 11.09375]    cast([300, 11.1, 10.9, -1, -3], dtype='uint8') = [44, 11, 10, 255, 253]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L311  Arguments   data::NDArray-or-SymbolicNode : The input.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Output data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.cast_storage     Method .  cast_storage(data, stype)  Casts tensor storage type to the new type.  When an NDArray with default storage type is cast to csr or row_sparse storage, the result is compact, which means:   for csr, zero values will not be retained  for row_sparse, row slices of all zeros will not be retained   The storage type of $cast_storage$ output depends on stype parameter:   cast_storage(csr, 'default') = default  cast_storage(row_sparse, 'default') = default  cast_storage(default, 'csr') = csr  cast_storage(default, 'row_sparse') = row_sparse   Example::  dense = [[ 0.,  1.,  0.],\n         [ 2.,  0.,  3.],\n         [ 0.,  0.,  0.],\n         [ 0.,  0.,  0.]]\n\n# cast to row_sparse storage type\nrsp = cast_storage(dense, 'row_sparse')\nrsp.indices = [0, 1]\nrsp.values = [[ 0.,  1.,  0.],\n              [ 2.,  0.,  3.]]\n\n# cast to csr storage type\ncsr = cast_storage(dense, 'csr')\ncsr.indices = [1, 0, 2]\ncsr.values = [ 1.,  2.,  3.]\ncsr.indptr = [0, 1, 3, 3, 3]  Defined in src/operator/tensor/cast_storage.cc:L69  Arguments   data::NDArray-or-SymbolicNode : The input.  stype::{'csr', 'default', 'row_sparse'}, required : Output storage type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.choose_element_0index     Method .  choose_element_0index(lhs, rhs)  Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.concat     Method .  concat(data, num_args, dim)  concat is an alias of Concat.  Note : concat takes variable number of positional inputs. So instead of calling as concat([x, y, z], num_args=3), one should call via concat(x, y, z), and num_args will be determined automatically.  Joins input arrays along a given axis.  .. note::  Concat  is deprecated. Use  concat  instead.  The dimensions of the input arrays should be the same except the axis along which they will be concatenated. The dimension of the output array along the concatenated axis will be equal to the sum of the corresponding dimensions of the input arrays.  Example::  x = [[1,1],[2,2]]    y = [[3,3],[4,4],[5,5]]    z = [[6,6], [7,7],[8,8]]  concat(x,y,z,dim=0) = [[ 1.,  1.],                           [ 2.,  2.],                           [ 3.,  3.],                           [ 4.,  4.],                           [ 5.,  5.],                           [ 6.,  6.],                           [ 7.,  7.],                           [ 8.,  8.]]  Note that you cannot concat x,y,z along dimension 1 since dimension    0 is not the same for all the input arrays.  concat(y,z,dim=1) = [[ 3.,  3.,  6.,  6.],                          [ 4.,  4.,  7.,  7.],                          [ 5.,  5.,  8.,  8.]]  Defined in src/operator/concat.cc:L104  Arguments   data::NDArray-or-SymbolicNode[] : List of arrays to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.crop     Method .  crop(data, begin, end, step)  crop is an alias of slice.  Slices a region of the array.  .. note:: $crop$ is deprecated. Use $slice$ instead.  This function returns a sliced array between the indices given by  begin  and  end  with the corresponding  step .  For an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m  = n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.  The resulting array's  k -th dimension contains elements from the  k -th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).  If the  k -th elements are  None  in the sequence of  begin ,  end , and  step , the following rule will be used to set default values. If  s_k  is  None , set  s_k=1 . If  s_k   0 , set  b_k=0 ,  e_k=d_k ; else, set  b_k=d_k-1 ,  e_k=-1 .  The storage type of $slice$ output depends on storage types of inputs   slice(csr) = csr  otherwise, $slice$ generates output with default storage   .. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.  Example::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]  Defined in src/operator/tensor/matrix_op.cc:L355  Arguments   data::NDArray-or-SymbolicNode : Source input  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.degrees     Method .  degrees(data)  Converts each element of the input array from radians to degrees.  .. math::    degrees([0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]) = [0, 90, 180, 270, 360]  The storage type of $degrees$ output depends upon the input storage type:   degrees(default) = default  degrees(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L163  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.elemwise_add     Method .  elemwise_add(lhs, rhs)  Adds arguments element-wise.  The storage type of $elemwise_add$ output depends on storage types of inputs   elemwise_add(row_sparse, row_sparse) = row_sparse  elemwise_add(csr, csr) = csr  otherwise, $elemwise_add$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.elemwise_div     Method .  elemwise_div(lhs, rhs)  Divides arguments element-wise.  The storage type of $elemwise_div$ output is always dense  Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.elemwise_mul     Method .  elemwise_mul(lhs, rhs)  Multiplies arguments element-wise.  The storage type of $elemwise_mul$ output depends on storage types of inputs   elemwise_mul(default, default) = default  elemwise_mul(row_sparse, row_sparse) = row_sparse  elemwise_mul(default, row_sparse) = default  elemwise_mul(row_sparse, default) = default  elemwise_mul(csr, csr) = csr  otherwise, $elemwise_mul$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.elemwise_sub     Method .  elemwise_sub(lhs, rhs)  Subtracts arguments element-wise.  The storage type of $elemwise_sub$ output depends on storage types of inputs   elemwise_sub(row_sparse, row_sparse) = row_sparse  elemwise_sub(csr, csr) = csr  otherwise, $elemwise_sub$ generates output with default storage   Arguments   lhs::NDArray-or-SymbolicNode : first input  rhs::NDArray-or-SymbolicNode : second input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.fill_element_0index     Method .  fill_element_0index(lhs, mhs, rhs)  Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  mhs::NDArray : Middle operand to the function.  rhs::NDArray : Right operand to the function.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.fix     Method .  fix(data)  Returns element-wise rounded value to the nearest \ninteger towards zero of the input.  Example::  fix([-2.1, -1.9, 1.9, 2.1]) = [-2., -1.,  1., 2.]  The storage type of $fix$ output depends upon the input storage type:   fix(default) = default  fix(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L521  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.flatten     Method .  flatten(data)  flatten is an alias of Flatten.  Flattens the input array into a 2-D array by collapsing the higher dimensions.  .. note::  Flatten  is deprecated. Use  flatten  instead.  For an input array with shape $(d1, d2, ..., dk)$,  flatten  operation reshapes the input array into an output array of shape $(d1, d2 ... dk)$.  Example::  x = [[\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n],\n[    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n]],\n\nflatten(x) = [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n   [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]]  Defined in src/operator/tensor/matrix_op.cc:L208  Arguments   data::NDArray-or-SymbolicNode : Input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.flip     Method .  flip(data, axis)  flip is an alias of reverse.  Reverses the order of elements along given axis while preserving array shape.  Note: reverse and flip are equivalent. We use reverse in the following examples.  Examples::  x = [[ 0.,  1.,  2.,  3.,  4.],        [ 5.,  6.,  7.,  8.,  9.]]  reverse(x, axis=0) = [[ 5.,  6.,  7.,  8.,  9.],                         [ 0.,  1.,  2.,  3.,  4.]]  reverse(x, axis=1) = [[ 4.,  3.,  2.,  1.,  0.],                         [ 9.,  8.,  7.,  6.,  5.]]  Defined in src/operator/tensor/matrix_op.cc:L662  Arguments   data::NDArray-or-SymbolicNode : Input data array  axis::Shape(tuple), required : The axis which to reverse elements.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.from_json     Method .  from_json(repr :: AbstractString, ::Type{SymbolicNode})  Load a  SymbolicNode  from a JSON string representation.  source  #  MXNet.mx.ftml_update     Method .  ftml_update(weight, grad, d, v, z, lr, beta1, beta2, epsilon, wd, rescale_grad, clip_gradient)  The FTML optimizer described in  FTML - Follow the Moving Leader in Deep Learning , available at http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf.  .. math::  g_t = \\nabla J(W_{t-1})\\\n v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\n d_t = \\frac{ (1 - \\beta_1^t) }{ \\eta_t } (\\sqrt{ \\frac{ v_t }{ 1 - \\beta_2^t } } + \\epsilon)  \\sigma_t = d_t - \\beta_1 d_{t-1}  z_t = \\beta_1 z_{ t-1 } + (1 - \\beta_1^t) g_t - \\sigma_t W_{t-1}  W_t = - \\frac{ z_t }{ d_t }  Defined in src/operator/optimizer_op.cc:L161  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  d::NDArray-or-SymbolicNode : Internal state $d_t$  v::NDArray-or-SymbolicNode : Internal state $v_t$  z::NDArray-or-SymbolicNode : Internal state $z_t$  lr::float, required : Learning rate  beta1::float, optional, default=0.9 : The decay rate for the 1st moment estimates.  beta2::float, optional, default=0.999 : The decay rate for the 2nd moment estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.ftrl_update     Method .  ftrl_update(weight, grad, z, n, lr, lamda1, beta, wd, rescale_grad, clip_gradient)  Update function for Ftrl optimizer. Referenced from  Ad Click Prediction: a View from the Trenches , available at http://dl.acm.org/citation.cfm?id=2488200.  It updates the weights using::  rescaled_grad = clip(grad * rescale_grad, clip_gradient)  z += rescaled_grad - (sqrt(n + rescaled_grad 2) - sqrt(n)) * weight / learning_rate  n += rescaled_grad 2  w = (sign(z) * lamda1 - z) / ((beta + sqrt(n)) / learning_rate + wd) * (abs(z)   lamda1)  If w, z and n are all of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for w, z and n)::  for row in grad.indices:      rescaled_grad[row] = clip(grad[row] * rescale_grad, clip_gradient)      z[row] += rescaled_grad[row] - (sqrt(n[row] + rescaled_grad[row] 2) - sqrt(n[row])) * weight[row] / learning_rate      n[row] += rescaled_grad[row] 2      w[row] = (sign(z[row]) * lamda1 - z[row]) / ((beta + sqrt(n[row])) / learning_rate + wd) * (abs(z[row])   lamda1)  Defined in src/operator/optimizer_op.cc:L341  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  z::NDArray-or-SymbolicNode : z  n::NDArray-or-SymbolicNode : Square of grad  lr::float, required : Learning rate  lamda1::float, optional, default=0.01 : The L1 regularization coefficient.  beta::float, optional, default=1 : Per-Coordinate Learning Rate beta.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.gammaln     Method .  gammaln(data)  Returns element-wise log of the absolute value of the gamma function \nof the input.  The storage type of $gammaln$ output is always dense  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.gather_nd     Method .  gather_nd(data, indices)  Gather elements or slices from  data  and store to a tensor whose shape is defined by  indices .  gather_nd  and  scatter_nd  are inverse functions to each other.  Given  data  with shape  (X_0, X_1, ..., X_{N-1})  and indices with shape  (M, Y_0, ..., Y_{K-1}) , the output will have shape  (Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1}) , where  M  = N . If  M == N , output shape will simply be  (Y_0, ..., Y_{K-1}) .  The elements in output is defined as follows::  output[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}] = data[indices[0, y_0, ..., y_{K-1}],                                                       ...,                                                       indices[M-1, y_0, ..., y_{K-1}],                                                       x_M, ..., x_{N-1}]  Examples::  data = [[0, 1], [2, 3]]   indices = [[1, 1, 0], [0, 1, 0]]   gather_nd(data, indices) = [2, 3, 0]  Arguments   data::NDArray-or-SymbolicNode : data  indices::NDArray-or-SymbolicNode : indices  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.get_attr     Method .  get_attr(self :: SymbolicNode, key :: Symbol)  Get attribute attached to this  SymbolicNode  belonging to key.  Returns the value belonging to key as a  Nullable .  source  #  MXNet.mx.get_children     Method .  get_children(x::SymbolicNode)  Gets a new grouped  SymbolicNode  whose output contains inputs to output nodes of the original symbol.  julia  x = mx.Variable(:x)\nMXNet.mx.SymbolicNode x\n\njulia  y = mx.Variable(:y)\nMXNet.mx.SymbolicNode y\n\njulia  z = x + y\nMXNet.mx.SymbolicNode _plus1\n\njulia  a |  mx.get_children |  mx.list_outputs\n2-element Array{Symbol,1}:\n :x\n :y  source  #  MXNet.mx.get_internals     Method .  get_internals(self :: SymbolicNode)  Get a new grouped  SymbolicNode  whose output contains all the internal outputs of this  SymbolicNode .  source  #  MXNet.mx.get_name     Method .  get_name(self :: SymbolicNode)  Get the name of the symbol.  julia  x = mx.Variable(:data)\njulia  mx.get_name(x)\n:data\n\njulia  y = mx.FullyConnected(x, num_hidden = 128)\njulia  mx.get_name(y)\n:fullyconnected0  source  #  MXNet.mx.grad     Method .  grad(self :: SymbolicNode, wrt :: Vector{SymbolicNode})  Get the autodiff gradient of the current  SymbolicNode . This function can only be used if the current symbol is a loss function.  Arguments:   self::SymbolicNode : current node.  wrt::Vector{Symbol} : the names of the arguments to the gradient.   Returns a gradient symbol of the corresponding gradient.  source  #  MXNet.mx.infer_shape     Method .  infer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)  Do shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by  list_arguments . Alternatively, the shape information could be specified via keyword arguments.  Returns a 3-tuple containing shapes of all the arguments, shapes of all the outputs and shapes of all the auxiliary variables. If shape inference failed due to incomplete or incompatible inputs, the return value will be  (nothing, nothing, nothing) .  source  #  MXNet.mx.infer_type     Method .  infer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)  Do type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by  list_arguments . Alternatively, the type information could be specified via keyword arguments.  Returns a 3-tuple containing types of all the arguments, types of all the outputs and types of all the auxiliary variables. If type inference failed due to incomplete or incompatible inputs, the return value will be  (nothing, nothing, nothing) .  source  #  MXNet.mx.khatri_rao     Method .  khatri_rao(args)  Note : khatri_rao takes variable number of positional inputs. So instead of calling as khatri_rao([x, y, z], num_args=3), one should call via khatri_rao(x, y, z), and num_args will be determined automatically.  Computes the Khatri-Rao product of the input matrices.  Given a collection of :math: n  input matrices,  .. math::    A_1 \\in \\mathbb{R}^{M_1 \\times M}, \\ldots, A_n \\in \\mathbb{R}^{M_n \\times N},  the (column-wise) Khatri-Rao product is defined as the matrix,  .. math::    X = A_1 \\otimes \\cdots \\otimes A_n \\in \\mathbb{R}^{(M_1 \\cdots M_n) \\times N},  where the :math: k th column is equal to the column-wise outer product :math: {A_1}_k \\otimes \\cdots \\otimes {A_n}_k  where :math: {A_i}_k  is the kth column of the ith matrix.  Example::     A = mx.nd.array([[1, -1],                  [2, -3]]) B = mx.nd.array([[1, 4],                  [2, 5],                  [3, 6]]) C = mx.nd.khatri_rao(A, B) print(C.asnumpy())     [[  1.  -4.]    [  2.  -5.]    [  3.  -6.]    [  2. -12.]    [  4. -15.]    [  6. -18.]]  Defined in src/operator/contrib/krprod.cc:L108  Arguments   args::NDArray-or-SymbolicNode[] : Positional input matrices  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_gelqf     Method .  linalg_gelqf(A)  linalg_gelqf is an alias of _linalg_gelqf.  LQ factorization for general matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , we compute the LQ factorization (LAPACK  gelqf , followed by  orglq ).  A  must have shape  (x, y)  with  x  = y , and must have full rank  =x . The LQ factorization consists of  L  with shape  (x, x)  and  Q  with shape  (x, y) , so that:  A  =  L  *  Q  Here,  L  is lower triangular (upper triangle equal to zero) with nonzero diagonal, and  Q  is row-orthonormal, meaning that  Q  *  Q \\ :sup: T  is equal to the identity matrix of shape  (x, x) .  If  n 2 ,  gelqf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single LQ factorization    A = [[1., 2., 3.], [4., 5., 6.]]    Q, L = gelqf(A)    Q = [[-0.26726124, -0.53452248, -0.80178373],         [0.87287156, 0.21821789, -0.43643578]]    L = [[-3.74165739, 0.],         [-8.55235974, 1.96396101]]  // Batch LQ factorization    A = [[[1., 2., 3.], [4., 5., 6.]],         [[7., 8., 9.], [10., 11., 12.]]]    Q, L = gelqf(A)    Q = [[[-0.26726124, -0.53452248, -0.80178373],          [0.87287156, 0.21821789, -0.43643578]],         [[-0.50257071, -0.57436653, -0.64616234],          [0.7620735, 0.05862104, -0.64483142]]]    L = [[[-3.74165739, 0.],          [-8.55235974, 1.96396101]],         [[-13.92838828, 0.],          [-19.09768702, 0.52758934]]]  Defined in src/operator/tensor/la_op.cc:L529  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be factorized  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_gemm     Method .  linalg_gemm(A, B, C, transpose_a, transpose_b, alpha, beta)  linalg_gemm is an alias of _linalg_gemm.  Performs general matrix multiplication and accumulation. Input are tensors  A ,  B ,  C , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B ) +  beta  *  C  Here,  alpha  and  beta  are scalar parameters, and  op()  is either the identity or matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply-add    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    C = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    gemm(A, B, C, transpose_b=True, alpha=2.0, beta=10.0)            = [[14.0, 14.0, 14.0], [14.0, 14.0, 14.0]]  // Batch matrix multiply-add    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    C = [[[10.0]], [[0.01]]]    gemm(A, B, C, transpose_b=True, alpha=2.0 , beta=10.0)            = [[[104.0]], [[0.14]]]  Defined in src/operator/tensor/la_op.cc:L69  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  C::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.  beta::double, optional, default=1 : Scalar factor multiplied with C.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_gemm2     Method .  linalg_gemm2(A, B, transpose_a, transpose_b, alpha)  linalg_gemm2 is an alias of _linalg_gemm2.  Performs general matrix multiplication. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 , the BLAS3 function  gemm  is performed:  out  =  alpha  *  op \\ ( A ) *  op \\ ( B )  Here  alpha  is a scalar parameter and  op()  is either the identity or the matrix transposition (depending on  transpose_a ,  transpose_b ).  If  n 2 ,  gemm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1.0, 1.0], [1.0, 1.0]]    B = [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]    gemm2(A, B, transpose_b=True, alpha=2.0)             = [[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]  // Batch matrix multiply    A = [[[1.0, 1.0]], [[0.1, 0.1]]]    B = [[[1.0, 1.0]], [[0.1, 0.1]]]    gemm2(A, B, transpose_b=True, alpha=2.0)            = [[[4.0]], [[0.04 ]]]  Defined in src/operator/tensor/la_op.cc:L128  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  B::NDArray-or-SymbolicNode : Tensor of input matrices  transpose_a::boolean, optional, default=0 : Multiply with transposed of first input (A).  transpose_b::boolean, optional, default=0 : Multiply with transposed of second input (B).  alpha::double, optional, default=1 : Scalar factor multiplied with A*B.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_potrf     Method .  linalg_potrf(A)  linalg_potrf is an alias of _linalg_potrf.  Performs Cholesky factorization of a symmetric positive-definite matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the Cholesky factor  L  of the symmetric, positive definite matrix  A  is computed.  L  is lower triangular (entries of upper triangle are all zero), has positive diagonal entries, and:  A  =  L  *  L \\ :sup: T  If  n 2 ,  potrf  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix factorization    A = [[4.0, 1.0], [1.0, 4.25]]    potrf(A) = [[2.0, 0], [0.5, 2.0]]  // Batch matrix factorization    A = [[[4.0, 1.0], [1.0, 4.25]], [[16.0, 4.0], [4.0, 17.0]]]    potrf(A) = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]  Defined in src/operator/tensor/la_op.cc:L178  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices to be decomposed  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_potri     Method .  linalg_potri(A)  linalg_potri is an alias of _linalg_potri.  Performs matrix inversion from a Cholesky factorization. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  is a lower triangular matrix (entries of upper triangle are all zero) with positive diagonal. We compute:  out  =  A \\ :sup: -T  *  A \\ :sup: -1  In other words, if  A  is the Cholesky factor of a symmetric positive definite matrix  B  (obtained by  potrf ), then  out  =  B \\ :sup: -1  If  n 2 ,  potri  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  .. note:: Use this operator only if you are certain you need the inverse of  B , and           cannot use the Cholesky factor  A  ( potrf ), together with backsubstitution           ( trsm ). The latter is numerically much safer, and also cheaper.  Examples::  // Single matrix inverse    A = [[2.0, 0], [0.5, 2.0]]    potri(A) = [[0.26563, -0.0625], [-0.0625, 0.25]]  // Batch matrix inverse    A = [[[2.0, 0], [0.5, 2.0]], [[4.0, 0], [1.0, 4.0]]]    potri(A) = [[[0.26563, -0.0625], [-0.0625, 0.25]],                [[0.06641, -0.01562], [-0.01562, 0,0625]]]  Defined in src/operator/tensor/la_op.cc:L236  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_sumlogdiag     Method .  linalg_sumlogdiag(A)  linalg_sumlogdiag is an alias of _linalg_sumlogdiag.  Computes the sum of the logarithms of the diagonal elements of a square matrix. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 ,  A  must be square with positive diagonal entries. We sum the natural logarithms of the diagonal elements, the result has shape (1,).  If  n 2 ,  sumlogdiag  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix reduction    A = [[1.0, 1.0], [1.0, 7.0]]    sumlogdiag(A) = [1.9459]  // Batch matrix reduction    A = [[[1.0, 1.0], [1.0, 7.0]], [[3.0, 0], [0, 17.0]]]    sumlogdiag(A) = [1.9459, 3.9318]  Defined in src/operator/tensor/la_op.cc:L405  Arguments   A::NDArray-or-SymbolicNode : Tensor of square matrices  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_syrk     Method .  linalg_syrk(A, transpose, alpha)  linalg_syrk is an alias of _linalg_syrk.  Multiplication of matrix with its transpose. Input is a tensor  A  of dimension  n  = 2 .  If  n=2 , the operator performs the BLAS3 function  syrk :  out  =  alpha  *  A  *  A \\ :sup: T  if  transpose=False , or  out  =  alpha  *  A \\ :sup: T  \\ *  A  if  transpose=True .  If  n 2 ,  syrk  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix multiply    A = [[1., 2., 3.], [4., 5., 6.]]    syrk(A, alpha=1., transpose=False)             = [[14., 32.],                [32., 77.]]    syrk(A, alpha=1., transpose=True)             = [[17., 22., 27.],                [22., 29., 36.],                [27., 36., 45.]]  // Batch matrix multiply    A = [[[1., 1.]], [[0.1, 0.1]]]    syrk(A, alpha=2., transpose=False) = [[[4.]], [[0.04]]]  Defined in src/operator/tensor/la_op.cc:L461  Arguments   A::NDArray-or-SymbolicNode : Tensor of input matrices  transpose::boolean, optional, default=0 : Use transpose of input matrix.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_trmm     Method .  linalg_trmm(A, B, transpose, rightside, alpha)  linalg_trmm is an alias of _linalg_trmm.  Performs multiplication with a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trmm :  out  =  alpha  *  op \\ ( A ) *  B  if  rightside=False , or  out  =  alpha  *  B  *  op \\ ( A )  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trmm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single triangular matrix multiply    A = [[1.0, 0], [1.0, 1.0]]    B = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]    trmm(A, B, alpha=2.0) = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]  // Batch triangular matrix multiply    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]    trmm(A, B, alpha=2.0) = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],                             [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L293  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.linalg_trsm     Method .  linalg_trsm(A, B, transpose, rightside, alpha)  linalg_trsm is an alias of _linalg_trsm.  Solves matrix equation involving a lower triangular matrix. Input are tensors  A ,  B , each of dimension  n  = 2  and having the same shape on the leading  n-2  dimensions.  If  n=2 ,  A  must be lower triangular. The operator performs the BLAS3 function  trsm , solving for  out  in:  op \\ ( A ) *  out  =  alpha  *  B  if  rightside=False , or  out  *  op \\ ( A ) =  alpha  *  B  if  rightside=True . Here,  alpha  is a scalar parameter, and  op()  is either the identity or the matrix transposition (depending on  transpose ).  If  n 2 ,  trsm  is performed separately on the trailing two dimensions for all inputs (batch mode).  .. note:: The operator supports float32 and float64 data types only.  Examples::  // Single matrix solve    A = [[1.0, 0], [1.0, 1.0]]    B = [[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]    trsm(A, B, alpha=0.5) = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]  // Batch matrix solve    A = [[[1.0, 0], [1.0, 1.0]], [[1.0, 0], [1.0, 1.0]]]    B = [[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]],         [[4.0, 4.0, 4.0], [8.0, 8.0, 8.0]]]    trsm(A, B, alpha=0.5) = [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],                             [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]  Defined in src/operator/tensor/la_op.cc:L356  Arguments   A::NDArray-or-SymbolicNode : Tensor of lower triangular matrices  B::NDArray-or-SymbolicNode : Tensor of matrices  transpose::boolean, optional, default=0 : Use transposed of the triangular matrix  rightside::boolean, optional, default=0 : Multiply triangular matrix from the right to non-triangular one.  alpha::double, optional, default=1 : Scalar factor to be applied to the result.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.list_all_attr     Method .  list_all_attr(self :: SymbolicNode)  Get all attributes from the symbol graph.  Returns a dictionary of attributes.  source  #  MXNet.mx.list_arguments     Method .  list_arguments(self :: SymbolicNode)  List all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a  FullyConnected  node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.  Returns a list of symbols indicating the names of the arguments.  source  #  MXNet.mx.list_attr     Method .  list_attr(self :: SymbolicNode)  Get all attributes from a symbol.  Returns a dictionary of attributes.  source  #  MXNet.mx.list_auxiliary_states     Method .  list_auxiliary_states(self :: SymbolicNode)  List all auxiliary states in the symbool.  Auxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.  Returns a list of symbols indicating the names of the auxiliary states.  source  #  MXNet.mx.list_outputs     Method .  list_outputs(self :: SymbolicNode)  List all the outputs of this node.  Returns a list of symbols indicating the names of the outputs.  source  #  MXNet.mx.load     Method .  load(filename :: AbstractString, ::Type{SymbolicNode})  Load a  SymbolicNode  from a JSON file.  source  #  MXNet.mx.make_loss     Method .  make_loss(data)  Make your own loss function in network construction.  This operator accepts a customized loss function symbol as a terminal loss and the symbol should be an operator with no backward dependency. The output of this function is the gradient of loss with respect to the input data.  For example, if you are a making a cross entropy loss function. Assume $out$ is the predicted output and $label$ is the true label, then the cross entropy can be defined as::  cross_entropy = label * log(out) + (1 - label) * log(1 - out)   loss = make_loss(cross_entropy)  We will need to use $make_loss$ when we are creating our own loss function or we want to combine multiple loss functions. Also we may want to stop some variables' gradients from backpropagation. See more detail in $BlockGrad$ or $stop_gradient$.  The storage type of $make_loss$ output depends upon the input storage type:   make_loss(default) = default  make_loss(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L200  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.max     Method .  max(data, axis, keepdims, exclude)  Computes the max of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L160  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx.max_axis     Method .  max_axis(data, axis, keepdims, exclude)  max_axis is an alias of max.  Computes the max of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L160  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx.min     Method .  min(data, axis, keepdims, exclude)  Computes the min of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L174  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx.min_axis     Method .  min_axis(data, axis, keepdims, exclude)  min_axis is an alias of min.  Computes the min of array elements over given axes.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L174  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx.mp_sgd_mom_update     Method .  mp_sgd_mom_update(weight, grad, mom, weight32, lr, momentum, wd, rescale_grad, clip_gradient)  Updater function for multi-precision sgd optimizer  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mom::NDArray-or-SymbolicNode : Momentum  weight32::NDArray-or-SymbolicNode : Weight32  lr::float, required : Learning rate  momentum::float, optional, default=0 : The decay rate of momentum estimates at each epoch.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.mp_sgd_update     Method .  mp_sgd_update(weight, grad, weight32, lr, wd, rescale_grad, clip_gradient)  Updater function for multi-precision sgd optimizer  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : gradient  weight32::NDArray-or-SymbolicNode : Weight32  lr::float, required : Learning rate  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.nanprod     Method .  nanprod(data, axis, keepdims, exclude)  Computes the product of array elements over given axes treating Not a Numbers ($NaN$) as one.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L146  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx.nansum     Method .  nansum(data, axis, keepdims, exclude)  Computes the sum of array elements over given axes treating Not a Numbers ($NaN$) as zero.  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L131  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx.negative     Method .  negative(data)  Numerical negative of the argument, element-wise.  The storage type of $negative$ output depends upon the input storage type:   negative(default) = default  negative(row_sparse) = row_sparse  negative(csr) = csr   Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.normal     Method .  normal(loc, scale, shape, ctx, dtype)  normal is an alias of _random_normal.  Draw random samples from a normal (Gaussian) distribution.  .. note:: The existing alias $normal$ is deprecated.  Samples are distributed according to a normal distribution parametrized by  loc  (mean) and  scale  (standard deviation).  Example::  normal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]  Defined in src/operator/random/sample_op.cc:L85  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.one_hot     Method .  one_hot(indices, depth, on_value, off_value, dtype)  Returns a one-hot array.  The locations represented by  indices  take value  on_value , while all other locations take value  off_value .  one_hot  operation with  indices  of shape $(i0, i1)$ and  depth   of $d$ would result in an output array of shape $(i0, i1, d)$ with::  output[i,j,:] = off_value   output[i,j,indices[i,j]] = on_value  Examples::  one_hot([1,0,2,0], 3) = [[ 0.  1.  0.]                            [ 1.  0.  0.]                            [ 0.  0.  1.]                            [ 1.  0.  0.]]  one_hot([1,0,2,0], 3, on_value=8, off_value=1,           dtype='int32') = [[1 8 1]                             [8 1 1]                             [1 1 8]                             [8 1 1]]  one_hot([[1,0],[1,0],[2,0]], 3) = [[[ 0.  1.  0.]                                       [ 1.  0.  0.]]                                   [[ 0.  1.  0.]\n                                  [ 1.  0.  0.]]\n\n                                 [[ 0.  0.  1.]\n                                  [ 1.  0.  0.]]]  Defined in src/operator/tensor/indexing_op.cc:L428  Arguments   indices::NDArray-or-SymbolicNode : array of locations where to set on_value  depth::int, required : Depth of the one hot dimension.  on_value::double, optional, default=1 : The value assigned to the locations represented by indices.  off_value::double, optional, default=0 : The value assigned to the locations not represented by indices.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'},optional, default='float32' : DType of the output  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.ones_like     Method .  ones_like(data)  Return an array of ones with the same shape and type as the input array.  Examples::  x = [[ 0.,  0.,  0.],        [ 0.,  0.,  0.]]  ones_like(x) = [[ 1.,  1.,  1.],                   [ 1.,  1.,  1.]]  Arguments   data::NDArray-or-SymbolicNode : The input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.pad     Method .  pad(data, mode, pad_width, constant_value)  pad is an alias of Pad.  Pads an input array with a constant or edge values of the array.  .. note::  Pad  is deprecated. Use  pad  instead.  .. note:: Current implementation only supports 4D and 5D input arrays with padding applied    only on axes 1, 2 and 3. Expects axes 4 and 5 in  pad_width  to be zero.  This operation pads an input array with either a  constant_value  or edge values along each axis of the input array. The amount of padding is specified by  pad_width .  pad_width  is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. The  pad_width  should be of length $2*N$ where $N$ is the number of dimensions of the array.  For dimension $N$ of the input array, $before_N$ and $after_N$ indicates how many values to add before and after the elements of the array along dimension $N$. The widths of the higher two dimensions $before_1$, $after_1$, $before_2$, $after_2$ must be 0.  Example::  x = [[[[  1.   2.   3.]           [  4.   5.   6.]]       [[  7.   8.   9.]\n      [ 10.  11.  12.]]]\n\n\n    [[[ 11.  12.  13.]\n      [ 14.  15.  16.]]\n\n     [[ 17.  18.  19.]\n      [ 20.  21.  22.]]]]  pad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  1.   1.   2.   3.   3.]\n        [  1.   1.   2.   3.   3.]\n        [  4.   4.   5.   6.   6.]\n        [  4.   4.   5.   6.   6.]]\n\n       [[  7.   7.   8.   9.   9.]\n        [  7.   7.   8.   9.   9.]\n        [ 10.  10.  11.  12.  12.]\n        [ 10.  10.  11.  12.  12.]]]\n\n\n      [[[ 11.  11.  12.  13.  13.]\n        [ 11.  11.  12.  13.  13.]\n        [ 14.  14.  15.  16.  16.]\n        [ 14.  14.  15.  16.  16.]]\n\n       [[ 17.  17.  18.  19.  19.]\n        [ 17.  17.  18.  19.  19.]\n        [ 20.  20.  21.  22.  22.]\n        [ 20.  20.  21.  22.  22.]]]]  pad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =       [[[[  0.   0.   0.   0.   0.]\n        [  0.   1.   2.   3.   0.]\n        [  0.   4.   5.   6.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.   7.   8.   9.   0.]\n        [  0.  10.  11.  12.   0.]\n        [  0.   0.   0.   0.   0.]]]\n\n\n      [[[  0.   0.   0.   0.   0.]\n        [  0.  11.  12.  13.   0.]\n        [  0.  14.  15.  16.   0.]\n        [  0.   0.   0.   0.   0.]]\n\n       [[  0.   0.   0.   0.   0.]\n        [  0.  17.  18.  19.   0.]\n        [  0.  20.  21.  22.   0.]\n        [  0.   0.   0.   0.   0.]]]]  Defined in src/operator/pad.cc:L766  Arguments   data::NDArray-or-SymbolicNode : An n-dimensional input array.  mode::{'constant', 'edge', 'reflect'}, required : Padding type to use. \"constant\" pads with  constant_value  \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.  pad_width::Shape(tuple), required : Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format $(before_1, after_1, ... , before_N, after_N)$. It should be of length $2*N$ where $N$ is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.  constant_value::double, optional, default=0 : The value used for padding when  mode  is \"constant\".  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.pick     Method .  pick(data, index, axis, keepdims)  Picks elements from an input array according to the input indices along the given axis.  Given an input array of shape $(d0, d1)$ and indices of shape $(i0,)$, the result will be an output array of shape $(i0,)$ with::  output[i] = input[i, indices[i]]  By default, if any index mentioned is too large, it is replaced by the index that addresses the last element along an axis (the  clip  mode).  This function supports n-dimensional input and (n-1)-dimensional indices arrays.  Examples::  x = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]  // picks elements with specified indices along axis 0   pick(x, y=[0,1], 0) = [ 1.,  4.]  // picks elements with specified indices along axis 1   pick(x, y=[0,1,0], 1) = [ 1.,  4.,  5.]  y = [[ 1.],        [ 0.],        [ 2.]]  // picks elements with specified indices along axis 1 and dims are maintained   pick(x,y, 1, keepdims=True) = [[ 2.],                                  [ 3.],                                  [ 6.]]  Defined in src/operator/tensor/broadcast_reduce_op_index.cc:L145  Arguments   data::NDArray-or-SymbolicNode : The input array  index::NDArray-or-SymbolicNode : The index array  axis::int or None, optional, default='None' : The axis along which to perform the reduction. Negative values means indexing from right to left. $Requires axis to be set as int, because global reduction is not supported yet.$  keepdims::boolean, optional, default=0 : If this is set to  True , the reduced axis is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.radians     Method .  radians(data)  Converts each element of the input array from degrees to radians.  .. math::    radians([0, 90, 180, 270, 360]) = [0, \\pi/2, \\pi, 3\\pi/2, 2\\pi]  The storage type of $radians$ output depends upon the input storage type:   radians(default) = default  radians(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_trig.cc:L182  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.random_exponential     Method .  random_exponential(lam, shape, ctx, dtype)  random_exponential is an alias of _random_exponential.  Draw random samples from an exponential distribution.  Samples are distributed according to an exponential distribution parametrized by  lambda  (rate).  Example::  exponential(lam=4, shape=(2,2)) = [[ 0.0097189 ,  0.08999364],                                       [ 0.04146638,  0.31715935]]  Defined in src/operator/random/sample_op.cc:L115  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the exponential distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.random_gamma     Method .  random_gamma(alpha, beta, shape, ctx, dtype)  random_gamma is an alias of _random_gamma.  Draw random samples from a gamma distribution.  Samples are distributed according to a gamma distribution parametrized by  alpha  (shape) and  beta  (scale).  Example::  gamma(alpha=9, beta=0.5, shape=(2,2)) = [[ 7.10486984,  3.37695289],                                             [ 3.91697288,  3.65933681]]  Defined in src/operator/random/sample_op.cc:L100  Arguments   alpha::float, optional, default=1 : Alpha parameter (shape) of the gamma distribution.  beta::float, optional, default=1 : Beta parameter (scale) of the gamma distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.random_generalized_negative_binomial     Method .  random_generalized_negative_binomial(mu, alpha, shape, ctx, dtype)  random_generalized_negative_binomial is an alias of _random_generalized_negative_binomial.  Draw random samples from a generalized negative binomial distribution.  Samples are distributed according to a generalized negative binomial distribution parametrized by  mu  (mean) and  alpha  (dispersion).  alpha  is defined as  1/k  where  k  is the failure limit of the number of unsuccessful experiments (generalized to real numbers). Samples will always be returned as a floating point data type.  Example::  generalized_negative_binomial(mu=2.0, alpha=0.3, shape=(2,2)) = [[ 2.,  1.],                                                                     [ 6.,  4.]]  Defined in src/operator/random/sample_op.cc:L168  Arguments   mu::float, optional, default=1 : Mean of the negative binomial distribution.  alpha::float, optional, default=1 : Alpha (dispersion) parameter of the negative binomial distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.random_negative_binomial     Method .  random_negative_binomial(k, p, shape, ctx, dtype)  random_negative_binomial is an alias of _random_negative_binomial.  Draw random samples from a negative binomial distribution.  Samples are distributed according to a negative binomial distribution parametrized by  k  (limit of unsuccessful experiments) and  p  (failure probability in each experiment). Samples will always be returned as a floating point data type.  Example::  negative_binomial(k=3, p=0.4, shape=(2,2)) = [[ 4.,  7.],                                                  [ 2.,  5.]]  Defined in src/operator/random/sample_op.cc:L149  Arguments   k::int, optional, default='1' : Limit of unsuccessful experiments.  p::float, optional, default=1 : Failure probability in each experiment.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.random_normal     Method .  random_normal(loc, scale, shape, ctx, dtype)  random_normal is an alias of _random_normal.  Draw random samples from a normal (Gaussian) distribution.  .. note:: The existing alias $normal$ is deprecated.  Samples are distributed according to a normal distribution parametrized by  loc  (mean) and  scale  (standard deviation).  Example::  normal(loc=0, scale=1, shape=(2,2)) = [[ 1.89171135, -1.16881478],                                           [-1.23474145,  1.55807114]]  Defined in src/operator/random/sample_op.cc:L85  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.random_poisson     Method .  random_poisson(lam, shape, ctx, dtype)  random_poisson is an alias of _random_poisson.  Draw random samples from a Poisson distribution.  Samples are distributed according to a Poisson distribution parametrized by  lambda  (rate). Samples will always be returned as a floating point data type.  Example::  poisson(lam=4, shape=(2,2)) = [[ 5.,  2.],                                   [ 4.,  6.]]  Defined in src/operator/random/sample_op.cc:L132  Arguments   lam::float, optional, default=1 : Lambda parameter (rate) of the Poisson distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.random_uniform     Method .  random_uniform(low, high, shape, ctx, dtype)  random_uniform is an alias of _random_uniform.  Draw random samples from a uniform distribution.  .. note:: The existing alias $uniform$ is deprecated.  Samples are uniformly distributed over the half-open interval  [low, high)  (includes  low , but excludes  high ).  Example::  uniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]  Defined in src/operator/random/sample_op.cc:L66  Arguments   low::float, optional, default=0 : Lower bound of the distribution.  high::float, optional, default=1 : Upper bound of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.rcbrt     Method .  rcbrt(data)  Returns element-wise inverse cube-root value of the input.  .. math::    rcbrt(x) = 1/\\sqrt[3]{x}  Example::  rcbrt([1,8,-125]) = [1.0, 0.5, -0.2]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L619  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.reciprocal     Method .  reciprocal(data)  Returns the reciprocal of the argument, element-wise.  Calculates 1/x.  Example::  reciprocal([-2, 1, 3, 1.6, 0.2]) = [-0.5, 1.0, 0.33333334, 0.625, 5.0]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L364  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.reshape_like     Method .  reshape_like(lhs, rhs)  Reshape lhs to have the same shape as rhs.  Arguments   lhs::NDArray-or-SymbolicNode : First input.  rhs::NDArray-or-SymbolicNode : Second input.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.rint     Method .  rint(data)  Returns element-wise rounded value to the nearest integer of the input.  .. note::   For input $n.5$ $rint$ returns $n$ while $round$ returns $n+1$.  For input $-n.5$ both $rint$ and $round$ returns $-n-1$.   Example::  rint([-1.5, 1.5, -1.9, 1.9, 2.1]) = [-2.,  1., -2.,  2.,  2.]  The storage type of $rint$ output depends upon the input storage type:   rint(default) = default  rint(row_sparse) = row_sparse   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L445  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.rmsprop_update     Method .  rmsprop_update(weight, grad, n, lr, gamma1, epsilon, wd, rescale_grad, clip_gradient, clip_weights)  Update function for  RMSProp  optimizer.  RMSprop  is a variant of stochastic gradient descent where the gradients are divided by a cache which grows with the sum of squares of recent gradients?  RMSProp  is similar to  AdaGrad , a popular variant of  SGD  which adaptively tunes the learning rate of each parameter.  AdaGrad  lowers the learning rate for each parameter monotonically over the course of training. While this is analytically motivated for convex optimizations, it may not be ideal for non-convex problems.  RMSProp  deals with this heuristically by allowing the learning rates to rebound as the denominator decays over time.  Define the Root Mean Square (RMS) error criterion of the gradient as :math: RMS[g]_t = \\sqrt{E[g^2]_t + \\epsilon} , where :math: g  represents gradient and :math: E[g^2]_t  is the decaying average over past squared gradient.  The :math: E[g^2]_t  is given by:  .. math::   E[g^2] t = \\gamma * E[g^2]  + (1-\\gamma) * g_t^2  The update step is  .. math::   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{RMS[g]_t} g_t  The RMSProp code follows the version in http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf Tieleman   Hinton, 2012.  Hinton suggests the momentum term :math: \\gamma  to be 0.9 and the learning rate :math: \\eta  to be 0.001.  Defined in src/operator/optimizer_op.cc:L262  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  n::NDArray-or-SymbolicNode : n  lr::float, required : Learning rate  gamma1::float, optional, default=0.95 : The decay rate of momentum estimates.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  clip_weights::float, optional, default=-1 : Clip weights to the range of [-clip_weights, clip_weights] If clip_weights  = 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.rmspropalex_update     Method .  rmspropalex_update(weight, grad, n, g, delta, lr, gamma1, gamma2, epsilon, wd, rescale_grad, clip_gradient, clip_weights)  Update function for RMSPropAlex optimizer.  RMSPropAlex  is non-centered version of  RMSProp .  Define :math: E[g^2]_t  is the decaying average over past squared gradient and :math: E[g]_t  is the decaying average over past gradient.  .. math::   E[g^2] t = \\gamma_1 * E[g^2]  + (1 - \\gamma_1) * g_t^2\\\n  E[g] t = \\gamma_1 * E[g]  + (1 - \\gamma_1) * g_t\\\n  \\Delta_t = \\gamma_2 * \\Delta_{t-1} - \\frac{\\eta}{\\sqrt{E[g^2]_t - E[g]_t^2 + \\epsilon}} g_t\\\n The update step is  .. math::   \\theta_{t+1} = \\theta_t + \\Delta_t  The RMSPropAlex code follows the version in http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.  Graves suggests the momentum term :math: \\gamma_1  to be 0.95, :math: \\gamma_2  to be 0.9 and the learning rate :math: \\eta  to be 0.0001.  Defined in src/operator/optimizer_op.cc:L301  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  n::NDArray-or-SymbolicNode : n  g::NDArray-or-SymbolicNode : g  delta::NDArray-or-SymbolicNode : delta  lr::float, required : Learning rate  gamma1::float, optional, default=0.95 : Decay rate.  gamma2::float, optional, default=0.9 : Decay rate.  epsilon::float, optional, default=1e-08 : A small constant for numerical stability.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  clip_weights::float, optional, default=-1 : Clip weights to the range of [-clip_weights, clip_weights] If clip_weights  = 0, weight clipping is turned off. weights = max(min(weights, clip_weights), -clip_weights).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.rsqrt     Method .  rsqrt(data)  Returns element-wise inverse square-root value of the input.  .. math::    rsqrt(x) = 1/\\sqrt{x}  Example::  rsqrt([4,9,16]) = [0.5, 0.33333334, 0.25]  The storage type of $rsqrt$ output is always dense  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L585  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_exponential     Method .  sample_exponential(lam, shape, dtype)  sample_exponential is an alias of _sample_exponential.  Concurrent sampling from multiple exponential distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_exponential(lam) = [ 0.51837951,  0.09994757]  // Draw a vector containing two samples for each distribution    sample_exponential(lam, shape=(2)) = [[ 0.51837951,  0.19866663],                                          [ 0.09994757,  0.50447971]]  Defined in src/operator/random/multisample_op.cc:L284  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_gamma     Method .  sample_gamma(alpha, shape, dtype, beta)  sample_gamma is an alias of _sample_gamma.  Concurrent sampling from multiple gamma distributions with parameters  alpha  (shape) and  beta  (scale).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  alpha = [ 0.0, 2.5 ]    beta = [ 1.0, 0.7 ]  // Draw a single sample for each distribution    sample_gamma(alpha, beta) = [ 0.        ,  2.25797319]  // Draw a vector containing two samples for each distribution    sample_gamma(alpha, beta, shape=(2)) = [[ 0.        ,  0.        ],                                            [ 2.25797319,  1.70734084]]  Defined in src/operator/random/multisample_op.cc:L282  Arguments   alpha::NDArray-or-SymbolicNode : Alpha (shape) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  beta::NDArray-or-SymbolicNode : Beta (scale) parameters of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_generalized_negative_binomial     Method .  sample_generalized_negative_binomial(mu, shape, dtype, alpha)  sample_generalized_negative_binomial is an alias of _sample_generalized_negative_binomial.  Concurrent sampling from multiple generalized negative binomial distributions with parameters  mu  (mean) and  alpha  (dispersion).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  mu = [ 2.0, 2.5 ]    alpha = [ 1.0, 0.1 ]  // Draw a single sample for each distribution    sample_generalized_negative_binomial(mu, alpha) = [ 0.,  3.]  // Draw a vector containing two samples for each distribution    sample_generalized_negative_binomial(mu, alpha, shape=(2)) = [[ 0.,  3.],                                                                  [ 3.,  1.]]  Defined in src/operator/random/multisample_op.cc:L293  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  alpha::NDArray-or-SymbolicNode : Alpha (dispersion) parameters of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_multinomial     Method .  sample_multinomial(data, shape, get_prob, dtype)  sample_multinomial is an alias of _sample_multinomial.  Concurrent sampling from multiple multinomial distributions.  data  is an  n  dimensional array whose last dimension has length  k , where  k  is the number of possible outcomes of each multinomial distribution. This operator will draw  shape  samples from each distribution. If shape is empty one sample will be drawn from each distribution.  If  get_prob  is true, a second array containing log likelihood of the drawn samples will also be returned. This is usually used for reinforcement learning where you can provide reward as head gradient for this array to estimate gradient.  Note that the input distribution must be normalized, i.e.  data  must sum to 1 along its last axis.  Examples::  probs = [[0, 0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1, 0]]  // Draw a single sample for each distribution    sample_multinomial(probs) = [3, 0]  // Draw a vector containing two samples for each distribution    sample_multinomial(probs, shape=(2)) = [[4, 2],                                            [0, 0]]  // requests log likelihood    sample_multinomial(probs, get_prob=True) = [2, 1], [0.2, 0.3]  Arguments   data::NDArray-or-SymbolicNode : Distribution probabilities. Must sum to one on the last axis.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  get_prob::boolean, optional, default=0 : Whether to also return the log probability of sampled result. This is usually used for differentiating through stochastic variables, e.g. in reinforcement learning.  dtype::{'int32'},optional, default='int32' : DType of the output in case this can't be inferred. Only support int32 for now.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_negative_binomial     Method .  sample_negative_binomial(k, shape, dtype, p)  sample_negative_binomial is an alias of _sample_negative_binomial.  Concurrent sampling from multiple negative binomial distributions with parameters  k  (failure limit) and  p  (failure probability).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Samples will always be returned as a floating point data type.  Examples::  k = [ 20, 49 ]    p = [ 0.4 , 0.77 ]  // Draw a single sample for each distribution    sample_negative_binomial(k, p) = [ 15.,  16.]  // Draw a vector containing two samples for each distribution    sample_negative_binomial(k, p, shape=(2)) = [[ 15.,  50.],                                                 [ 16.,  12.]]  Defined in src/operator/random/multisample_op.cc:L289  Arguments   k::NDArray-or-SymbolicNode : Limits of unsuccessful experiments.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  p::NDArray-or-SymbolicNode : Failure probabilities in each experiment.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_normal     Method .  sample_normal(mu, shape, dtype, sigma)  sample_normal is an alias of _sample_normal.  Concurrent sampling from multiple normal distributions with parameters  mu  (mean) and  sigma  (standard deviation).  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  mu = [ 0.0, 2.5 ]    sigma = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_normal(mu, sigma) = [-0.56410581,  0.95934606]  // Draw a vector containing two samples for each distribution    sample_normal(mu, sigma, shape=(2)) = [[-0.56410581,  0.2928229 ],                                           [ 0.95934606,  4.48287058]]  Defined in src/operator/random/multisample_op.cc:L279  Arguments   mu::NDArray-or-SymbolicNode : Means of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  sigma::NDArray-or-SymbolicNode : Standard deviations of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_poisson     Method .  sample_poisson(lam, shape, dtype)  sample_poisson is an alias of _sample_poisson.  Concurrent sampling from multiple Poisson distributions with parameters lambda (rate).  The parameters of the distributions are provided as an input array. Let  [s]  be the shape of the input array,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input array,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input value at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input array.  Samples will always be returned as a floating point data type.  Examples::  lam = [ 1.0, 8.5 ]  // Draw a single sample for each distribution    sample_poisson(lam) = [  0.,  13.]  // Draw a vector containing two samples for each distribution    sample_poisson(lam, shape=(2)) = [[  0.,   4.],                                      [ 13.,   8.]]  Defined in src/operator/random/multisample_op.cc:L286  Arguments   lam::NDArray-or-SymbolicNode : Lambda (rate) parameters of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sample_uniform     Method .  sample_uniform(low, shape, dtype, high)  sample_uniform is an alias of _sample_uniform.  Concurrent sampling from multiple uniform distributions on the intervals given by  [low,high) .  The parameters of the distributions are provided as input arrays. Let  [s]  be the shape of the input arrays,  n  be the dimension of  [s] ,  [t]  be the shape specified as the parameter of the operator, and  m  be the dimension of  [t] . Then the output will be a  (n+m) -dimensional array with shape  [s]x[t] .  For any valid  n -dimensional index  i  with respect to the input arrays,  output[i]  will be an  m -dimensional array that holds randomly drawn samples from the distribution which is parameterized by the input values at index  i . If the shape parameter of the operator is not set, then one sample will be drawn per distribution and the output array has the same shape as the input arrays.  Examples::  low = [ 0.0, 2.5 ]    high = [ 1.0, 3.7 ]  // Draw a single sample for each distribution    sample_uniform(low, high) = [ 0.40451524,  3.18687344]  // Draw a vector containing two samples for each distribution    sample_uniform(low, high, shape=(2)) = [[ 0.40451524,  0.18017688],                                            [ 3.18687344,  3.68352246]]  Defined in src/operator/random/multisample_op.cc:L277  Arguments   low::NDArray-or-SymbolicNode : Lower bounds of the distributions.  shape::Shape(tuple), optional, default=[] : Shape to be sampled from each random distribution.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  high::NDArray-or-SymbolicNode : Upper bounds of the distributions.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.save     Method .  save(filename :: AbstractString, node :: SymbolicNode)  Save a  SymbolicNode  to a JSON file.  source  #  MXNet.mx.scatter_nd     Method .  scatter_nd(data, indices, shape)  Scatters data into a new tensor according to indices.  gather_nd  and  scatter_nd  are inverse functions to each other.  Given  data  with shape  (Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1})  and indices with shape  (M, Y_0, ..., Y_{K-1}) , the output will have shape  (X_0, X_1, ..., X_{N-1}) , where  M  = N . If  M == N , data shape should simply be  (Y_0, ..., Y_{K-1}) .  The elements in output is defined as follows::  output[indices[0, y_0, ..., y_{K-1}],          ...,          indices[M-1, y_0, ..., y_{K-1}],          x_M, ..., x_{N-1}] = data[y_0, ..., y_{K-1}, x_M, ..., x_{N-1}]  all other entries in output are 0.  Examples::  data = [2, 3, 0]   indices = [[1, 1, 0], [0, 1, 0]]   shape = (2, 2)   scatter_nd(data, indices, shape) = [[0, 0], [2, 3]]  Arguments   data::NDArray-or-SymbolicNode : data  indices::NDArray-or-SymbolicNode : indices  shape::Shape(tuple), required : Shape of output.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.set_attr     Method .  set_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)  Set the attribute key to value for this  SymbolicNode .   Note  It is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the  SymbolicNode . Changing the attributes of a  SymbolicNode  that is already been used somewhere else might cause unexpected behavior and inconsistency.   source  #  MXNet.mx.sgd_mom_update     Method .  sgd_mom_update(weight, grad, mom, lr, momentum, wd, rescale_grad, clip_gradient)  Momentum update function for Stochastic Gradient Descent (SDG) optimizer.  Momentum update has better convergence rates on neural networks. Mathematically it looks like below:  .. math::  v_1 = \\alpha * \\nabla J(W_0)\\\n  v_t = \\gamma v_{t-1} - \\alpha * \\nabla J(W_{t-1})\\\n  W_t = W_{t-1} + v_t  It updates the weights using::  v = momentum * v - learning_rate * gradient   weight += v  Where the parameter $momentum$ is the decay rate of momentum estimates at each epoch.  If weight and momentum are both of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated (for both weight and momentum)::  for row in gradient.indices:       v[row] = momentum[row] * v[row] - learning_rate * gradient[row]       weight[row] += v[row]  Defined in src/operator/optimizer_op.cc:L94  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  mom::NDArray-or-SymbolicNode : Momentum  lr::float, required : Learning rate  momentum::float, optional, default=0 : The decay rate of momentum estimates at each epoch.  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sgd_update     Method .  sgd_update(weight, grad, lr, wd, rescale_grad, clip_gradient)  Update function for Stochastic Gradient Descent (SDG) optimizer.  It updates the weights using::  weight = weight - learning_rate * gradient  If weight is of $row_sparse$ storage type, only the row slices whose indices appear in grad.indices are updated::  for row in gradient.indices:      weight[row] = weight[row] - learning_rate * gradient[row]  Defined in src/operator/optimizer_op.cc:L54  Arguments   weight::NDArray-or-SymbolicNode : Weight  grad::NDArray-or-SymbolicNode : Gradient  lr::float, required : Learning rate  wd::float, optional, default=0 : Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.  rescale_grad::float, optional, default=1 : Rescale gradient to grad = rescale_grad*grad.  clip_gradient::float, optional, default=-1 : Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient  = 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.slice     Method .  slice(data, begin, end, step)  Slices a region of the array.  .. note:: $crop$ is deprecated. Use $slice$ instead.  This function returns a sliced array between the indices given by  begin  and  end  with the corresponding  step .  For an input array of $shape=(d_0, d_1, ..., d_n-1)$, slice operation with $begin=(b_0, b_1...b_m-1)$, $end=(e_0, e_1, ..., e_m-1)$, and $step=(s_0, s_1, ..., s_m-1)$, where m  = n, results in an array with the shape $(|e_0-b_0|/|s_0|, ..., |e_m-1-b_m-1|/|s_m-1|, d_m, ..., d_n-1)$.  The resulting array's  k -th dimension contains elements from the  k -th dimension of the input array starting from index $b_k$ (inclusive) with step $s_k$ until reaching $e_k$ (exclusive).  If the  k -th elements are  None  in the sequence of  begin ,  end , and  step , the following rule will be used to set default values. If  s_k  is  None , set  s_k=1 . If  s_k   0 , set  b_k=0 ,  e_k=d_k ; else, set  b_k=d_k-1 ,  e_k=-1 .  The storage type of $slice$ output depends on storage types of inputs   slice(csr) = csr  otherwise, $slice$ generates output with default storage   .. note:: When input data storage type is csr, it only supports step=(), or step=(None,), or step=(1,) to generate a csr output. For other step parameter values, it falls back to slicing a dense tensor.  Example::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice(x, begin=(0,1), end=(2,4)) = [[ 2.,  3.,  4.],                                      [ 6.,  7.,  8.]]   slice(x, begin=(None, 0), end=(None, 3), step=(-1, 2)) = [[9., 11.],                                                             [5.,  7.],                                                             [1.,  3.]]  Defined in src/operator/tensor/matrix_op.cc:L355  Arguments   data::NDArray-or-SymbolicNode : Source input  begin::Shape(tuple), required : starting indices for the slice operation, supports negative indices.  end::Shape(tuple), required : ending indices for the slice operation, supports negative indices.  step::Shape(tuple), optional, default=[] : step for the slice operation, supports negative values.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.slice_axis     Method .  slice_axis(data, axis, begin, end)  Slices along a given axis.  Returns an array slice along a given  axis  starting from the  begin  index to the  end  index.  Examples::  x = [[  1.,   2.,   3.,   4.],        [  5.,   6.,   7.,   8.],        [  9.,  10.,  11.,  12.]]  slice_axis(x, axis=0, begin=1, end=3) = [[  5.,   6.,   7.,   8.],                                            [  9.,  10.,  11.,  12.]]  slice_axis(x, axis=1, begin=0, end=2) = [[  1.,   2.],                                            [  5.,   6.],                                            [  9.,  10.]]  slice_axis(x, axis=1, begin=-3, end=-1) = [[  2.,   3.],                                              [  6.,   7.],                                              [ 10.,  11.]]  Defined in src/operator/tensor/matrix_op.cc:L442  Arguments   data::NDArray-or-SymbolicNode : Source input  axis::int, required : Axis along which to be sliced, supports negative indexes.  begin::int, required : The beginning index along the axis to be sliced,  supports negative indexes.  end::int or None, required : The ending index along the axis to be sliced,  supports negative indexes.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.smooth_l1     Method .  smooth_l1(data, scalar)  Calculate Smooth L1 Loss(lhs, scalar) by summing  .. math::  f(x) =\n\\begin{cases}\n(\\sigma x)^2/2,  \\text{if }x   1/\\sigma^2\\\\\n|x|-0.5/\\sigma^2,  \\text{otherwise}\n\\end{cases}  where :math: x  is an element of the tensor  lhs  and :math: \\sigma  is the scalar.  Example::  smooth_l1([1, 2, 3, 4], sigma=1) = [0.5, 1.5, 2.5, 3.5]  Defined in src/operator/tensor/elemwise_binary_scalar_op_extended.cc:L103  Arguments   data::NDArray-or-SymbolicNode : source input  scalar::float : scalar input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.softmax_cross_entropy     Method .  softmax_cross_entropy(data, label)  Calculate cross entropy of softmax output and one-hot label.    This operator computes the cross entropy in two steps:   Applies softmax function on the input array.  Computes and returns the cross entropy loss between the softmax output and the labels.   The softmax function and cross entropy loss is given by:    Softmax Function:    .. math:: \\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}   Cross Entropy Function:   .. math:: \\text{CE(label, output)} = - \\sum_i \\text{label}_i \\log(\\text{output}_i)    Example::  x = [[1, 2, 3],        [11, 7, 5]]  label = [2, 0]  softmax(x) = [[0.09003057, 0.24472848, 0.66524094],                 [0.97962922, 0.01794253, 0.00242826]]  softmax_cross_entropy(data, label) = - log(0.66524084) - log(0.97962922) = 0.4281871  Defined in src/operator/loss_binary_op.cc:L59  Arguments   data::NDArray-or-SymbolicNode : Input data  label::NDArray-or-SymbolicNode : Input label  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.square     Method .  square(data)  Returns element-wise squared value of the input.  .. math::    square(x) = x^2  Example::  square([2, 3, 4]) = [4, 9, 16]  The storage type of $square$ output depends upon the input storage type:   square(default) = default  square(row_sparse) = row_sparse  square(csr) = csr   Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L542  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.stack     Method .  stack(data, axis, num_args)  Note : stack takes variable number of positional inputs. So instead of calling as stack([x, y, z], num_args=3), one should call via stack(x, y, z), and num_args will be determined automatically.  Join a sequence of arrays along a new axis.  The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.  Examples::  x = [1, 2]   y = [3, 4]  stack(x, y) = [[1, 2],                  [3, 4]]   stack(x, y, axis=1) = [[1, 3],                          [2, 4]]  Arguments   data::NDArray-or-SymbolicNode[] : List of arrays to stack  axis::int, optional, default='0' : The axis in the result array along which the input arrays are stacked.  num_args::int, required : Number of inputs to be stacked.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.stop_gradient     Method .  stop_gradient(data)  stop_gradient is an alias of BlockGrad.  Stops gradient computation.  Stops the accumulated gradient of the inputs from flowing through this operator in the backward direction. In other words, this operator prevents the contribution of its inputs to be taken into account for computing gradients.  Example::  v1 = [1, 2]   v2 = [0, 1]   a = Variable('a')   b = Variable('b')   b_stop_grad = stop_gradient(3 * b)   loss = MakeLoss(b_stop_grad + a)  executor = loss.simple_bind(ctx=cpu(), a=(1,2), b=(1,2))   executor.forward(is_train=True, a=v1, b=v2)   executor.outputs   [ 1.  5.]  executor.backward()   executor.grad_arrays   [ 0.  0.]   [ 1.  1.]  Defined in src/operator/tensor/elemwise_unary_op_basic.cc:L167  Arguments   data::NDArray-or-SymbolicNode : The input array.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.sum_axis     Method .  sum_axis(data, axis, keepdims, exclude)  sum_axis is an alias of sum.  Computes the sum of array elements over given axes.  .. Note::  sum  and  sum_axis  are equivalent.   For ndarray of csr storage type summation along axis 0 and axis 1 is supported.   Setting keepdims or exclude to True will cause a fallback to dense operator.  Example::  data = [[[1,2],[2,3],[1,3]],           [[1,4],[4,3],[5,2]],           [[7,1],[7,2],[7,3]]]  sum(data, axis=1)   [[  4.   8.]    [ 10.   9.]    [ 21.   6.]]  sum(data, axis=[1,2])   [ 12.  19.  27.]  data = [[1,2,0],           [3,0,1],           [4,1,0]]  csr = cast_storage(data, 'csr')  sum(csr, axis=0)   [ 8.  3.  1.]  sum(csr, axis=1)   [ 3.  4.  5.]  Defined in src/operator/tensor/broadcast_reduce_op_value.cc:L85  Arguments   data::NDArray-or-SymbolicNode : The input   axis::Shape(tuple), optional, default=[] : The axis or axes along which to perform the reduction.  ``\nThe default, axis=() , will compute over all elements into a\nscalar array with shape (1,)`.  If  axis  is int, a reduction is performed on a particular axis.  If  axis  is a tuple of ints, a reduction is performed on all the axes\nspecified in the tuple.  If  exclude  is true, reduction will be performed on the axes that are\nNOT in axis instead.  Negative values means indexing from right to left. ``\n  * keepdims::boolean, optional, default=0 : If this is set to True , the reduced axes are left in the result as dimension with size one.\n  * exclude::boolean, optional, default=0 : Whether to perform reduction on axis that are NOT in axis instead.\n  * name::Symbol : The name of the SymbolicNode . (e.g. :my_symbol ), optional.\n  * attrs::Dict{Symbol, AbstractString} : The attributes associated with this SymbolicNode`.    source  #  MXNet.mx.swapaxes     Method .  swapaxes(data, dim1, dim2)  swapaxes is an alias of SwapAxis.  Interchanges two axes of an array.  Examples::  x = [[1, 2, 3]])   swapaxes(x, 0, 1) = [[ 1],                        [ 2],                        [ 3]]  x = [[[ 0, 1],         [ 2, 3]],        [[ 4, 5],         [ 6, 7]]]  // (2,2,2) array  swapaxes(x, 0, 2) = [[[ 0, 4],                        [ 2, 6]],                       [[ 1, 5],                        [ 3, 7]]]  Defined in src/operator/swapaxis.cc:L70  Arguments   data::NDArray-or-SymbolicNode : Input array.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.take     Method .  take(a, indices, axis, mode)  Takes elements from an input array along the given axis.  This function slices the input array along a particular axis with the provided indices.  Given an input array with shape $(d0, d1, d2)$ and indices with shape $(i0, i1)$, the output will have shape $(i0, i1, d1, d2)$, computed by::  output[i,j,:,:] = input[indices[i,j],:,:]  .. note::   axis - Only slicing along axis 0 is supported for now.  mode - Only  clip  mode is supported for now.   Examples::  x = [[ 1.,  2.],        [ 3.,  4.],        [ 5.,  6.]]  // takes elements with specified indices along axis 0   take(x, [[0,1],[1,2]]) = [[[ 1.,  2.],                              [ 3.,  4.]],                          [[ 3.,  4.],\n                         [ 5.,  6.]]]  Defined in src/operator/tensor/indexing_op.cc:L327  Arguments   a::NDArray-or-SymbolicNode : The input array.  indices::NDArray-or-SymbolicNode : The indices of the values to be extracted.  axis::int, optional, default='0' : The axis of input array to be taken.  mode::{'clip', 'raise', 'wrap'},optional, default='clip' : Specify how out-of-bound indices bahave. \"clip\" means clip to the range. So, if all indices mentioned are too large, they are replaced by the index that addresses the last element along an axis.  \"wrap\" means to wrap around.  \"raise\" means to raise an error.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.tile     Method .  tile(data, reps)  Repeats the whole array multiple times.  If $reps$ has length  d , and input array has dimension of  n . There are three cases:    n=d . Repeat  i -th dimension of the input by $reps[i]$ times::  x = [[1, 2],        [3, 4]]  tile(x, reps=(2,3)) = [[ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.],                          [ 1.,  2.,  1.,  2.,  1.,  2.],                          [ 3.,  4.,  3.,  4.,  3.,  4.]]\n  *  n d . $reps$ is promoted to length  n  by pre-pending 1's to it. Thus for an input shape $(2,3)$, $repos=(2,)$ is treated as $(1,2)$::    tile(x, reps=(2,)) = [[ 1.,  2.,  1.,  2.],\n                      [ 3.,  4.,  3.,  4.]]    n d . The input is promoted to be d-dimensional by prepending new axes. So a shape $(2,2)$ array is promoted to $(1,2,2)$ for 3-D replication::  tile(x, reps=(2,2,3)) = [[[ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.],                             [ 1.,  2.,  1.,  2.,  1.,  2.],                             [ 3.,  4.,  3.,  4.,  3.,  4.]],  [[ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.],\n                        [ 1.,  2.,  1.,  2.,  1.,  2.],\n                        [ 3.,  4.,  3.,  4.,  3.,  4.]]]    Defined in src/operator/tensor/matrix_op.cc:L621  Arguments   data::NDArray-or-SymbolicNode : Input data array  reps::Shape(tuple), required : The number of times for repeating the tensor a. If reps has length d, the result will have dimension of max(d, a.ndim); If a.ndim   d, a is promoted to be d-dimensional by prepending new axes. If a.ndim   d, reps is promoted to a.ndim by pre-pending 1's to it.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.to_json     Method .  to_json(self :: SymbolicNode)  Convert a  SymbolicNode  into a JSON string.  source  #  MXNet.mx.topk     Method .  topk(data, axis, k, ret_typ, is_ascend)  Returns the top  k  elements in an input array along the given axis.  Examples::  x = [[ 0.3,  0.2,  0.4],        [ 0.1,  0.3,  0.2]]  // returns an index of the largest element on last axis   topk(x) = [[ 2.],              [ 1.]]  // returns the value of top-2 largest elements on last axis   topk(x, ret_typ='value', k=2) = [[ 0.4,  0.3],                                    [ 0.3,  0.2]]  // returns the value of top-2 smallest elements on last axis   topk(x, ret_typ='value', k=2, is_ascend=1) = [[ 0.2 ,  0.3],                                                [ 0.1 ,  0.2]]  // returns the value of top-2 largest elements on axis 0   topk(x, axis=0, ret_typ='value', k=2) = [[ 0.3,  0.3,  0.4],                                            [ 0.1,  0.2,  0.2]]  // flattens and then returns list of both values and indices   topk(x, ret_typ='both', k=2) = [[[ 0.4,  0.3], [ 0.3,  0.2]] ,  [[ 2.,  0.], [ 1.,  2.]]]  Defined in src/operator/tensor/ordering_op.cc:L63  Arguments   data::NDArray-or-SymbolicNode : The input array  axis::int or None, optional, default='-1' : Axis along which to choose the top k indices. If not given, the flattened array is used. Default is -1.  k::int, optional, default='1' : Number of top elements to select, should be always smaller than or equal to the element number in the given axis. A global sort is performed if set k   1.  ret_typ::{'both', 'indices', 'mask', 'value'},optional, default='indices' : The return type.   \"value\" means to return the top k values, \"indices\" means to return the indices of the top k values, \"mask\" means to return a mask array containing 0 and 1. 1 means the top k values. \"both\" means to return a list of both values and indices of top k elements.   is_ascend::boolean, optional, default=0 : Whether to choose k largest or k smallest elements. Top K largest elements will be chosen if set to false.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.uniform     Method .  uniform(low, high, shape, ctx, dtype)  uniform is an alias of _random_uniform.  Draw random samples from a uniform distribution.  .. note:: The existing alias $uniform$ is deprecated.  Samples are uniformly distributed over the half-open interval  [low, high)  (includes  low , but excludes  high ).  Example::  uniform(low=0, high=1, shape=(2,2)) = [[ 0.60276335,  0.85794562],                                           [ 0.54488319,  0.84725171]]  Defined in src/operator/random/sample_op.cc:L66  Arguments   low::float, optional, default=0 : Lower bound of the distribution.  high::float, optional, default=1 : Upper bound of the distribution.  shape::Shape(tuple), optional, default=[] : Shape of the output.  ctx::string, optional, default='' : Context of output, in format  cpu|gpu|cpu_pinned . Only used for imperative calls.  dtype::{'None', 'float16', 'float32', 'float64'},optional, default='None' : DType of the output in case this can't be inferred. Defaults to float32 if not defined (dtype=None).  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.where     Method .  where(condition, x, y)  Given three ndarrays, condition, x, and y, return an ndarray with the elements from x or y, depending on the elements from condition are true or false. x and y must have the same shape. If condition has the same shape as x, each element in the output array is from x if the corresponding element in the condition is true, and from y if false. If condition does not have the same shape as x, it must be a 1D array whose size is the same as x's first dimension size. Each row of the output array is from x's row if the corresponding element from condition is true, and from y's row if false.  From:src/operator/tensor/control_flow_op.cc:40  Arguments   condition::NDArray-or-SymbolicNode : condition array  x::NDArray-or-SymbolicNode :  y::NDArray-or-SymbolicNode :  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source  #  MXNet.mx.zeros_like     Method .  zeros_like(data)  Return an array of zeros with the same shape and type as the input array.  The storage type of $zeros_like$ output depends on the storage type of the input   zeros_like(row_sparse) = row_sparse  zeros_like(csr) = csr  zeros_like(default) = default   Examples::  x = [[ 1.,  1.,  1.],        [ 1.,  1.,  1.]]  zeros_like(x) = [[ 0.,  0.,  0.],                    [ 0.,  0.,  0.]]  Arguments   data::NDArray-or-SymbolicNode : The input  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   source", 
            "title": "notice that the first row is all 0 because label[0] is 1, which is equal to ignore_label."
        }, 
        {
            "location": "/api/nn-factory/", 
            "text": "Neural Network Factory\n\n\nNeural network factory provide convenient helper functions to define common neural networks.\n\n\n#\n\n\nMXNet.mx.MLP\n \n \nMethod\n.\n\n\nMLP(input, spec; hidden_activation = :relu, prefix)\n\n\n\n\nConstruct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.\n\n\nArguments:\n\n\n\n\ninput::SymbolicNode\n: the input to the mlp.\n\n\nspec\n: the mlp specification, a list of hidden dimensions. For example,         \n[128, (512, :sigmoid), 10]\n. The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).\n\n\nhidden_activation::Symbol\n: keyword argument, default \n:relu\n, indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the \nspec\n argument. Also activation is not         applied to the last, i.e. the prediction layer. See \nActivation\n for a         list of supported activation types.\n\n\nprefix\n: keyword argument, default \ngensym()\n, used as the prefix to         name the constructed layers.\n\n\n\n\nReturns the constructed MLP.\n\n\nsource", 
            "title": "Neural Networks Factory"
        }, 
        {
            "location": "/api/nn-factory/#neural-network-factory", 
            "text": "Neural network factory provide convenient helper functions to define common neural networks.  #  MXNet.mx.MLP     Method .  MLP(input, spec; hidden_activation = :relu, prefix)  Construct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.  Arguments:   input::SymbolicNode : the input to the mlp.  spec : the mlp specification, a list of hidden dimensions. For example,          [128, (512, :sigmoid), 10] . The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).  hidden_activation::Symbol : keyword argument, default  :relu , indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the  spec  argument. Also activation is not         applied to the last, i.e. the prediction layer. See  Activation  for a         list of supported activation types.  prefix : keyword argument, default  gensym() , used as the prefix to         name the constructed layers.   Returns the constructed MLP.  source", 
            "title": "Neural Network Factory"
        }, 
        {
            "location": "/api/executor/", 
            "text": "Executor\n\n\n#\n\n\nMXNet.mx.Executor\n \n \nType\n.\n\n\nExecutor\n\n\n\n\nAn executor is a realization of a symbolic architecture defined by a \nSymbolicNode\n. The actual forward and backward computation specified by the network architecture can be carried out with an executor.\n\n\nsource\n\n\n#\n\n\nBase.bind\n \n \nMethod\n.\n\n\nbind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)\n\n\n\n\nCreate an \nExecutor\n by binding a \nSymbolicNode\n to concrete \nNDArray\n.\n\n\nArguments\n\n\n\n\nsym::SymbolicNode\n: the network architecture describing the computation graph.\n\n\nctx::Context\n: the context on which the computation should run.\n\n\nargs\n: either a list of \nNDArray\n or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels.         See \nlist_arguments\n and \ninfer_shape\n.\n\n\nargs_grad\n: a \nVector\n of \nNDArray\n or a \nDict\n contains \nNDArray\n\n\naux_states\n: a \nVector\n of \nNDArray\n or a \nDict\n contains \nNDArray\n\n\ngrad_req\n: single value, a \nVector\n of \nGRAD_REQ\n or a \nDict{Symbol,GRAD_REQ}\n\n\n\n\nsource\n\n\n#\n\n\nBase.print\n \n \nMethod\n.\n\n\nprint([io::IO], x::Executor)\n\n\n\n\nGet a debug string about internal execution plan.\n\n\nCan be used to get an estimated about the memory cost.\n\n\njulia\n x = mx.Variable(:x)\nMXNet.mx.SymbolicNode x\n\njulia\n exec = mx.bind(x + 1, mx.cpu(), Dict(:x =\n mx.ones(2,3)))\nmx.Executor Ptr{Void} @0x000055c3dee9eb30\n\njulia\n print(exec)\nSymbol Outputs:\n        output[0]=_plus_scalar0(0)\nVariable:x\n--------------------\nOp:_plus_scalar, Name=_plus_scalar0\nInputs:\n        arg[0]=x(0) version=0\nAttrs:\n        scalar=1.00000000e+00\nTotal 0 MB allocated\nTotal 11 TempSpace resource requested\n\n\n\n\nsource", 
            "title": "Executor"
        }, 
        {
            "location": "/api/executor/#executor", 
            "text": "#  MXNet.mx.Executor     Type .  Executor  An executor is a realization of a symbolic architecture defined by a  SymbolicNode . The actual forward and backward computation specified by the network architecture can be carried out with an executor.  source  #  Base.bind     Method .  bind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)  Create an  Executor  by binding a  SymbolicNode  to concrete  NDArray .  Arguments   sym::SymbolicNode : the network architecture describing the computation graph.  ctx::Context : the context on which the computation should run.  args : either a list of  NDArray  or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels.         See  list_arguments  and  infer_shape .  args_grad : a  Vector  of  NDArray  or a  Dict  contains  NDArray  aux_states : a  Vector  of  NDArray  or a  Dict  contains  NDArray  grad_req : single value, a  Vector  of  GRAD_REQ  or a  Dict{Symbol,GRAD_REQ}   source  #  Base.print     Method .  print([io::IO], x::Executor)  Get a debug string about internal execution plan.  Can be used to get an estimated about the memory cost.  julia  x = mx.Variable(:x)\nMXNet.mx.SymbolicNode x\n\njulia  exec = mx.bind(x + 1, mx.cpu(), Dict(:x =  mx.ones(2,3)))\nmx.Executor Ptr{Void} @0x000055c3dee9eb30\n\njulia  print(exec)\nSymbol Outputs:\n        output[0]=_plus_scalar0(0)\nVariable:x\n--------------------\nOp:_plus_scalar, Name=_plus_scalar0\nInputs:\n        arg[0]=x(0) version=0\nAttrs:\n        scalar=1.00000000e+00\nTotal 0 MB allocated\nTotal 11 TempSpace resource requested  source", 
            "title": "Executor"
        }, 
        {
            "location": "/api/visualize/", 
            "text": "Network Visualization\n\n\n#\n\n\nMXNet.mx.to_graphviz\n \n \nMethod\n.\n\n\nto_graphviz(network)\n\n\n\n\n\n\nnetwork::SymbolicNode\n: the network to visualize.\n\n\ntitle::AbstractString:\n keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.\n\n\ninput_shapes\n: keyword argument, default \nnothing\n. If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.\n\n\n\n\nReturns the graph description in GraphViz \ndot\n language.\n\n\nsource", 
            "title": "Network Visualization"
        }, 
        {
            "location": "/api/visualize/#network-visualization", 
            "text": "#  MXNet.mx.to_graphviz     Method .  to_graphviz(network)   network::SymbolicNode : the network to visualize.  title::AbstractString:  keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.  input_shapes : keyword argument, default  nothing . If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.   Returns the graph description in GraphViz  dot  language.  source", 
            "title": "Network Visualization"
        }
    ]
}